---
title: "System Identification: Chapter 10"
subtitle: "Sections 10.1, 10.2, 10.5: Computing the Estimate"
author: "System Identification: Theory for the User"
format:
  revealjs:
    theme: serif
    slide-number: true
    chalkboard: true
    preview-links: auto
    transition: slide
    html-math-method: mathjax
    css: assets/styles/styles.css
revealjs-plugins:
  - pointer
---

## Presentation Overview

**Chapter 10 - Computing the Estimate**

-   Section 10.1: Linear Regressions and Least Squares

-   Section 10.2: Numerical Solution by Iterative Search Methods

-   Section 10.5: Local Solutions and Initial Values

::: notes
Let's proceed now to chapter 10. 

**Chapter 10 - Computing the Estimate**

The main question this chapter answers is: how do we actually compute parameter estimates?

We'll cover three main sections:

- **Section 10.1**: Linear Regressions and Least Squares—for when we have linear problems we can solve directly

- **Section 10.2**: Numerical Solution by Iterative Search Methods—for nonlinear problems where we need to search for the solution

- **Section 10.5**: Local Solutions and Initial Values—practical issues like where to start and how to avoid getting stuck
:::

------------------------------------------------------------------------

# Chapter 10: Computing the Estimate {background-color="#1a4d6b"}

------------------------------------------------------------------------

## Chapter 10 Overview

In Chapter 7, we introduced three basic procedures for parameter estimation:

1.  **Prediction-error approach**: Minimize $V_N(\theta, Z^N)$ with respect to $\theta$

2.  **Correlation approach**: Solve equation $f_N(\theta, Z^N) = 0$ for $\theta$

3.  **Subspace approach**: For estimating state space models

. . .

**This chapter**: How to solve these problems numerically

::: notes
Let's start with some context from Chapter 7.

**In Chapter 7**, we learned about three approaches to parameter estimation:

- **Prediction-error approach**: Minimize criterion V_N(θ)

- **Correlation approach**: Solve equation f_N(θ) = 0

- **Subspace approach**: For state space models

[→ Click for this chapter's focus]

**This chapter**: Chapter 7 told us *what* to do—minimize this, solve that—but not *how* to compute it on a computer. That's what Chapter 10 is about.
:::

------------------------------------------------------------------------

## The Numerical Problem

At time $N$, when the data set $Z^N$ is known:

-   $V_N$ and $f_N$ are ordinary functions of a finite-dimensional parameter vector $\theta$

-   This amounts to standard **nonlinear programming** and **numerical analysis**

. . .

**However**: The specific structure of parameter estimation problems makes specialized methods worthwhile

::: notes
At time N, when the data set Z^N is known, **V_N and f_N are ordinary functions of a finite-dimensional parameter vector θ** (THAY-tah). **This amounts to standard nonlinear programming and numerical analysis**—the kind of optimization problems you'd solve with Excel Solver, `optim()` in R, or `scipy.minimize()` in Python.

[→ Click]

**However**, the specific structure of parameter estimation problems makes specialized methods worthwhile. Generic optimization tools (like R's `optim()`, Python's `scipy.optimize`, or MATLAB's `fminunc`) work, but methods designed for this structure are faster and more reliable.
:::

------------------------------------------------------------------------

# Section 10.1: Linear Regressions and Least Squares {background-color="#2c5f77"}

------------------------------------------------------------------------

## The Normal Equations

For linear regressions, the prediction is:

$$\hat{y}(t|\theta) = \varphi^T(t)\theta$$

where:

-   $\varphi(t)$ is the **regression vector**

-   $\theta$ is the **parameter vector**

::: notes
Now we move to Section 10.1 on linear regressions.

**For linear regressions, the prediction is ŷ(t|θ) = φᵀ(t)θ** (fie-transpose times THAY-tah). This formula shows that the prediction is a weighted sum of the regression vector components.

- **φ(t)** (fie) is the regression vector

- **θ** (THAY-tah) is the parameter vector
:::

------------------------------------------------------------------------

## Least Squares Solution

The prediction-error approach with quadratic norm gives the LS estimate:

$$\hat{\theta}_N^{LS} = R^{-1}(N)f(N)$$

::: notes
Here's the least squares solution.

**The prediction-error approach with quadratic norm gives the LS estimate**: θ̂ = R⁻¹(N)f(N) (THAY-tah hat equals R inverse times f).

But what are R and f? Let's break them down...
:::

------------------------------------------------------------------------

## Least Squares Solution - The Components

$$\hat{\theta}_N^{LS} = R^{-1}(N)f(N)$$

. . .

**R(N)** — Sample covariance matrix of regressors:

$$R(N) = \frac{1}{N}\sum_{t=1}^{N}\varphi(t)\varphi^T(t)$$

. . .

**f(N)** — Sample cross-covariance of regressors and outputs:

$$f(N) = \frac{1}{N}\sum_{t=1}^{N}\varphi(t)y(t)$$

::: notes
Now let's break down R and f.

[→ Click for R(N)]

**R(N) — Sample covariance matrix of regressors**: This measures how the regression vectors φ (fie) correlate with each other across all time points.

[→ Click for f(N)]

**f(N) — Sample cross-covariance of regressors and outputs**: This measures how the regressors φ (fie) correlate with the outputs y.

This solution is analytical—but computing it numerically can be tricky, as we'll see next.
:::

------------------------------------------------------------------------

## The Normal Equations (Alternative View)

The LS estimate $\hat{\theta}_N^{LS}$ solves:

$$R(N)\hat{\theta}_N^{LS} = f(N)$$

. . .

**These are called the *normal equations***

::: notes
Here's an alternative way to express the LS estimate.

**The normal equations**: R(N) times θ̂ (THAY-tah hat) equals f(N).

[→ Click for why "normal"]

The term "normal" comes from geometry—the solution makes the error vector perpendicular (normal) to the space of regressors. This is a standard linear system: Ax = b.
:::

------------------------------------------------------------------------

## Numerical Challenge

**Problem**: The coefficient matrix $R(N)$ may be **ill-conditioned**

-   Particularly when dimension is high

-   Direct solution can be numerically unstable

. . .

::: {.warning}
**The core issue**: Matrix multiplication **squares the condition number** κ

- Example: κ(Φ) = 100 → κ(R(N)) = 10,000 (100× worse!)
:::

. . .

**Solution**: Use **QR factorization** for superior numerical stability

::: notes
Here's a numerical challenge with the direct approach.

**Problem: The coefficient matrix R(N) may be ill-conditioned**—particularly when dimension is high. Direct solution can be numerically unstable.

[→ Click for the core issue]

**The core issue: Matrix multiplication squares the condition number κ** (CAP-uh). If κ of Φ is 100, then κ of R(N) becomes 10,000—that's 100 times worse!

[→ Click for solution]

**Solution: Use QR factorization** for superior numerical stability.
:::

------------------------------------------------------------------------

## QR Factorization Definition

For an $n \times d$ matrix $A$:

$$A = QR$$

where:

-   $Q$ is $n \times n$ orthogonal: $QQ^T = I$

-   $R$ is $n \times d$ upper triangular

. . .

Various approaches exist: Householder transformations, Gram-Schmidt procedure, Cholesky decomposition

::: notes
Here's the definition of QR factorization.

**For an n × d matrix A, we write A = QR** where **Q is n × n orthogonal** (QQᵀ = I) and **R is n × d upper triangular**.

[→ Click for implementation methods]

**Various approaches exist**: Householder, Gram-Schmidt, Cholesky. Standard libraries like R's `qr()` or NumPy's `numpy.linalg.qr()` handle this for you.
:::

------------------------------------------------------------------------

## Understanding QR Factorization

**What does this decomposition do?**

Any matrix $A$ can be written as the product of:

1.  $Q$: An **orthogonal matrix** (preserves lengths and angles)
    -   Think of it as a rotation/reflection
    -   Property: $QQ^T = I$ (its transpose is its inverse)
2.  $R$: An **upper triangular matrix** (zeros below diagonal)
    -   Easy to solve systems with (back-substitution)

::: notes
**What does this decomposition do?** The slide explains both components—Q preserves geometry (like rotation/reflection), R is triangular for easy solving.

Now let's see a concrete example of why QR factorization gives better numerical stability.
:::

------------------------------------------------------------------------

## Numerical Example: Why Multiplication is Bad

Consider a simple case where data has condition number $\kappa = 100$

. . .

**Direct approach:**

-   Form $R(N) = \mathbf{\Phi}^T\mathbf{\Phi}$

-   Condition number of $R(N)$: $\kappa^2 = 10,000$

-   Relative error magnified by factor of 10,000!

::: notes
Here's a concrete numerical example.

**Consider a simple case where data has condition number κ = 100**.

[→ Click for direct approach]

**Direct approach**: Form R(N) = ΦᵀΦ. The condition number of R(N) becomes κ² = 10,000. Relative error magnified by factor of 10,000!

That means a tiny 0.01% data error can become a 100% solution error.
:::

------------------------------------------------------------------------

## Numerical Example: Why Multiplication is Bad

Consider a simple case where data has condition number $\kappa = 100$

**QR approach:**

-   Work with $R_1$ from QR factorization

-   Condition number of $R_1$: $\kappa = 100$

-   Relative error magnified only by factor of 100

. . .

**Result:** 100× improvement in numerical stability!

::: notes
Now compare to the QR approach.

**QR approach**: Work with R₁ from QR factorization. The condition number stays at κ (CAP-uh) = 100—it doesn't get squared!

[→ Click for the result]

**Result**: 100× improvement in numerical stability. This is huge—the difference between reliable answers and garbage from rounding errors.

This is why QR factorization is the standard method in practice.
:::


------------------------------------------------------------------------

## QR Factorization: The Key Insight

**Instead of computing** $R(N) = \mathbf{\Phi}^T\mathbf{\Phi}$ directly...

. . .

**Factor** $\mathbf{\Phi}$ itself:

$$\mathbf{\Phi} = QR_1$$

. . .

**Then:**

$$R(N) = \mathbf{\Phi}^T\mathbf{\Phi} = (QR_1)^T(QR_1) = R_1^T Q^T Q R_1 = R_1^T R_1$$

(using $Q^TQ = I$)

. . .

**Key point:**

-   We never form $\mathbf{\Phi}^T\mathbf{\Phi}$

-   We work with $R_1$ directly!

::: notes
Here's the key insight behind QR factorization.

**Instead of** computing R(N) = ΦᵀΦ directly (which squares κ)...

[→ Click for the factorization]

**Factor Φ (big PHI) itself**: Φ = QR₁, where Q is orthogonal and R₁ is triangular.

[→ Click for the derivation]

**Then**: R(N) = R₁ᵀR₁. We use QᵀQ = I to simplify.

[→ Click for the key point]

**Key point**: We never form ΦᵀΦ—we work with R₁ directly. R₁ has the square root of κ, not the squared version.
:::


------------------------------------------------------------------------

## Concrete Example: 2×2 Case

Let's work through a small example:

$$\mathbf{\Phi} = \begin{bmatrix} 3 & 0 \\ 4 & 0 \\ 0 & 5 \end{bmatrix}$$

. . .

**Direct approach:** Form $R(N) = \mathbf{\Phi}^T\mathbf{\Phi}$

$$R(N) = \begin{bmatrix} 3 & 4 & 0 \\ 0 & 0 & 5 \end{bmatrix} \begin{bmatrix} 3 & 0 \\ 4 & 0 \\ 0 & 5 \end{bmatrix} = \begin{bmatrix} 25 & 0 \\ 0 & 25 \end{bmatrix}$$

::: notes
Let me work through a concrete example.

**Data matrix Φ (big PHI)**: A 3×2 matrix with entries 3, 4, and 5.

[→ Click for direct approach]

**Direct approach**: Form R(N) = ΦᵀΦ. We get 25s on the diagonal.

Notice: Numbers went from 3-5 in Φ to 25 in R(N). That's the squaring effect!
:::


------------------------------------------------------------------------

## Concrete Example: QR Approach - Setup {.small-font}

Same matrix: $\mathbf{\Phi} = \begin{bmatrix} 3 & 0 \\ 4 & 0 \\ 0 & 5 \end{bmatrix}$

. . .

We want to factor this as: $\mathbf{\Phi} = QR_1$

. . .

**What will we get?**

-   $Q$: An orthogonal matrix (preserves geometry)

-   $R_1$: An upper triangular matrix (easy to solve)

::: notes
Now let's use QR on the same matrix.

**We want**: Φ = QR₁

[→ Click for what we'll get]

**What we get**:

- **Q**: Orthogonal matrix—preserves lengths and angles, like a rotation

- **R₁**: Upper triangular—zeros below diagonal, easy to solve

Key difference: We're not squaring anything—just rearranging Φ into a different form.
:::


------------------------------------------------------------------------

## Concrete Example: QR Approach - Results {.small-font}

**QR factorization of** $\mathbf{\Phi} = \begin{bmatrix} 3 & 0 \\ 4 & 0 \\ 0 & 5 \end{bmatrix}$ **gives:**

$$Q = \begin{bmatrix} 0.6 & 0 & -0.8 \\ 0.8 & 0 & 0.6 \\ 0 & 1 & 0 \end{bmatrix}$$

. . .

$$R_1 = \begin{bmatrix} 5 & 0 \\ 0 & 5 \end{bmatrix}$$

::: notes
Here are the QR factorization results.

**Q**: A 3×3 orthogonal matrix. You can verify QᵀQ = I.

[→ Click for R₁]

**R₁**: Upper triangular with 5s on the diagonal.

**Compare**: Direct approach gave us 25s. QR gives us 5s—the square root. Much better conditioning!
:::


------------------------------------------------------------------------

## QR Approach: Verification

**Check:** $R_1^T R_1$ equals $R(N)$:

$$R_1^T R_1 = \begin{bmatrix} 5 & 0 \\ 0 & 5 \end{bmatrix} \begin{bmatrix} 5 & 0 \\ 0 & 5 \end{bmatrix} = \begin{bmatrix} 25 & 0 \\ 0 & 25 \end{bmatrix} = R(N) \checkmark$$

. . .

**Key difference:**

We work with $R_1$ (entries \~5) not $R(N)$ (entries \~25)!

::: notes
Let's verify our QR approach gives the right answer.

**Check**: R₁ᵀR₁ should equal R(N). Yes! 5×5 = 25. ✓

[→ Click for the key difference]

**Key difference**:

- QR approach: Work with R₁ (entries ~5)

- Direct approach: Work with R(N) (entries ~25)

Same final answer, but much better numerical stability along the way.
:::

------------------------------------------------------------------------

## QR Factorization in R: Setup and Phi Matrix

```{.r code-line-numbers="|1-5|7-8"}
# Example: Simple linear regression
set.seed(123)
N <- 10
x <- rnorm(N)
y <- 3 * x + 2 + rnorm(N, sd = 0.4)

# Design matrix Phi: [intercept, x]
Phi <- cbind(1, x)
```

. . .

**What Phi looks like** (10×2 matrix, first 5 rows):
```{.r}
head(Phi, 5)
```
```
     [,1]       [,2]
[1,]    1 -0.5604756
[2,]    1  1.7150650
[3,]    1  0.4609162
[4,]    1 -1.2650612
[5,]    1 -0.6868529
```

::: notes
Let me show you QR factorization in R.

**Setup**: We generate data following y = 3x + 2, plus some noise. We build our design matrix Φ (big PHI) with an intercept column (all 1s) and the x values.

[→ Click to see Φ]

**What Φ looks like**: It's 10×2—column 1 is all 1s for the intercept, column 2 is our x values. This is the standard setup for simple linear regression.
:::

------------------------------------------------------------------------

## QR Factorization in R: Decomposition

```{.r}
# QR decomposition
qr_decomp <- qr(Phi)
Q <- qr.Q(qr_decomp)
R <- qr.R(qr_decomp)
```

. . .

**What Q looks like** (first 4 of 10 rows):
```
           [,1]       [,2]
[1,] -0.3162278 -0.2428765
[2,] -0.3162278  0.5440482
[3,] -0.3162278  0.1995752
[4,] -0.3162278 -0.5073257
```

::: notes
Now we do the QR decomposition.

**QR decomposition in R**: `qr()` computes the factorization, `qr.Q()` extracts Q, `qr.R()` extracts R.

[→ Click for Q matrix]

**What Q looks like**: First 4 rows shown—decimal values from the orthogonalization process.
:::

------------------------------------------------------------------------

## QR Factorization in R: Verifying Orthogonality

**Verify Q is orthogonal:** $Q^TQ = I$
```{.r}
round(t(Q) %*% Q, 10)
```
```
     [,1] [,2]
[1,]    1    0
[2,]    0    1
```

The identity matrix—Q is orthogonal! ✓

::: notes
Let's verify Q is orthogonal.

**Verify Q is orthogonal**: QᵀQ = I. We get 1s on the diagonal, 0s off-diagonal—the identity matrix.

This confirms Q preserves lengths and doesn't amplify errors.
:::

------------------------------------------------------------------------

## QR Factorization in R: The R Matrix

**What R looks like** (2×2 upper triangular):
```{.r}
print(R)
```
```
          [,1]      [,2]
[1,] -3.162278 0.5914424
[2,]  0.000000 2.7595206
```

Note: Bottom left is exactly zero (upper triangular). Entries are ~3.

::: notes
Now let's look at R.

**What R looks like**: 2×2 upper triangular—bottom left is exactly zero. Entries are around 3 and 2.7.
:::

------------------------------------------------------------------------

## QR Factorization in R: Comparison with Direct Approach

**Direct approach:** $\Phi^T\Phi$
```
          [,1]      [,2]
[1,] 10.000000  1.871398
[2,]  1.871398  7.617174
```

. . .

**Verify:** $R^TR = \Phi^T\Phi$
```
        [,1]     [,2]
[1,] 10.000000 1.871398
[2,]  1.871398 7.617174
```

Same result! But we work with R (values ~3), not $\Phi^T\Phi$ (values ~10)

::: notes
Let's compare with the direct approach.

**Direct approach ΦᵀΦ**: Values around 10 and 7—bigger numbers.

[→ Click to verify]

**Verify RᵀR = ΦᵀΦ**: Same result! But we work with R (entries ~3) not ΦᵀΦ (entries ~10). Smaller numbers mean less error amplification.
:::

------------------------------------------------------------------------

## QR Factorization in R: Solution

**Solve for theta:**
```{.r}
theta <- solve(R, t(Q) %*% y)
print(theta)
```
```
         [,1]
[1,] 1.897726  # Intercept (true: 2)
[2,] 3.219099  # Slope (true: 3)
```

. . .

**Compare with direct approach:**
```{.r}
theta_direct <- solve(t(Phi) %*% Phi, t(Phi) %*% y)
print(theta_direct)
```
```
         [,1]
[1,] 1.897726  # Same!
[2,] 3.219099  # Same!
```

Same answer, but QR has better numerical stability!

::: notes
Now let's solve for θ (THAY-tah).

**Solve Rθ = Qᵀy**: Intercept is 1.9 (true: 2), slope is 3.2 (true: 3)—close estimates!

[→ Click to compare with direct approach]

**Compare with direct approach**: Same answer—same intercept, same slope. They're mathematically equivalent.

**Same answer, but QR has better numerical stability!** For this small problem both work fine, but for large ill-conditioned problems, the direct approach can fail while QR still works.
:::


------------------------------------------------------------------------

## Applying QR to LS Estimation

Define matrices for the multivariable case:

$$\mathbf{Y}^T = [y^T(1) \; \cdots \; y^T(N)], \quad \mathbf{Y} \text{ is } Np \times 1$$

$$\mathbf{\Phi}^T = [\varphi(1) \; \cdots \; \varphi(N)], \quad \mathbf{\Phi} \text{ is } Np \times d$$

. . .

The LS criterion:

$$V_N(\theta, Z^N) = |\mathbf{Y} - \mathbf{\Phi}\theta|^2 = \sum_{t=1}^{N}|y(t) - \varphi^T(t)\theta|^2$$

::: notes
Now let's apply QR to the full LS problem.

**Y** (big Y): Stacks all outputs from t=1 to N. It's Np × 1.

**Φ** (big PHI): Stacks all regression vectors φ (fie). It's Np × d.

[→ Click for the LS criterion]

**The LS criterion**: V_N(θ) = |Y − Φθ|²—the sum of squared prediction errors. Our goal: find θ (THAY-tah) that minimizes this.
:::


------------------------------------------------------------------------

## Orthonormal Transformation Property

**Key insight**: The norm is invariant under orthonormal transformations

For any vector $v$ and orthonormal matrix $Q$ ($QQ^T = I$):

$$|Qv|^2 = |v|^2$$

. . .

**Why?** Because $|Qv|^2 = (Qv)^T(Qv) = v^TQ^TQv = v^Tv = |v|^2$

. . .

**Application to our problem:**

$$V_N(\theta) = |\mathbf{Y} - \mathbf{\Phi}\theta|^2 = |Q(\mathbf{Y} - \mathbf{\Phi}\theta)|^2$$

We can multiply by $Q$ without changing the criterion!

::: notes
Here's a key property we need.

**The norm is invariant under orthonormal transformations.** Multiplying a vector by Q doesn't change its length.

[→ Click for why]

**Why?** The proof is on the slide. The key step is that Q-transpose times Q equals the identity, so they cancel out.

[→ Click for application]

**Application to our problem:** We can multiply our error vector by Q without changing the loss function. This is the key trick that makes QR work!
:::


------------------------------------------------------------------------

## QR Factorization of Augmented Matrix

**Key idea**: Stack $\mathbf{\Phi}$ and $\mathbf{Y}$ side-by-side, then factor

$$[\mathbf{\Phi} \; \mathbf{Y}] = QR$$

. . .

**What does this look like?**

$$\begin{bmatrix} \varphi^T(1) & y(1) \\ \varphi^T(2) & y(2) \\ \vdots & \vdots \\ \varphi^T(N) & y(N) \end{bmatrix} \quad \leftarrow \text{Each row: one time point}$$

. . .

**Why?** Transforming by $Q$ doesn't change the loss function!

::: notes
Here's a clever trick.

**Key idea—stack Φ and Y side-by-side, then factor.**

[→ Click for what it looks like]

**What does this look like?** Each row is one time point. On the left you have the regression vector—your inputs. On the right you have y—your output. We're just gluing them together into one matrix.

[→ Click for why]

**Why do this?** Because multiplying by Q doesn't change the loss function. So we can transform both Φ and Y together, and the optimization problem stays the same.
:::

------------------------------------------------------------------------

## Structure of the QR Result

After factorization: $[\mathbf{\Phi} \; \mathbf{Y}] = QR$

**R is mostly zeros:**

$$R = \begin{bmatrix} R_0 \\ 0 \\ 0 \\ \vdots \\ 0 \end{bmatrix} \quad \leftarrow \text{Only top block } R_0 \text{ is non-zero}$$

. . .

**Big picture:** We have lots of data (many rows), but few parameters (few columns). Since R is upper triangular, everything below the small top block is zeros.

→ **We only need to work with the small block $R_0$, not the huge matrix $R$!**

::: notes
After factorization, let's look at R.

**R is mostly zeros.** Since R is upper triangular, everything below the top block is all zeros.

[→ Click for big picture]

**Big picture**: We typically have thousands of data points but only a handful of parameters. So R is a tall, skinny matrix—but we only care about the small square block at the top. This is a massive computational saving.
:::

------------------------------------------------------------------------

## Decomposing $R_0$: Separating Data and Outputs

Partition $R_0$ to separate the $\mathbf{\Phi}$ part from the $\mathbf{Y}$ part:

$$R_0 = \begin{bmatrix} R_1 & R_2 \\ 0 & R_3 \end{bmatrix}$$

where:

-   $R_1$ is $d \times d$ (corresponds to $\mathbf{\Phi}$)

-   $R_2$ is $d \times 1$ (interaction between $\mathbf{\Phi}$ and $\mathbf{Y}$)

-   $R_3$ is scalar (corresponds to $\mathbf{Y}$)

::: notes
Now let's decompose R₀ into smaller blocks.

Remember, R₀ came from our augmented matrix which had both Φ and Y. So R₀ naturally splits into parts: R₁ captures the data, R₃ captures the output, and R₂ captures how they interact.

Why does this matter? Because we'll see that each block plays a specific role in finding our solution.
:::

------------------------------------------------------------------------

## How This Transforms the LS Criterion

Original criterion: $V_N(\theta) = |\mathbf{Y} - \mathbf{\Phi}\theta|^2$

After applying $Q^T$ (using $QQ^T = I$):

$V_N(\theta) = |Q^T(\mathbf{Y} - \mathbf{\Phi}\theta)|^2 = |Q^T[\mathbf{\Phi} \; \mathbf{Y}] \begin{bmatrix} -\theta \\ 1 \end{bmatrix}|^2$

. . .

$V_N(\theta) = |R \begin{bmatrix} -\theta \\ 1 \end{bmatrix}|^2$

::: notes
Now here's where everything comes together.

We started with our original criterion (loss function)—the sum of squared prediction errors. The key trick is that we can multiply by Q-transpose without changing anything, because orthogonal matrices preserve lengths.

[→ Click for final form]

After some algebra, our loss function now only involves R. And since R is mostly zeros, we really only need R₀. This is the payoff of all that setup!
:::

------------------------------------------------------------------------

## Transformed Criterion (Final Form)

Since only $R_0$ is non-zero, and using the block structure:

$$V_N(\theta) = \left|\begin{bmatrix} R_1 & R_2 \\ 0 & R_3 \end{bmatrix} \begin{bmatrix} -\theta \\ 1 \end{bmatrix}\right|^2$$

$$= \left|\begin{bmatrix} -R_1\theta + R_2 \\ R_3 \end{bmatrix}\right|^2 = |R_2 - R_1\theta|^2 + |R_3|^2$$

::: notes
Here's the transformed criterion in its final form.

Using the block structure of R₀, our loss function splits into two parts. Look at the final expression on the slide—it's a sum of two squared terms.

This split is important because one part depends on θ and one part doesn't. That's what lets us find the minimum.
:::

------------------------------------------------------------------------

## Finding the Minimum

**Goal:** Minimize $V_N(\theta) = |R_2 - R_1\theta|^2 + |R_3|^2$

. . .

**Two terms:**
1. $|R_2 - R_1\theta|^2$ ← depends on θ (we can control this!)
2. $|R_3|^2$ ← constant (can't change it)

. . .

**Strategy:** Make term 1 as small as possible → set it to zero!

$$|R_2 - R_1\theta|^2 = 0 \quad \Rightarrow \quad \boxed{R_1\hat{\theta}_N = R_2}$$

. . .

**Solution method:**

- $R_1$ is upper triangular → use **back-substitution** (very fast!)

- No matrix inversion needed

- Minimum value: $V_N(\hat{\theta}_N) = |R_3|^2$ ← goodness-of-fit

::: notes
Now let's find the minimum.

Our goal is on the slide—minimize the loss function.

[→ Click for the two terms]

Notice it has two terms. The first term we can control by choosing θ. The second term is fixed—it doesn't depend on θ at all.

[→ Click for strategy]

So the strategy is simple: make the first term as small as possible. The smallest it can be is zero. That gives us the equation shown in the box.

[→ Click for solution method]

Here's the beautiful part. Because R₁ is upper triangular, we can solve this equation using back-substitution—no matrix inversion needed. It's fast and numerically stable.

And the leftover term tells us how good our fit is.
:::

------------------------------------------------------------------------

## Summary: Why QR Works

**Step-by-step what we solved:**

Starting from the transformed criterion: $V_N(\theta) = |R_2 - R_1\theta|^2 + |R_3|^2$

1.  **Used all blocks** ($R_1$, $R_2$, $R_3$) to find the optimal $\theta$
2.  **Solved**: $R_1\hat{\theta}_N = R_2$ using back-substitution
3.  **Residual**: $|R_3|^2$ tells us the minimum achievable loss

::: notes
Let me summarize what we just did.

We took our transformed loss function and found that all three blocks of R₀ play a role. R₁ and R₂ give us the equation to solve. R₃ tells us the residual error—how well our model fits.

The key is that solving the equation is easy because R₁ is triangular.
:::


------------------------------------------------------------------------

## Summary: Why QR Works

**Why this is better than normal equations:**

-   **Conditioning**: $\kappa(R_1) = \sqrt{\kappa(R(N))}$ — square root improvement!

-   **Stability**: Never compute $\mathbf{\Phi}^T\mathbf{\Phi}$ which squares the condition number

-   **Speed**: Back-substitution on triangular $R_1$ is fast and numerically stable

::: notes
So why is QR better than the normal equations?

Three reasons. First, conditioning. The condition number of R₁ is the square root of what we'd get with the direct method. That's a huge improvement.

Second, stability. We never form Φ-transpose Φ, which is where the condition number gets squared and things go wrong.

Third, speed. Back-substitution on a triangular matrix is fast and stable.

This is why QR is the standard method in practice.
:::


------------------------------------------------------------------------

## Initial Conditions Problem

**Key challenge in real applications**: The regression vector $\varphi(t)$ typically contains **shifted data**:

$$\varphi(t) = \begin{bmatrix} z(t-1) \\ z(t-2) \\ \vdots \\ z(t-n) \end{bmatrix}$$

. . .

**The issue**: When $t = 1$, we need $z(0), z(-1), \ldots, z(1-n)$ but we only have data for $t \geq 1$

**What do we do with these missing initial conditions?**

::: notes
Now a practical problem—initial conditions.

**Key challenge**: The regression vector φ (fie) contains shifted data—past values like z(t−1), z(t−2), ..., z(t−n).

[→ Click for the issue]

**The issue**: When t = 1, we need z(0), z(−1), etc.—but we only have data for t ≥ 1!

**What do we do with these missing initial conditions?** This is a practical problem every system identification method must handle.
:::

------------------------------------------------------------------------

## Example: ARX Model

**Concrete example**: What does shifted data look like in practice?

For ARX model with $n_a = n_b = n$:

$$z(t) = \begin{bmatrix} -y(t) \\ u(t) \end{bmatrix}$$

. . .

For AR model ($p$-dimensional process):

$$z(t) = -y(t)$$

::: notes
Let me give you a concrete example.

**Concrete example**: What does shifted data look like?

**For ARX model** with n_a = n_b = n: z(t) = [−y(t); u(t)] contains outputs and inputs.

[→ Click for AR model]

**For AR model** (p-dimensional process): z(t) = −y(t)—just past outputs.
:::

------------------------------------------------------------------------

## Initial Conditions: "Windowed" Data

**Typical structure** of regression vector $\varphi(t)$:

$$\varphi(t) = \begin{bmatrix} z(t-1) \\ \vdots \\ z(t-n) \end{bmatrix}$$

. . .

It consists of **shifted data** (possibly after trivial reordering)

::: notes
Here's another way to look at it.

**Typical structure of φ(t)**: A column vector containing z(t−1) down to z(t−n)—past values.

[→ Click for the key point]

**It consists of shifted data**—like a sliding window showing the last n measurements.
:::

------------------------------------------------------------------------

## The Initial Conditions Problem (Formal)

With structure $\varphi(t) = [z(t-1)^T \; \cdots \; z(t-n)^T]^T$:

$$R_{ij}(N) = \frac{1}{N}\sum_{t=1}^{N} z(t-i)z^T(t-ji)$$

. . .

**Problem**: If we only have data for $1 \leq t \leq N$, what about initial conditions for $t \leq 0$?

We can't compute $z(0), z(-1), \ldots, z(1-n)$ because they don't exist!

::: notes
Let's formalize this.

**With the shifted structure shown on the slide**: The correlation matrix R computes correlations between lagged measurements.

[→ Click for the problem]

**Problem**: The sum starts at t = 1, but when t = 1, we need values at t = 0, t = −1, and so on. These don't exist in our dataset!

We can't compute these values—we need a strategy to handle missing initial conditions.
:::

------------------------------------------------------------------------

## Two Approaches

**Approach 1: Start summation later** (Covariance method)

-   Start at $t = n+1$ instead of $t=1$

-   All sums involve only known data

-   Loses $n$ data points, but straightforward

. . .

**Approach 2: Prewindowing (zero padding)** (Autocorrelation method)

-   Replace unknown initial values by zeros

-   For symmetry: also replace trailing values by zeros ("postwindowing")

-   **Advantage**: Keeps all $N$ data points

. . .

**Key insight**: Approach 2 gives $R(N)$ a special **block Toeplitz structure**, leading to the **Yule-Walker equations**

::: notes
Here are two approaches to handle missing initial conditions.

**Approach 1 (Covariance method)**: Start summation at t = n+1 instead of t = 1. All sums involve only known data, but you lose n data points.

[→ Click for Approach 2]

**Approach 2 (Autocorrelation method/Prewindowing)**: Replace unknown values with zeros. For symmetry, also pad zeros at the end. **Advantage**: Keeps all N data points.

[→ Click for key insight]

**Key insight**: Approach 2 gives R(N) a special block Toeplitz structure, leading to the Yule-Walker equations.
:::

------------------------------------------------------------------------

## Why Toeplitz Structure Matters

**The connection we just established:**

-   Prewindowing (zero padding) → Toeplitz structure → Yule-Walker equations

. . .

**Why is this important?**

The Toeplitz structure means the normal equations have **redundancy**:

-   Entries depend only on distance from diagonal, not absolute position

-   This redundancy can be exploited computationally!

. . .

**What does this enable?**

When fitting AR models of different orders, we can **reuse previous computations** instead of starting from scratch each time → **Levinson algorithm**

::: notes
Now let's see why the Toeplitz structure matters.

**The connection we just established**: Prewindowing → Toeplitz structure → Yule-Walker equations.

[→ Click for why important]

**Why is this important?** The Toeplitz structure means entries depend only on distance from diagonal. This redundancy can be exploited computationally!

[→ Click for what this enables]

**What does this enable?** When fitting AR models of different orders, we can reuse previous computations—this leads us to the Levinson algorithm with O(n²) instead of O(n³)!
:::

------------------------------------------------------------------------

## Levinson Algorithm - The Problem

**Real-world scenario**: You're building an AR model but don't know the right order $n$

. . .

**The challenge**:

-   Too low an order: Model misses important dynamics

-   Too high an order: Model overfits noise

. . .

**Solution**: Try multiple orders ($n=1, 2, 3, ..., 10$) and pick the best using criteria like AIC or BIC

. . .

**But there's a computational cost...**

::: notes
Let me set up the problem Levinson solves.

**Real-world scenario**: Building an AR model but don't know the right order n.

[→ Click for the challenge]

**The challenge**: Too low → miss important dynamics. Too high → overfit noise.

[→ Click for solution]

**Solution**: Try multiple orders (n=1, 2, ..., 10) and pick the best using AIC or BIC.

[→ Click]

**But there's a computational cost...** Each order solved from scratch is expensive!
:::

------------------------------------------------------------------------

## Levinson Algorithm - Naive Approach

**Naive approach**: Try $n=1, n=2, n=3, ..., n=10$ separately

-   Each requires solving the normal equations from scratch

-   For each order: $O(n^3)$ operations (using QR factorization or matrix inversion)

-   Total: $10 \times O(n^3)$ = **very expensive!**

. . .

**Example with** $n=10$:

-   Each solve: \~1000 operations

-   Total: $10 \times 1000 = 10,000$ operations

. . .

**The waste**: We throw away all our work from order $n$ when computing order $n+1$

::: notes
Here's the naive approach.

**Naive approach**: Try n=1, n=2, n=3, ..., n=10 separately. Each requires O(n³) operations. Total: 10 × O(n³) = very expensive!

[→ Click for example]

**Example with n=10**: Each solve ~1000 operations. Total: 10 × 1000 = 10,000 operations.

[→ Click for the waste]

**The waste**: We throw away all work from order n when computing order n+1.
:::

------------------------------------------------------------------------

## Levinson Algorithm - The Clever Solution

**Levinson approach**: Build solutions incrementally, reusing previous work!

-   Start with $n=1$ solution

-   Use it to build $n=2$ solution (just $O(2)$ extra work)

-   Use $n=2$ to build $n=3$ solution (just $O(3)$ extra work)

-   ... and so on

. . .

**Total cost**: $O(1+2+3+...+10) = O(n^2)$ — **much faster!**

. . .

**Example with** $n=10$:

-   Total: $1+2+3+...+10 = 55$ operations

-   **Speedup**: $10,000 / 55 \approx 180$ times faster!

::: notes
Now here's the clever solution.

**Levinson approach—build solutions incrementally**: Start with n=1 solution, use it to build n=2, use n=2 to build n=3, and so on.

[→ Click for total cost]

**Total cost**: O(1+2+3+...+10) = O(n²)—much faster!

[→ Click for example]

**Example with n=10**: Total = 1+2+3+...+10 = 55 operations. **Speedup**: 180× faster!
:::

------------------------------------------------------------------------

## How Levinson Algorithm Works

**Core idea**: Build the solution for order $n+1$ from the solution for order $n$

. . .

**Key observation**: When using **prewindowing** (zero padding), the matrix $R(N)$ has **Toeplitz structure**

-   Toeplitz = entries depend only on distance from diagonal

-   This special structure means previous solutions contain useful information!

. . .

**Update mechanism**:

1.  Start with order $n$ solution: $\hat{\theta}_n$

2.  Add a "correction term" proportional to the old solution

3.  Scale by **reflection coefficient** $\rho_n$ which measures the new information at order $n+1$

::: notes
Now let me explain how the Levinson algorithm works.

**Core idea**: Build the solution for order n+1 from the solution for order n.

[→ Click for key observation]

**Key observation**: When using prewindowing, R(N) has **Toeplitz structure**—entries depend only on distance from diagonal. This means previous solutions contain useful information!

[→ Click for update mechanism]

**Update mechanism**: Start with order n solution, add a correction term, and scale by **reflection coefficient** ρₙ (row) which measures new information at order n+1.
:::

------------------------------------------------------------------------

## Levinson Algorithm - The Formulas

**Recursive update relationships:**

$$\hat{\theta}_k^{n+1} = \hat{\theta}_k^n + \rho_n \hat{\theta}_{n+1-k}^n, \quad k = 1, \ldots, n$$

$$\hat{\theta}_{n+1}^{n+1} = \rho_n, \quad V_{n+1} = V_n + \rho_n \alpha_n$$

where $\rho_n = -\alpha_n / V_n$ is a **reflection coefficient** capturing new information at order $n+1$

::: notes
Here are the actual Levinson formulas.

**Recursive update relationships**: The first equation shows new parameter = old parameter + ρₙ (row) × correction term. The second equation shows the newest parameter equals ρₙ.

**Reflection coefficient ρₙ**: Measures how much new information we gain by adding order n. Always bounded: |ρₙ| < 1 for stable systems.
:::

------------------------------------------------------------------------

## Levinson Algorithm - Computational Impact

**Key efficiency gains:**

-   **Going from order** $n$ to $n+1$: Only $O(n)$ operations (not $O(n^3)$!)

-   **Computing all orders 1 to** $n$: Total $O(n^2)$ operations

. . .

**Real impact**: For $n=20$ (testing orders 1-20):

-   **Standard approach**: $20 \times 8000 = 160,000$ units of work

-   **Levinson approach**: $400$ units of work

-   **Speedup**: 400× faster!

. . .

**Why it matters**:

-   Makes **model order selection** practical

-   **Developed by Levinson (1947)** — classical workhorse in signal processing

::: notes
Let's look at the computational impact.

**Key efficiency gains**: Order n to n+1 is only O(n) operations. Computing all orders 1 to n is O(n²) total.

[→ Click for real impact]

**Real impact**: For n=20, standard approach = 160,000 operations, Levinson = 400 operations. **Speedup**: 400× faster!

[→ Click for why it matters]

**Why it matters**: Makes model order selection practical. Developed by Levinson in 1947—still widely used today.
:::

------------------------------------------------------------------------

## Levinson Algorithm - Intuition

**Think of it as building a "ladder" of models**:

-   **Order 1**: Simple model with 1 parameter

-   **Order 2**: Add parameter 2 using info from order 1

-   **Order 3**: Add parameter 3 using info from orders 1 & 2

-   ... and so on

. . .

Each step **reuses previous work** rather than starting from scratch. This is why adding one more order only costs $O(n)$ extra work, not $O(n^3)$.

. . .

**The reflection coefficient** $\rho_n$ acts like a **diagnostic**:

-   If $|\rho_n| \approx 0$: Adding order $n$ gives little new information

-   If $|\rho_n| \approx 1$: Adding order $n$ is critical for the fit

::: notes
Here's the intuition behind Levinson.

**Think of it as building a "ladder" of models**: Order 1 is a simple model with 1 parameter. Order 2 adds parameter 2 using info from order 1, and so on.

[→ Click for why it works]

Each step **reuses previous work**—this is why adding one more order costs only O(n) extra work, not O(n³).

[→ Click for the diagnostic]

**The reflection coefficient ρₙ (row) acts like a diagnostic**: If |ρₙ| ≈ 0, adding order n gives little new information. If |ρₙ| ≈ 1, adding order n is critical for the fit.
:::

------------------------------------------------------------------------

## Lattice Filters

**What is a lattice filter?**

An alternative **network architecture** for computing the same Levinson recursion, but structured differently:

**How it works**: Instead of updating parameter vectors directly, it processes two parallel **error streams**:

-   **Forward error** $e_n(t)$: Predicting $y(t)$ from its past (standard prediction)

-   **Backward error** $f_n(t)$: "Predicting" past data from future (novel idea!)

. . .

**Key insight**: These two errors are **orthogonal** (independent) at different orders $$\frac{1}{N}\sum_{t=1}^{N} e_n(t)e_{n-k}(t-k) = \begin{cases} V_n, & k = 0 \\ 0, & k \neq 0 \end{cases}$$

This orthogonality is **numerically stabilizing**.

::: notes
Now let me introduce lattice filters.

**What is a lattice filter?** An alternative network architecture for computing the same Levinson recursion.

**How it works**: It processes two parallel error streams. **Forward error** e_n(t) predicts y(t) from its past. **Backward error** f_n(t) "predicts" past data from future.

[→ Click for key insight]

**Key insight**: These two errors are orthogonal at different orders, as shown in the formula. This orthogonality is **numerically stabilizing**.
:::


------------------------------------------------------------------------

## Lattice Filters - Why They're Superior

**Reflection coefficients** $\rho_n$ (also called PACF):

-   **Bounded**: Always $|\rho_n| < 1$ (naturally prevents numerical overflow!)

-   **Interpretation**: Measure of how much order $n$ adds beyond previous orders

-   **Stability detector**: If $|\rho_n| \approx 1$ → system becoming unstable

. . .

**Advantages over standard Levinson**:

1.  **Better numerical stability**: Bounded coefficients prevent round-off errors from growing

2.  **Adaptive/real-time capable**: Can process data one sample at a time (streaming)

3.  **Applications**: Speech processing, Kalman filtering, adaptive signal processing

::: notes
Now why are lattice filters superior?

**Reflection coefficients ρₙ (row)**: Always **bounded** with |ρₙ| < 1. They measure how much order n adds, and serve as a **stability detector**.

[→ Click for advantages]

**Advantages over standard Levinson**: Better numerical stability, adaptive/real-time capable (streaming), and widely used in speech processing and adaptive signal processing.
:::


------------------------------------------------------------------------

## Comparing Levinson vs. Lattice Filters

| Aspect | Levinson | Lattice Filters |
|------------------|--------------------|----------------------------------|
| **What it updates** | Parameter vector $\hat{\theta}_n$ | Error streams $e_n(t)$ and $f_n(t)$ |
| **Complexity** | $O(n^2)$ | $O(n^2)$ |
| **Numerical stability** | Good | Excellent (bounded coefficients) |
| **Real-time capable** | Not naturally | Yes (process sample-by-sample) |
| **Industry use** | Academic/theoretical | Speech, adaptive filtering, control |

. . .

**Bottom line**: Both solve the same problem recursively. Lattice filters trade computation for better stability and adaptability.

::: notes
Let me compare Levinson versus Lattice filters side by side.

The table shows the key differences: what they update, complexity, stability, real-time capability, and industry use.

[→ Click for the bottom line]

**Bottom line**: Both solve the same problem recursively. Lattice filters trade computation for better stability and adaptability.
:::


------------------------------------------------------------------------

## Data Tapering (Optional refinement)

To soften artifacts from zero padding:

-   Apply **tapering weights** to both ends of the data record

-   Reduces edge effects from the appended zeros

-   Used in conjunction with prewindowing for refinement

::: notes
Here's an optional refinement called data tapering.

**To soften artifacts from zero padding**: Apply tapering weights to both ends of the data record. This reduces edge effects from the appended zeros.

This is used in conjunction with prewindowing for refinement in high-precision applications.
:::

------------------------------------------------------------------------

## Summary of Section 10.1

**Key takeaways:**

-   **Normal equations** provide the foundation for LS estimation

-   **QR factorization** offers superior numerical stability

-   **Why it works**: Better conditioning, triangular structure, avoids ill-conditioned matrix products

-   **Initial conditions** are handled by prewindowing or starting summation later

-   **Practical insight**: Implement via $R_0$ only, avoiding large matrix storage

::: notes
Let me summarize Section 10.1 on Linear Regressions and Least Squares.

**Key takeaways**:

-   **Normal equations** provide the foundation for LS estimation

-   **QR factorization** offers superior numerical stability

-   **Why it works**: Better conditioning, triangular structure, avoids ill-conditioned matrix products

-   **Initial conditions** are handled by prewindowing or starting summation later

-   **Practical insight**: Implement via R₀ only, avoiding large matrix storage
:::


------------------------------------------------------------------------

# Section 10.2: Numerical Solution by Iterative Search Methods {background-color="#2c5f77"}

------------------------------------------------------------------------

## When Analytical Solutions Fail

In general, the function

$$V_N(\theta, Z^N) = \frac{1}{N}\sum_{t=1}^{N} \ell(\varepsilon(t, \theta), \theta)$$

**cannot be minimized by analytical methods.**

. . .

Similarly, the equation $f_N(\theta, Z^N) = 0$ **cannot be solved directly in general.**

::: notes
Now we move to Section 10.2 on iterative methods.

**In Section 10.1**: We solved for θ directly using normal equations—the easy case.

**Now**: The general situation. When the loss function is non-quadratic, or prediction errors are nonlinear in θ, we can't minimize analytically.

[→ Click for the correlation equation]

**Similarly**: The correlation equation f_N(θ) = 0 can't be solved directly in general.
:::

------------------------------------------------------------------------

## When Analytical Solutions Fail (cont.)

::: {.warning}
**No closed-form solution exists** for nonlinear problems:

- Nonlinear model structures (e.g., Hammerstein-Wiener models)

- Non-quadratic loss functions (L1, robust errors)

- Output error models with parameter-dependent errors
:::

. . .

::: {.callout}
**Solution**: Use **iterative numerical techniques** to solve these problems step-by-step
:::

::: notes
Here are the specific cases where analytical solutions fail.

**No closed-form solution** for:

- **Hammerstein-Wiener**: Cascade of static nonlinearity + linear system + static nonlinearity

- **Non-quadratic loss**: L1 (absolute), Huber loss for robust estimation

- **Output error models**: Prediction errors are nonlinear in parameters

[→ Click for the solution]

**The solution**: We'll use iterative numerical techniques—start with a guess and improve it step by step.
:::

------------------------------------------------------------------------

## Numerical Minimization - General Approach

Methods for numerical minimization update the estimate iteratively:

$$\hat{\theta}^{(i+1)} = \hat{\theta}^{(i)} + \alpha f^{(i)}$$

where:

-   $f^{(i)}$ is a **search direction** based on information about $V(\theta)$
-   $\alpha$ is a **positive constant** (step size) to ensure decrease in $V(\theta)$

. . .

**Three categories of methods:**

1.  **Function values only** (e.g., simplex methods)

2.  **Function + gradient** (e.g., steepest descent, Gauss-Newton)

3.  **Function + gradient + Hessian** (e.g., Newton's method)

::: notes
Here's the general approach for numerical minimization.

**The update formula**: θ̂⁽ⁱ⁺¹⁾ = θ̂⁽ⁱ⁾ + αf⁽ⁱ⁾. Start with initial guess, compute search direction f, take step of size α, repeat until convergence.

[→ Click for three categories]

**Three categories** based on what information they use:

- **Category 1** (function only): Slow but robust. Example: Nelder-Mead simplex.

- **Category 2** (gradient): Much faster, most common. Examples: steepest descent, Gauss-Newton.

- **Category 3** (gradient + Hessian): Best convergence, but expensive. Example: Newton's method.

The trade-off: more information → smarter directions but costlier iterations. Category 2 hits the sweet spot.
:::

------------------------------------------------------------------------

## Overview: Specific Methods in Section 10.2

**Problem**: Minimize $V_N(\theta, Z^N)$ or solve $f_N(\theta, Z^N) = 0$

. . .

**For minimization problems** (focus of this section):

| Method | Category | Information Used | Typical Use |
|-----------------|-----------------|---------------------|-----------------|
| **Newton's Method** | Group 3 | Gradient + Hessian | Near minimum, expensive |
| **Steepest Descent** | Group 2 | Gradient only | Simple, robust, slow |
| **Gauss-Newton** | Group 2 | Gradient + approx. Hessian | Least squares, fast |
| **Levenberg-Marquardt** | Group 2 | Gradient + adaptive damping | Industrial workhorse |

. . .

**For solving equations** (briefly): - **Newton-Raphson method**: Analog of Newton's method for $f_N(\theta) = 0$ - **Substitution method**: Simple iteration on the equation

::: notes
Here's an overview of the specific methods we'll cover.

**Problem**: Minimize V_N(θ) or solve f_N(θ) = 0.

[→ Click for the methods table]

**For minimization problems**: The table shows the four main methods—Newton's Method (Group 3), Steepest Descent, Gauss-Newton, and Levenberg-Marquardt (all Group 2).

[→ Click for solving equations]

**For solving equations**: Newton-Raphson is analogous to Newton's method. Substitution method is simple iteration.
:::

------------------------------------------------------------------------

## Newton's Method

The classic **Newton algorithm** belongs to group 3:

$$f^{(i)} = -[V''(\hat{\theta}^{(i)})]^{-1} V'(\hat{\theta}^{(i)})$$

where:

-   $V'(\theta)$ is the **gradient** (first derivative)

-   $V''(\theta)$ is the **Hessian** (second derivative matrix)

. . .

This approximates $V(\theta)$ by a quadratic function and finds its minimum

::: notes
Now let's look at Newton's method.

**The classic Newton algorithm** belongs to group 3. The formula shows the search direction using the inverse Hessian times the gradient.

**Where**: V'(θ) is the gradient and V''(θ) is the Hessian (second derivative matrix).

[→ Click for the interpretation]

**This approximates V(θ) by a quadratic function** and finds its minimum—like fitting a bowl and jumping to its bottom.
:::

------------------------------------------------------------------------

## Newton's Method - Properties

::: {.key-insight}
**Why Newton's method is powerful:**

- **Quadratic convergence**: Error decreases quadratically with each iteration

- **One-step for quadratics**: If V(θ) is exactly quadratic, Newton finds the minimum in one step

- **Natural step size**: Formula implicitly determines both direction and step size
:::

. . .

::: {.warning}
**Practical issue**: Computing the full Hessian $V''(\theta)$ is expensive

- Requires computing all $d^2$ entries of the Hessian matrix

- Then inverting it costs $O(d^3)$ operations

- For large parameter spaces: **prohibitively expensive**
:::

. . .

::: {.callout}
**This motivates quasi-Newton methods** (coming next)
:::

::: notes
Here's why Newton's method is powerful.

**Why Newton's method is powerful**: Quadratic convergence (error decreases quadratically), one-step solution for quadratic functions, and natural step size built into the formula.

[→ Click for the practical issue]

**Practical issue**: Computing the full Hessian is expensive—requires d² entries and O(d³) to invert. Prohibitively expensive for large parameter spaces.

[→ Click for the motivation]

**This motivates quasi-Newton methods** which approximate the Hessian more cheaply.
:::

------------------------------------------------------------------------

## Why We Need to Understand the Gradient

**Key insight from Newton's Method:**

-   Newton's method requires **both** $V'(\theta)$ (gradient) and $V''(\theta)$ (Hessian)

-   The Hessian is expensive to compute

-   But to understand quasi-Newton approximations, we need to know what the gradient looks like

. . .

**The gradient formula is central because:**

1.  **All methods need it**: Steepest Descent, Gauss-Newton, Levenberg-Marquardt all compute $V'(\theta)$

2.  **Hessian structure depends on it**: Understanding $V'(\theta)$ helps us approximate $V''(\theta)$

3.  **Computational bottleneck**: Computing the gradient for all $N$ data points is the main cost

. . .

**Next:** Let's see the explicit formula for $V'(\theta)$ and understand its structure

::: notes
Before we look at specific methods, let's understand the gradient.

**Key insight from Newton's Method**: Newton requires both gradient and Hessian. The Hessian is expensive, but we need to understand the gradient to approximate it.

[→ Click for why the gradient is central]

**The gradient formula is central because**: All methods need it, the Hessian structure depends on it, and computing it for all N data points is the main cost.

[→ Click for what's next]

**Next**: Let's see the explicit formula for V'(θ).
:::

------------------------------------------------------------------------

## The Gradient Formula

For the criterion $V_N(\theta, Z^N) = \frac{1}{N}\sum_{t=1}^{N} \ell(\varepsilon(t, \theta), \theta)$

The gradient is:

$$V_N'(\theta, Z^N) = -\frac{1}{N}\sum_{t=1}^{N} \{\psi(t, \theta)\ell_{\varepsilon}'(\varepsilon(t, \theta), \theta) - \ell_{\theta}'(\varepsilon(t, \theta), \theta)\}$$

where $\psi(t, \theta) = \frac{\partial \hat{y}(t|\theta)}{\partial \theta}$ is the $d \times p$ gradient matrix

. . .

**Components of the gradient:**

-   $\psi(t, \theta)$: How predicted output changes with parameters

-   $\ell_{\varepsilon}'$: Derivative of loss w.r.t. prediction error

-   $\ell_{\theta}'$: Direct derivative of loss w.r.t. parameters (often zero)

. . .

**Major computational burden**: Computing $\psi(t, \theta)$ for all $t = 1, \ldots, N$

::: notes
Here's the gradient formula.

**The gradient** V'_N is shown in the formula. The key term is ψ(t,θ) (sigh)—how predicted output changes with parameters.

[→ Click for the components]

**Components of the gradient**: ψ(t,θ) is prediction sensitivity, ℓ'_ε is the loss derivative w.r.t. error, and ℓ'_θ is direct parameter dependence (often zero).

[→ Click for the computational burden]

**Major computational burden**: Computing ψ(t,θ) for all t = 1, ..., N is where most CPU time goes.
:::

------------------------------------------------------------------------

## Overview: Iterative Search Methods

**The general update formula:**

$$\hat{\theta}_N^{(i+1)} = \hat{\theta}_N^{(i)} - \mu_N^{(i)} [R_N^{(i)}]^{-1} V_N'(\hat{\theta}_N^{(i)}, Z^N)$$

Different choices of $R_N^{(i)}$ (Hessian approximation) and $\mu_N^{(i)}$ (step size) give different algorithms.

. . .

**Methods we'll cover:**

1.  **Steepest Descent**: Simple gradient direction, slow convergence

2.  **Gauss-Newton**: Hessian approximation using prediction sensitivity

3.  **Levenberg-Marquardt**: Adaptive blend between Gauss-Newton and Steepest Descent

. . .

**Trade-off to understand:**

-   More accurate $R_N^{(i)}$ → faster convergence but more computation

-   How do we choose step size $\mu_N^{(i)}$? (Line search or adaptive)

::: notes
Here's the framework that unifies all iterative search methods.

**The general update formula**: Different choices of R (Hessian approximation) and μ (mew, step size) give different algorithms.

[→ Click for methods we'll cover]

**Methods we'll cover**: Steepest Descent (simple, slow), Gauss-Newton (fast near minimum), and Levenberg-Marquardt (adaptive blend).

[→ Click for the trade-off]

**Trade-off**: More accurate R → faster convergence but more computation per iteration.
:::

------------------------------------------------------------------------

## Steepest Descent Method

The simplest choice: Take $R_N^{(i)} = I$ (identity matrix)

$$\hat{\theta}_N^{(i+1)} = \hat{\theta}_N^{(i)} - \mu_N^{(i)} V_N'(\hat{\theta}_N^{(i)}, Z^N)$$

This is the **gradient descent** or **steepest descent method**.

. . .

**The idea:** Move in direction of negative gradient (steepest downhill)

. . .

::::: columns
::: {.column width="50%"}
**Pros** ✓

-   Simple to implement

-   Doesn't require second derivatives

-   Always descends for small enough $\mu_N^{(i)}$
:::

::: {.column width="50%"}
**Cons** ✗

-   Very slow convergence near minimum

-   Zig-zag behavior in narrow valleys

-   Step size $\mu_N^{(i)}$ critical (too small→slow, too large→diverge)
:::
:::::

::: notes
Let's start with the simplest iterative method—steepest descent.

**The simplest choice**: Take R = I (identity matrix). This gives the gradient descent formula.

[→ Click for the idea]

**The idea**: Move in direction of negative gradient—steepest downhill.

[→ Click for pros and cons]

**Pros**: Simple, no second derivatives needed, always descends for small μ (mew). **Cons**: Slow near minimum, zig-zag behavior, step size critical.
:::

------------------------------------------------------------------------

## Gauss-Newton Method - The Hessian Structure

The full Hessian for quadratic criterion has two terms:

$$V_N''(\theta, Z^N) = \underbrace{\frac{1}{N}\sum_{t=1}^{N} \psi(t, \theta)\psi^T(t, \theta)}_{\text{First term (always ≥ 0)}} - \underbrace{\frac{1}{N}\sum_{t=1}^{N} \psi'(t, \theta)\varepsilon(t, \theta)}_{\text{Second term (requires 2nd derivatives)}}$$

. . .

**Key insight**: Near the minimum, prediction errors $\varepsilon(t, \theta)$ become independent innovations with mean zero → **second term averages to ≈ 0**

::: notes
Now we get to the Gauss-Newton method—one of the most important algorithms in system identification.

**The full Hessian** has two terms:

- **First term**: ψψᵀ—always positive semidefinite (≥ 0)

- **Second term**: ψ'ε—requires expensive second derivatives

[→ Click for the key insight]

**Key insight**: Near the minimum, errors ε(t) become mean-zero innovations. So the second term averages to approximately zero when summed over many data points.
:::

------------------------------------------------------------------------

## Gauss-Newton Method - The Approximation

**Gauss-Newton approximation** (drop the second term):

$$V_N''(\theta, Z^N) \approx H_N(\theta) = \frac{1}{N}\sum_{t=1}^{N} \psi(t, \theta)\psi^T(t, \theta)$$

. . .

**Benefits**:

- Only needs **first derivatives** (no expensive ψ'(t,θ))

- **Always positive semidefinite** → guarantees descent direction

- We already computed ψ for gradient anyway—minimal extra cost

::: notes
Here's the Gauss-Newton approximation.

**The approximation**: We keep only the first term of the Hessian—the ψψᵀ term. Drop the second term since it averages to zero near the minimum.

[→ Click for benefits]

**Benefits**: Only needs first derivatives, always positive semidefinite (guarantees descent), and we already computed ψ for the gradient.
:::

------------------------------------------------------------------------

## Gauss-Newton Method - The Algorithm

Using $R_N^{(i)} = H_N(\hat{\theta}_N^{(i)})$ in the search formula:

$$\hat{\theta}_N^{(i+1)} = \hat{\theta}_N^{(i)} - \mu_N^{(i)} [H_N(\hat{\theta}_N^{(i)})]^{-1} V_N'(\hat{\theta}_N^{(i)}, Z^N)$$

This is the **Gauss-Newton method**.

. . .

**Why it's powerful:**

1.  **Computational efficiency**: Avoids expensive second derivatives $\psi'(t, \theta)$

2.  **Numerical stability**: $H_N(\theta) \geq 0$ guarantees descent direction

3.  **Fast convergence**: Newton-like speed near the minimum

. . .

**Common choice**: $\mu_N^{(i)} = 1$ (no line search needed)

Also called **"method of scoring"** in statistics literature

::: notes
Now let's see the Gauss-Newton algorithm.

**Using R = H_N(θ)** in the search formula gives us the Gauss-Newton method.

[→ Click for why it's powerful]

**Why it's powerful**: Computational efficiency (avoids ψ'), numerical stability (H_N ≥ 0), and fast convergence near the minimum.

[→ Click for common choice]

**Common choice**: μ = 1 (no line search needed). Also called the "method of scoring" in statistics.
:::

------------------------------------------------------------------------

## Levenberg-Marquardt Method

**Problem with Gauss-Newton**: $H_N(\theta)$ may be singular/nearly singular

-   Overparameterized models (redundant parameters)

-   Insufficient data

-   Numerical ill-conditioning

. . .

**Levenberg-Marquardt solution**: Add regularization!

$$R_N^{(i)}(\lambda) = H_N(\hat{\theta}_N^{(i)}) + \lambda I = \frac{1}{N}\sum_{t=1}^{N} \psi\psi^T + \lambda I$$

where $\lambda > 0$ is the **damping parameter**

. . .

**How** $\lambda$ **controls the algorithm:**

| $\lambda$ value | Behavior         | When to use      |
|-----------------|------------------|------------------|
| Small (≈0)      | Gauss-Newton     | Near minimum     |
| Large           | Steepest descent | Far from minimum |

**Adaptive strategy**: Start large $\lambda$ (robust), decrease as converging (fast)

::: notes
Now let's look at the Levenberg-Marquardt method.

**Problem with Gauss-Newton**: H_N(θ) may be singular due to overparameterization, insufficient data, or ill-conditioning.

[→ Click for the solution]

**Levenberg-Marquardt solution**: Add regularization! R = H_N + λI where λ (LAM-duh) is the damping parameter.

[→ Click for how λ controls the algorithm]

**How λ controls the algorithm**: Small λ → Gauss-Newton (fast, near minimum). Large λ → steepest descent (robust, far from minimum). **Adaptive strategy**: Start large λ, decrease as converging.
:::


------------------------------------------------------------------------

## Levenberg-Marquardt - Adaptive λ Strategy

**The algorithm adapts** $\lambda$ **based on progress:**

```         
Initialize: λ = λ₀ (e.g., 0.01)

At each iteration:
  1. Compute update with current λ
  2. Try the new parameter values

  If V(θ_new) < V(θ_old):  [Success!]
     ✓ Accept the update
     ✓ Decrease λ ← λ/10  (trust model more)

  Else:  [Function increased - bad step]
     ✗ Reject the update
     ✗ Increase λ ← λ×10  (be more cautious)
```

. . .

**Result**: Automatic balance between robustness and speed!

::: notes
Here's how the adaptive λ strategy works.

**The algorithm adapts λ based on progress**: Initialize λ, compute update, try new values. If V decreases → accept and decrease λ (trust more). If V increases → reject and increase λ (be cautious).

[→ Click for the result]

**Result**: Automatic balance between robustness and speed! This is why Levenberg-Marquardt is the default in most optimization software.
:::

------------------------------------------------------------------------

## Summary: Iterative Methods for Nonlinear Least Squares

**Problem**: Minimize $V_N(\theta) = \frac{1}{N}\sum_{t=1}^{N} \frac{1}{2}\varepsilon^2(t, \theta)$ where $\varepsilon(t, \theta)$ is nonlinear in $\theta$

. . .

**Key algorithms**:

| Method | Matrix $R_N^{(i)}$ | Pros | Cons |
|------------------|-------------------|------------------|------------------|
| Steepest Descent | $I$ | Simple | Slow near minimum |
| Gauss-Newton | $H_N(\theta) = \frac{1}{N}\sum \psi\psi^T$ | Fast near minimum | May have singularity |
| Levenberg-Marquardt | $H_N(\theta) + \lambda I$ | Robust + fast | Extra tuning param |

. . .

**Practical recommendation**: Use **Levenberg-Marquardt** with adaptive $\lambda$ for robustness

::: notes
Let me summarize the three iterative methods for nonlinear least squares.

**Problem**: Minimize V_N(θ) where ε(t,θ) is nonlinear in θ.

[→ Click for the key algorithms]

**Key algorithms**: The table shows Steepest Descent (R=I, simple but slow), Gauss-Newton (R=H_N, fast but may be singular), and Levenberg-Marquardt (R=H_N+λI, robust + fast).

[→ Click for the practical recommendation]

**Practical recommendation**: Use Levenberg-Marquardt with adaptive λ for robustness.
:::

------------------------------------------------------------------------

## Solving the Correlation Equation

Recall the general equation:

$$0 = f_N(\theta, Z^N) = \frac{1}{N}\sum_{t=1}^{N} \zeta(t, \theta)\alpha(\varepsilon(t, \theta))$$

This is analogous to minimization of $V_N(\theta)$. Standard procedures:

. . .

**Two methods** (analogous to what we've seen):

-   **Substitution method** [like steepest descent]: $\hat{\theta}_N^{(i)} = \hat{\theta}_N^{(i-1)} - \mu_N^{(i)} f_N(\hat{\theta}_N^{(i-1)}, Z^N)$

-   **Newton-Raphson method** [like Newton]: $\hat{\theta}_N^{(i)} = \hat{\theta}_N^{(i-1)} - \mu_N^{(i)} [f_N']^{-1} f_N$

::: notes
Now let me briefly mention solving correlation equations.

**Recall the general equation**: f_N(θ) = 0. This is analogous to minimization of V_N(θ).

[→ Click for the two methods]

**Two methods**: Substitution method (like steepest descent, simple iteration) and Newton-Raphson method (like Newton, uses Jacobian f'_N for quadratic convergence).
:::

------------------------------------------------------------------------

## Summary of Section 10.2

**Key takeaways:**

-   When analytical solutions fail, use **iterative numerical methods**

-   Three levels of information: function values only, gradient, or gradient + Hessian

-   **Gauss-Newton method**: Quasi-Newton approach using only first derivatives

-   **Levenberg-Marquardt**: Regularized Gauss-Newton for robustness

-   Main computational burden: Computing $\varepsilon(t, \theta)$ and $\psi(t, \theta)$ for all $t$

-   Same techniques apply to solving correlation equations

::: notes
Let me summarize Section 10.2.

**Key takeaways**:

-   When analytical solutions fail, use **iterative numerical methods**

-   Three levels of information: function values only, gradient, or gradient + Hessian

-   **Gauss-Newton method**: Quasi-Newton approach using only first derivatives

-   **Levenberg-Marquardt**: Regularized Gauss-Newton for robustness

-   Main computational burden: Computing ε(t,θ) and ψ(t,θ) (sigh) for all t

-   Same techniques apply to solving correlation equations
:::

------------------------------------------------------------------------

# Section 10.5: Local Solutions and Initial Values {background-color="#2c5f77"}

------------------------------------------------------------------------

## The Local Minimum Problem

The iterative methods from Section 10.2 converge to a **local minimum**, not necessarily the **global minimum**.

. . .

**For minimization**: We want $\hat{\theta}_N$ that minimizes $V_N(\theta, Z^N)$ globally

. . .

**For equation solving**: We want $\hat{\theta}_N^*$ that satisfies $f_N(\hat{\theta}_N^*, Z^N) = 0$

. . .

**The challenge**: Iterative search only guarantees convergence to a **local solution**

::: notes
Now we discuss a fundamental limitation of iterative methods.

**The iterative methods from Section 10.2** converge to a local minimum, not necessarily the global minimum.

[→ Click for minimization goal]

**For minimization**: We want θ̂_N that minimizes V_N(θ) globally.

[→ Click for equation solving goal]

**For equation solving**: We want θ̂*_N that satisfies f_N(θ*) = 0.

[→ Click for the challenge]

**The challenge**: Iterative search only guarantees convergence to a local solution.
:::

------------------------------------------------------------------------

## Finding the Global Minimum

**Strategy**: Start the iterative minimization from different feasible initial values and compare results

. . .

**Why this is necessary**:

-   No algorithm can guarantee finding the global minimum in general

-   Starting point determines which local minimum you converge to

-   Must explore the parameter space with multiple starts

. . .

**Practical approach**:

1.  Use preliminary estimation procedures for good initial values

2.  Run optimization from several different starting points

3.  Select the solution with the lowest criterion value

::: notes
Here's how to find the global minimum.

**Strategy**: Start the iterative minimization from different feasible initial values and compare results.

[→ Click for why this is necessary]

**Why this is necessary**: No algorithm can guarantee finding the global minimum. Starting point determines which local minimum you find. Must explore with multiple starts.

[→ Click for the practical approach]

**Practical approach**: Use preliminary estimation for good initial values, run from several starting points, select the solution with the lowest criterion value.
:::

------------------------------------------------------------------------

## Two Aspects of "False" Local Minima

Local minima arise from **two distinct sources**:

. . .

**Aspect 1: Structural Local Minima**

-   The limit criterion $\bar{V}(\theta)$ itself has multiple minima

-   Inherent to the problem formulation

. . .

**Aspect 2: Sample-Induced Local Minima**

-   Finite-sample criterion $V_N(\theta, Z^N)$ has minima due to data randomness

-   May disappear with more data

::: notes
Not all local minima are created equal.

**Local minima arise from two distinct sources**:

[→ Click for Aspect 1]

**Aspect 1: Structural Local Minima**—the limit criterion V̄(θ) itself has multiple minima. Inherent to the problem formulation.

[→ Click for Aspect 2]

**Aspect 2: Sample-Induced Local Minima**—finite-sample V_N(θ) has minima due to data randomness. May disappear with more data.
:::

------------------------------------------------------------------------

## Aspect 1: Structural Local Minima

**The limit criterion** $\bar{V}(\theta) = \lim_{N \to \infty} V_N(\theta, Z^N)$ **itself has multiple minima**

. . .

**Why this happens:**

-   Complex model structures (neural networks, nonlinear models)

-   Model mismatch: true system $S \notin \mathcal{M}$

-   Identifiability issues (different $\theta$ give similar predictions)

. . .

**Consequence**: For large $N$, $V_N(\theta, Z^N)$ will have minima near those same locations (Lemma 8.2)

. . .

**This is inherent to the problem**, not just sampling variation!

::: notes
Let's dive deeper into structural local minima.

**The limit criterion V̄(θ)** itself has multiple minima—not about your specific data, but about the theoretical problem.

[→ Click for why this happens]

**Why this happens**: Complex model structures, model mismatch (S ∉ M), and identifiability issues.

[→ Click for the consequence]

**Consequence**: For large N, V_N(θ) will have minima near those same locations (Lemma 8.2).

[→ Click for the key point]

**This is inherent to the problem**, not just sampling variation! More data doesn't help.
:::

------------------------------------------------------------------------

## Aspect 2: Sample-Induced Local Minima

**Even if** $\bar{V}(\theta)$ **is convex (single minimum)**, the finite-sample criterion $V_N(\theta, Z^N)$ can have multiple local minima

. . .

**Causes:**

-   Random fluctuations in the data
-   Outliers
-   Finite sample effects

. . .

**Much harder to analyze theoretically** - depends on the specific data realization

. . .

**May disappear with more data** as $N \to \infty$


::: notes
**SCRIPT:**

Now let's talk about Aspect 2 - sample-induced local minima. This is a different beast entirely.

Here's the setup: even if V bar of theta is convex - meaning it has only a single global minimum, a perfect bowl shape - the finite-sample criterion V N with your specific data can have multiple local minima. How is this possible?

It's all about randomness. Your data Z to the N is one random realization from a stochastic process. And random fluctuations can create bumps and valleys in the loss surface that aren't there in the expected value.

What causes this? Three things. First, random fluctuations in the data. Maybe you happened to measure during a period where the noise was correlated in a weird way. Second, outliers. One bad measurement can create a spurious valley as the optimizer tries to fit that outlier. Third, finite sample effects. With limited data, you're essentially looking at V bar through a noisy lens, and that noise can create false structure.

Now here's the thing - this is much harder to analyze theoretically than structural local minima. Why? Because it depends on the specific data realization. There's no clean formula for where these minima will appear or how many there will be. It's random. Different datasets will have different spurious minima in different places.

But here's the good news - these may disappear with more data. As N goes to infinity, V N converges to V bar. If V bar is convex, those sample-induced bumps get smoothed out and you're left with just the one global minimum. So sometimes the solution is literally just "collect more data."

In practice, how do you deal with sample-induced local minima? Use regularization to smooth out noise-induced bumps. Use robust estimation methods that aren't sensitive to outliers. Or use cross-validation - if a minimum disappears when you use a different subset of your data, it's probably spurious.

________________________________________
ADDITIONAL NOTES:

- Convex: V''(θ) > 0 everywhere, one global minimum
- Stochastic process: random variables indexed by time
- Regularization: adding penalty terms to smooth the objective function
- This is why validation on held-out data is important
:::
Sample-induced local minima are "false" local minima that appear in the finite-sample criterion \$V_N(\\theta, Z\^N)\$ due to **random fluctuations in the data**, even when the theoretical limit criterion \$\\bar{V}(\\theta)\$ is perfectly convex (has only one global minimum).

**Aspect 1**: Structural Local Minima - Easier to Analyze
Why? Because structural local minima exist in the limit criterion $\bar{V}(\theta)$, which is:
Deterministic - It's a fixed mathematical function, not random
Well-defined - $\bar{V}(\theta) = E[V_N(\theta, Z^N)]$ is an expectation (a mathematical formula)
Can be studied theoretically - You can write down equations and prove theorems

**Aspect 2**: Sample-Induced Local Minima - Much Harder to Analyze
Why? Because sample-induced local minima only exist in the finite-sample criterion $V_N(\theta, Z^N)$, which is:
Random - Different data realizations give different functions
Data-dependent - The location and number of local minima depend on the specific data you collected
No closed-form formula - You can't write down where these minima will appear
:::

------------------------------------------------------------------------

## The Linear Regression Exception

**For linear regression with least squares:**

$$V_N(\theta) = \frac{1}{N}\sum_{t=1}^{N} (y(t) - \varphi^T(t)\theta)^2$$

. . .

This is a **quadratic function** of $\theta$ → strictly convex

. . .

**Result**: Exactly **one minimum** (the global one), regardless of data

. . .

**No false local minima possible by construction!**

::: notes
**SCRIPT:**

Now here's an important exception to the local minima problem - linear regression.

For linear regression with least squares, the criterion V N of theta equals one over N sum of y of t minus phi transpose of t times theta, all squared.

This is a quadratic function of theta, which means it's strictly convex. Think of it like a perfect bowl shape - there's exactly one bottom point. It has exactly one minimum, which is the global minimum, regardless of what data you have.

No false local minima are possible by construction. This is why linear regression is so well-behaved - any descent algorithm will find the global optimum. You don't have to worry about initialization or getting stuck.

This is the special property that made Section 10.1 so clean - we could solve everything analytically. But once we move to nonlinear models, we lose this guarantee.

________________________________________
ADDITIONAL NOTES:

- Quadratic function: V(θ) = θᵀAθ + bᵀθ + c (strictly convex if A > 0)
- Convexity: any local minimum is also the global minimum
- This is why ARX models (linear regression) never have initialization problems
:::

------------------------------------------------------------------------
**Takeaway**

Understanding which type of local minima you're dealing with helps determine the right strategy. Structural local minima require careful model selection; sample-induced minima may disappear with more data.

------------------------------------------------------------------------

## Results for SISO Black-box Models

**Assumption**: The true system can be described within the model set: $S \in \mathcal{M}$

Consider the criterion:

$$\bar{V}(\theta) = \bar{E}\frac{1}{2}\varepsilon^2(t, \theta)$$

. . .

**Key results for different model structures:**

-   **ARMA models** ($B \equiv 0, D \equiv F \equiv 1$): All stationary points are global minima
-   **ARARX models** ($C = F \equiv 1$): No false minima if signal-to-noise ratio is large enough
-   **Single-input models** ($n_f = 1$): No false local minima
-   **Box-Jenkins** ($A = C = D \equiv 1$): No false minima if input is white noise

::: notes
**SCRIPT:**

Now let's look at results for SISO - Single-Input Single-Output - black-box models. These are important practical results about when you need to worry about local minima.

The key assumption is that the true system can be described within the model set - S is in M. This means there exists some theta 0 such that your model perfectly describes reality. This is a strong assumption, often not true in practice, but it lets us analyze the problem theoretically.

Let me go through the key results for different model structures.

For ARMA models - output without input - every stationary point is a global minimum. This is the best case scenario. Gradient-based methods will find the global solution from any reasonable starting point.

For ARARX models - ARX with colored noise - there are no false local minima if the signal-to-noise ratio is large enough. But if SNR is very small, false local minima can exist.

For single-input case where n f equals 1, no false local minima exist. This is a nice theoretical result.

For Box-Jenkins special case - pure transfer function estimation - no false local minima if the input is white noise. For other inputs, false local minima can exist.

What about ARMAX, the general case? It's not known whether false local minima exist theoretically. However, practical experience suggests that the global minimum is usually found without too much difficulty.

For output error structures, convergence to false local minima is not uncommon in practice. This is why initialization matters a lot for these models.

The takeaway is that some model structures are better behaved than others. ARMA is the best. Output error is the worst. ARMAX is in between and usually works well in practice.

________________________________________
ADDITIONAL NOTES:

- SISO: Single-Input Single-Output
- Results assume S ∈ M (system in model class)
- ARMA: best case (all stationary points are global minima)
- Output Error: worst case (false local minima common)
:::

------------------------------------------------------------------------

## Initial Parameter Values

**Why good initial values matter:**

1.  Avoid converging to undesired local minima
2.  Faster convergence (fewer iterations needed)
3.  Shorter total computing time

. . .

**For physically parametrized models**:

-   Use physical insight for reasonable initial values (e.g. mass must be positive, damping ratios between 0 and 1, etc.k)
-   Allows monitoring and interaction with the search

. . .

**For linear black-box models**: Use a start-up procedure...

::: notes
**SCRIPT:**

Let me explain why good initial parameter values matter so much.

Newton-type methods like Gauss-Newton and Levenberg-Marquardt have good local convergence - they're fast near the minimum with quadratic convergence rate. But they have poor global convergence - not necessarily fast from far away. So the initial guess has a big impact.

It affects which minimum you converge to if multiple exist, how many iterations it takes to get there, and the total computational cost of the identification.

For physically parametrized models, this is easier. When parameters have physical meaning - mass, damping, time constants - you often have good intuition about reasonable ranges. Mass must be positive. Damping ratios are typically between 0 and 1. Time constants roughly match observed dynamics.

The benefits of physical parametrization are that it's easy to provide good initial guesses. You can monitor the iterative search to check if parameters are staying physical. And you can interact with the search - stop it if parameters become unrealistic.

For black-box models like ARMAX, parameters don't have direct physical meaning. We need systematic procedures to generate good initial values. That's what the next slides cover - a standard start-up procedure.

The bottom line is that effort spent on good initialization usually pays off. You get fewer total iterations, a better chance of finding the global minimum, and more confidence in the result.

________________________________________
ADDITIONAL NOTES:

- Local vs. global convergence: fast near minimum, slow from far away
- Physical parameters: mass, damping, time constants (have intuitive ranges)
- Black-box parameters: polynomial coefficients (no direct physical meaning)
:::

------------------------------------------------------------------------

## Start-up Procedure for Black-box Models

**General SISO model structure** (10.62):

$$y(t) = \frac{B(q)}{A(q)F(q)}u(t) + \frac{C(q)}{A(q)D(q)}e(t)$$

. . .

**Three-step initialization procedure:**

1.  **Estimate transfer function** $B/AF$ using IV method
    -   For open-loop systems: First estimate ARX model, use it to generate instruments
    -   Most often one of $A$ or $F$ is unity

. . .

2.  **Estimate equation noise model** from residuals

. . .

3.  **Estimate noise model** $C$ and/or $D$ using high-order AR model

::: notes
**SCRIPT:**

Let me walk you through the three-step initialization procedure for black-box models.

The general SISO model structure is y equals B over A F times u plus C over A D times e. This is equation 10.62 from the book. B over A F describes how the input affects the output. C over A D describes the noise characteristics. Different special cases include ARX, ARMAX, Output Error, and Box-Jenkins.

Step 1: Estimate the transfer function. The goal is to get initial estimates for A, B, and F. We use the Instrumental Variable method, which provides consistent estimates even with colored noise. For systems in open loop, first estimate an ARX model, use it to generate instruments, then apply the IV method to estimate B over A F. Usually one of A or F is taken as unity to make estimation simpler.

Step 2: Estimate the equation noise. After estimating the transfer function in step 1, compute residuals: e hat of t equals y of t minus B hat over A hat F hat times u of t. These residuals approximate the equation noise that needs to be modeled by C over A D.

Step 3: Estimate the noise model. Use the residuals to fit a high-order AR model. Choose the AR order as the sum of all model orders in C and D to balance computational effort.

Why does this work? If the true system is in the model set, this procedure brings the initial parameter estimate arbitrarily close to the true values as N increases. Then the iterative methods we discussed in Section 10.2 efficiently take us to the global minimum. This gives us a procedure that is globally convergent for large enough N!

________________________________________
ADDITIONAL NOTES:

- IV method: Instrumental Variables, consistent with colored noise
- ARX first step: simple, fast, provides good instruments
- Global convergence: initial estimate → true value as N → ∞
:::

------------------------------------------------------------------------

## Start-up for Nonlinear Black-box Models

**Nonlinear model structure** (10.61):

$$y(t) = g(u(t), y_k, \beta_k) + v(t), \quad v(t) = h(\gamma_k, e_k)$$

where $g$ and $h$ are nonlinear functions.

. . .

**Simple initialization approach**:

1.  "Seed" many fixed values of the nonlinear parameters $\beta_k$, $\gamma_k$
2.  For each seed, estimate the linear parameters by linear least squares
3.  Select the estimates with most significant values (relative to standard deviations)
4.  Use selected $\beta_k$ and $\gamma_k$ as initial values for Gauss-Newton

::: notes
**SCRIPT:**

Now for nonlinear black-box models, the initialization problem is harder.

The general nonlinear SISO model has the form y of t equals g of u, y k, beta k plus v of t. Here g is a nonlinear function of past inputs, outputs, and parameters beta k. Examples include Hammerstein-Wiener models with static nonlinearity before or after linear dynamics, neural network models, and polynomial NARX models.

The challenge is that unlike linear models, there's no systematic IV or ARX method to get good initial estimates. The parameter space is much more complex and nonconvex.

The seeding strategy is a grid-search approach. Here's how it works.

First, fix the nonlinear parameters. Choose many different values of beta k and gamma k - these are the parameters that appear nonlinearly in the model.

Second, for each fixed choice, the remaining parameters enter linearly. Estimate them by ordinary least squares. This is fast because it's just linear regression for each grid point.

Third, compute standard deviations for the estimated parameters. Select combinations where the estimates are statistically significant - large compared to their uncertainty. These are the best candidates.

Fourth, use the selected beta k and gamma k as initial values for Gauss-Newton to refine the full nonlinear optimization.

This is essentially a coarse grid search followed by local refinement. It's more computationally intensive than the linear case, but it's often necessary to avoid bad local minima in nonlinear problems.

________________________________________
ADDITIONAL NOTES:

- Hammerstein-Wiener: static nonlinearity + linear dynamics
- Grid search: try many combinations, expensive but thorough
- Statistical significance: parameter estimate / standard error > threshold
:::

4.  **Gauss-Newton refinement**: Use the selected parameter values as starting points for the full nonlinear Gauss-Newton optimization.

**Why this works:**

-   Linear LS is fast, so we can try many seeds
-   Statistically significant estimates are more likely to be near the true values
-   Provides multiple good starting points for the final optimization

**How many seeds?**

This depends on the problem. Typical ranges:

-   10-100 seeds for simple nonlinear models
-   100-1000 seeds for complex models like neural networks

The seeds can be chosen randomly or on a grid in the parameter space.

**Modern alternatives:**

For neural networks and deep learning, more sophisticated initialization schemes exist (Xavier initialization, He initialization, etc.), but the basic principle is similar.
:::

------------------------------------------------------------------------

## Initial Filter Conditions: The Problem

**The predictor** computes $\hat{y}(t)$ from past data using filters:

$$\hat{y}(t|\theta) = \underbrace{H_f(q, \theta)}_{\text{output filter}}y(t) + \underbrace{H_g(q, \theta)}_{\text{input filter}}u(t)$$

where $q$ = shift operator, $\theta$ = parameters

. . .

**Problem**: Filters are **recursive** - need past values $\varphi(0, \theta)$ to start

. . .

**At $t=1$**: Need $\varphi(0, \theta)$ - but we have **no data before** $t=1$!

. . .

**Example (ARX)**: To predict $y(1)$, need $y(0), y(-1), u(0)$ - not measured!

::: notes
**SCRIPT:**

Now we move to a different practical issue - initial filter conditions. This is less glamorous than global optimization, but it's important for getting good results in practice.

The predictor computes y hat of t from past data using filters. Look at this formula: y hat of t given theta equals H f of q theta times y of t, plus H g of q theta times u of t. H f is the output filter, and H g is the input filter. The q is the shift operator - it represents time delays. And theta contains all the parameters.

What do these filters do? They weight and combine past outputs and inputs to make predictions. For example, y hat of t might be 0.8 times y of t minus 1, plus 0.3 times u of t minus 1, plus 0.1 times u of t minus 2. The filter coefficients depend on theta.

Now here's the problem. These filters are recursive - they need past values to start. Specifically, they need phi of 0 theta - the initial state of the filter at time zero.

At t equals 1, when we want to make our first prediction, we need phi of 0 theta. But we have no data before t equals 1! Our measurements start at t equals 1. We don't have y of 0, y of negative 1, u of 0, or any of that.

Let me give you a concrete example with ARX models. To predict y of 1, you typically need y of 0, y of negative 1, and u of 0. But those aren't measured! So what do you do? You need some strategy for handling these missing initial conditions. That's what the next slides are about.

________________________________________
ADDITIONAL NOTES:

- H_f and H_g are pronounced "H f" and "H g"
- q is pronounced "cue" (shift operator)
- Phi is pronounced "fie" (rhymes with "pie")
- Recursive filters: output depends on past outputs (feedback)
- This is a practical implementation detail but affects results, especially for short data sets
:::

------------------------------------------------------------------------

## Four Approaches to Initial Conditions

| Approach | Method | Best for |
|----------|--------|----------|
| **1. Zero** | $\varphi(0, \theta) = 0$ | Most cases (default) ⭐ |
| **2. Match data** | Choose so $\hat{y}(t) = y(t)$ initially | When you don't want any initial errors messing up your first predictions |
| **3. As parameter** | Estimate $\varphi(0) = \eta$ with $\theta$ | Short data, slow dynamics |
| **4. Backforecast** | Run filters backwards | High-precision apps |

. . .

**Complexity**: Simple → → → → Complex

**Performance**: Good enough → → → → Optimal

::: notes
**SCRIPT:**

Now let's look at four approaches to handling initial filter conditions.

Approach 1 is zero initialization - set phi of 0 equal to 0. This is the default and works in most cases. It's simple and good enough for the vast majority of applications.

Approach 2 is to match the data. Choose initial conditions so that y hat of t equals y of t initially. Use this when you don't want any initial errors messing up your first predictions.

Approach 3 treats initial conditions as parameters. Estimate phi of 0 equals eta together with theta. This is useful for short data or slow dynamics where the initial transient matters.

Approach 4 is backforecasting - run the filters backwards from the end of the data. This is for high-precision applications where you need optimal handling of initial conditions.

The complexity goes from simple to complex. The performance goes from good enough to optimal. In practice, approach 1 works about 90% of the time. Only use sophisticated methods 3 or 4 when dealing with slow system dynamics or limited data. There's a trade-off between simplicity and handling transients.

________________________________________
ADDITIONAL NOTES:

- Zero init: simplest, works when N >> time constant
- Backforecasting: most accurate, computationally expensive
- Rule: use simple methods unless you have a specific reason not to
:::

------------------------------------------------------------------------

## When Do Initial Conditions Matter?

**Doesn't matter much** ✓ (zero init is fine):

- Long data records (N >> system time constant)

- Fast system dynamics

- ARX models with reasonable data

. . .

**Matters significantly** ⚠️ (use better methods):

- Short data records (N ≈ system time constant)

- Slow/poorly damped dynamics

- **Output Error (OE) models** 

- especially sensitive!

. . .

**Rule of thumb**: Data length > 10× time constant → zero init okay

::: notes
**SCRIPT:**

An important practical question is: when do initial conditions actually matter?

Initial conditions don't matter much in several cases. If you have long data records - N much greater than the system time constant - zero initialization is fine. The initial transient dies out quickly relative to the data length. If you have fast system dynamics, the effect disappears in a few samples. And for ARX models with reasonable data, it's generally not an issue.

But initial conditions matter significantly in other cases. If you have short data records where N is approximately equal to the system time constant, the initial transient is a significant fraction of your data. For slow or poorly damped dynamics, the transient takes a long time to die out. And Output Error models are especially sensitive because there's no noise model to absorb the initial transient errors.

Here's a practical rule of thumb: if your data length is greater than 10 times the system time constant, zero initialization is okay. Otherwise, consider using better initialization methods.

OE models are particularly problematic because all the modeling error goes into the transfer function estimate. There's no noise model to absorb initial transient errors. So be extra careful with OE models and short data.

________________________________________
ADDITIONAL NOTES:

- N >> τ: data length much greater than time constant
- OE models: no noise model, all errors affect transfer function estimate
- 10× rule: data > 10× time constant → zero init sufficient
:::

------------------------------------------------------------------------

## Summary of Section 10.5

**Key takeaways:**

-   Iterative methods converge to **local minima**, not necessarily global

-   **Multi-start strategy**: Use multiple initial values and compare results

-   **Two types of local minima**: Structural (in $\bar{V}(\theta)$) and Sample-induced (randomness in $V_N(\theta, Z^N)$)

-   **Good initialization is crucial** for efficiency and finding global minimum

. . .

**Practical recommendations:**

-   For black-box models: Use systematic start-up procedures (IV method → noise estimation)

-   For nonlinear models: Use seeding + grid search

-   Initial filter conditions: Usually zero initialization is sufficient

::: notes
**SCRIPT:**

Let me wrap up Section 10.5 with the key takeaways.

First, and this is crucial to understand: iterative methods converge to local minima, not necessarily the global minimum. This is a fundamental limitation of gradient-based optimization on non-convex problems. You can't escape it with a better algorithm - it's inherent to the problem structure.

Second, the multi-start strategy is your main weapon against local minima. Use multiple initial values, run the optimization from each one, and compare the results. Pick the one with the lowest criterion value. Yes, this multiplies your computational cost, but it's often the only way to have confidence you've found the global minimum.

Third, there are two types of local minima, and understanding which you're dealing with matters. Structural local minima are in V bar theta - they're part of the theoretical problem and won't go away with more data. Sample-induced local minima come from randomness in your finite dataset V N - they might disappear if you collect more data or use regularization.

Fourth, good initialization is crucial both for efficiency and for finding the global minimum. A good starting point can reduce the number of iterations dramatically and increase your chances of landing in the basin of attraction of the global minimum.

Now for practical recommendations. For black-box models like ARMAX or output error, use systematic start-up procedures. The textbook recommends starting with the IV method to get initial parameter estimates, then estimating the noise model. This gives you a good starting point that's in the right neighborhood.

For nonlinear models, use seeding plus grid search. Try multiple random seeds - 10 to 100 for simple models, up to 1000 for complex models like neural networks. You can choose seeds randomly or on a grid in parameter space.

Finally, for initial filter conditions, usually zero initialization is sufficient. Unless you have very short data, slow dynamics, or are using output error models, just set phi of 0 equals zero and don't worry about it. The initial transient dies out quickly relative to your data length.

That concludes our coverage of Chapter 10. We've learned how to compute parameter estimates for linear regressions using QR factorization, how to solve nonlinear problems using iterative methods like Levenberg-Marquardt, and how to handle practical issues like local minima and initial conditions. These are the computational workhorses of system identification.

________________________________________
ADDITIONAL NOTES:

- IV method: Instrumental Variables, a way to get consistent estimates even with colored noise
- Basin of attraction: region of parameter space that converges to a particular minimum
- ARMAX: AutoRegressive Moving Average with eXogenous input
- Zero initialization works when N >> system time constant (typically N > 10τ)
:::