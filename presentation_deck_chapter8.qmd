---
title: "System Identification: Chapter 8"
subtitle: "Section 8.5: Frequency-Domain Description of the Limit Model"
author: "System Identification: Theory for the User"
format:
  revealjs:
    theme: serif
    slide-number: true
    chalkboard: true
    preview-links: auto
    transition: slide
    html-math-method: mathjax
    css: assets/styles/styles.css
revealjs-plugins:
  - pointer
---

## Presentation Overview

**Chapter 8 - Convergence and Consistency**

Section 8.5:

**Linear Time-Invariant Models: Frequency-Domain Description of the Limit Model**

::: notes
Hello everyone! Before we jump to chapter 10, I'll be covering one section in chapter 8 that we missed last time. Today we're going to focus on Section 8.5.

**Chapter 8 - Convergence and Consistency**

This chapter is about what happens as our sample size N goes to infinity. Do our estimates converge? Do they converge to the true parameters?

**Section 8.5**: The full title is "Linear Time-Invariant Models: Frequency-Domain Description of the Limit Model." Let me break down each key term as we go through the next few slides.
:::

------------------------------------------------------------------------

## Presentation Overview {transition="none"}

**Chapter 8 - Convergence and Consistency**

Section 8.5:

**<mark>Linear</mark> Time-Invariant Models: Frequency-Domain Description of the Limit Model**

::: notes
**Linear**: These are systems where superposition applies. Double the input, you double the output. No saturation, no weird nonlinearities.
:::

---

## Presentation Overview {transition="none"}

**Chapter 8 - Convergence and Consistency**

Section 8.5:

**Linear <mark>Time-Invariant</mark> Models: Frequency-Domain Description of the Limit Model**

::: notes
**Time-Invariant**: The system doesn't evolve or drift over time. The same input produces the same output today as tomorrow. Parameters stay constant.
:::

---

## Presentation Overview {transition="none"}

**Chapter 8 - Convergence and Consistency**

Section 8.5:

**Linear Time-Invariant Models: <mark>Frequency-Domain</mark> Description of the Limit Model**

::: notes
And we're going to describe all of this in the frequency domain rather than the time domain. The reason is that it helps us understand where our model is accurate and where it struggles.
:::

---

## Presentation Overview {transition="none"}

**Chapter 8 - Convergence and Consistency**

Section 8.5:

**Linear Time-Invariant Models: Frequency-Domain Description of the <mark>Limit Model</mark>**

::: notes
The limit model is what we converge to as our sample size goes to infinity. It's the best we can do with our chosen model structure. And that's what we're describing today - how well does this limit model actually approximate the true system?
:::

---

## Presentation Overview {transition="none"}

**Chapter 8 - Convergence and Consistency**

Section 8.5:

**Linear Time-Invariant Models: Frequency-Domain Description of the Limit Model**

::: notes
So this section answers: as our estimates converge to θ*, how well does it actually approximate the true system? And we answer this using frequency-domain analysis.
:::


------------------------------------------------------------------------

## Section 8.5: Introduction

**The Big Question**: As data grows (N → ∞), our estimates converge to $\theta^*$. But:

- What is $\theta^*$ exactly?
- How well does it approximate the true system?
- Where does the model fit well vs. poorly?

. . .

**Our Approach**: Develop **frequency-domain tools** to answer these questions

Instead of one error number → see error **as a function of frequency**

. . .

**Context from Theorem 8.2** (as $N \to \infty$):

$$\theta^* = \arg\min_{\theta \in D_c} \bar{V}(\theta), \quad \bar{V}(\theta) = E[\varepsilon^2(t, \theta)]$$

where $\theta^*$ is the "best" parameters given our model structure

::: notes
**The Big Question**: As data grows (N → ∞), our estimates converge to θ*. This is the best set of parameters possible within our model structure. So we have three fundamental questions:

- What is θ* exactly?
- How well does it approximate the true system?
- Where does the model fit well vs. poorly?

[→ Click to reveal approach]

**Our Approach**: We're going to develop frequency-domain tools to answer these questions. Instead of just one error number, we'll see error as a function of frequency—broken down by which frequencies the model captures well and which it struggles with.

[→ Click for mathematical foundation]

**Context from Theorem 8.2**: Mathematically, θ* is defined as the parameter vector that minimizes V̄(θ), where V̄(θ) is the expected squared prediction error. That's our target.
:::

------------------------------------------------------------------------

## Two Cases: Model Match

**Two cases**:

. . .

**Case 1**: If $S \in \mathcal{M}$: $\theta^* = \theta_0$

- True system is in our model set
- We find the true parameters (true description, Theorem 8.3)

. . .

**Case 2**: If $S \notin \mathcal{M}$: Model differs from true system

- Model structure cannot perfectly represent true system
- $\theta^*$ is still best approximation within model class
- There will be inherent misfit

. . .

::: {.warning}
**In practice**: Almost always Case 2! Real systems are more complex than our models
:::

**This section**: Develop expressions for this misfit for linear time-invariant models

::: notes
**Two cases**: We need to understand two fundamental scenarios that can occur.

[→ Click to see Case 1]

**Case 1**: If $S \in \mathcal{M}$: $\theta^* = \theta_0$

The true system is actually in our model set—meaning our model structure is flexible enough to perfectly represent the truth. In this ideal scenario, we find the true parameters exactly. This is Theorem 8.3.

[→ Click to see Case 2]

**Case 2**: If $S \notin \mathcal{M}$: Model differs from true system

The true system is NOT in our model set. Our model structure can't perfectly represent the truth. But θ* is still the best approximation we can get within the class we chose. There will be inherent misfit.

[→ Click for practical reality]

**In practice**: Almost always Case 2! Real systems are much more complex than our models can capture.

**This section**: We're going to develop mathematical expressions for this misfit, specifically for linear time-invariant models in the frequency domain. This will let us characterize exactly where and how our model misses reality.
:::

------------------------------------------------------------------------

## Frequency Domain Benefits

**Why frequency domain?** Because it reveals WHERE your model struggles

. . .

**Advantage 1: Localization**

- **Time domain**: "Total error = 5.2" (no detail)
- **Frequency domain**: Error spectrum shows low-freq errors = 0.8, high-freq errors = 4.4
- Actionable diagnosis of which frequencies need improvement

. . .

**Advantage 2: Connect to control engineering**

- Engineers know Bode plots and frequency response
- Transfer function misfit visualized directly: $|G_0(e^{i\omega}) - G(e^{i\omega}, \theta)|$
- Weighting functions show which frequencies the criterion emphasizes

. . .

**Advantage 3: Model validation**

- Plot error spectrum vs. frequency → see where model is accurate
- Decide: add model complexity, or accept limitations in unimportant band?

::: notes
**Why frequency domain?** Because it reveals WHERE your model struggles. Let me show you three key advantages.

[→ Click to reveal Advantage 1]

**Advantage 1: Localization**

In the time domain, you get one blurry number: "total error = 5.2." But in the frequency domain, you can see exactly which frequencies contribute how much—low frequencies might be 0.8, high frequencies might be 4.4. This gives you actionable diagnosis of exactly which frequencies need improvement.

[→ Click to reveal Advantage 2]

**Advantage 2: Connect to control engineering**

Engineers already speak the language of frequency response and Bode plots. In the frequency domain, we can visualize the transfer function misfit directly. The weighting functions show which frequencies the criterion emphasizes.

[→ Click to reveal Advantage 3]

**Advantage 3: Model validation**

Plot the error spectrum versus frequency and you can see precisely where the model is accurate and where it struggles. Then you can decide: do I add model complexity to capture those frequencies, or are they unimportant for my application?
:::

------------------------------------------------------------------------

## Section 8.5 Roadmap

**How do we characterize model misfit in frequency domain?**

. . .

**Step 1**: Define a reference system (even for arbitrary/nonlinear systems)

- Use Wiener filter to get "best linear approximation"

- Computed from signal spectra (frequency-domain)

. . .

**Step 2**: Express prediction error criterion in frequency domain

- Use Parseval's theorem to convert time → frequency

- Shows which frequencies contribute to error

. . .

**Step 3**: Compare model to reference

- Explicit formulas showing misfit at each frequency

- Weighting functions reveal what criterion emphasizes

::: notes
**How do we characterize model misfit in frequency domain?** We're going to follow three logical steps to build up the complete picture.

[→ Click to reveal Step 1]

**Step 1**: Define a reference system (even for arbitrary/nonlinear systems)

We need a benchmark to compare our parametric model against. The Wiener filter gives us the "best linear approximation" for any data, including data from nonlinear systems. And crucially, it's computed directly from signal spectra—so we're already in the frequency domain.

[→ Click to reveal Step 2]

**Step 2**: Express prediction error criterion in frequency domain

We take our prediction error criterion and convert it from time domain to frequency domain using Parseval's theorem. This shows us which frequencies contribute to the error.

[→ Click to reveal Step 3]

**Step 3**: Compare model to reference

Finally, we compare our parametric model to the Wiener reference and derive explicit formulas showing the misfit at each frequency. Weighting functions reveal what the criterion emphasizes.
:::

------------------------------------------------------------------------

## Data from Arbitrary Systems

**Surprising result**: Approximation expressions apply to data from:

- Nonlinear systems

- Time-varying systems

- Arbitrary stochastic processes

. . .

**Only assumption**: Signals are jointly quasi-stationary (spectra exist)

::: notes
**Surprising result**: The approximation expressions we're about to derive apply to data from much more general systems than you might expect.

[→ Click to reveal the surprise]

Our approach works for:
- Nonlinear systems
- Time-varying systems
- Arbitrary stochastic processes

Why is this remarkable? The Wiener filter is based on second-order statistics—spectra and correlations. These are well-defined even for nonlinear and time-varying systems.

**Only assumption**: Signals are jointly quasi-stationary (spectra exist)

This just means the spectra don't drift too much over the observation period. That's a very weak requirement that most real systems satisfy. So even though we'll develop the theory for linear systems, the results apply much more broadly.
:::

------------------------------------------------------------------------

## The Wiener Filter: Optimal Predictor

**What we need**: A reference system to compare our model against

**Solution**: The Wiener filter - the **best possible linear predictor** given the data spectra

$$\hat{y}(t|t-1) = W_u(q)u(t) + W_y(q)y(t)$$

. . .

**Why this works**:

- Computed directly from signal spectra (frequency-domain quantities!)
- Even works for data from **arbitrary nonlinear systems**
- Only assumption: signals are quasi-stationary (spectra exist)

. . .

**Remarkable property**: The Wiener prediction error $e_0(t)$ is:

- **Uncorrelated** with all past inputs and outputs
- = Already extracted all predictable information from past!
- = True innovation (unavoidable noise)

::: notes
**What we need**: A reference system to compare our model against. Something that represents "the best we could possibly do" given the data.

**Solution**: The Wiener filter - the best possible linear predictor given the data spectra. The formula shows we use filters on past inputs (W_u) and past outputs (W_y) to predict the current output.

[→ Click to reveal why this works]

**Why this works**:
- Computed directly from signal spectra—frequency-domain quantities!
- Even works for data from arbitrary nonlinear systems
- Only assumption: signals are quasi-stationary (spectra exist)

[→ Click for the remarkable property]

**Remarkable property**: The Wiener prediction error $e_0(t)$ is:
- **Uncorrelated** with all past inputs and outputs
- This means it's already extracted all predictable information from the past
- What remains is true innovation—unavoidable noise

The Wiener filter is our perfect benchmark: it represents the best possible fit we could achieve if we had complete knowledge of the system.
:::

------------------------------------------------------------------------

## Standard Form Representation

**Define**:

- $H_0(q) = (1 - W_y(q))^{-1}$

- $G_0(q) = H_0(q)W_u(q)$

. . .

**Then rewrite as**:

$$y(t) = G_0(q)u(t) + H_0(q)e_0(t)$$

where $e_0(t)$ is uncorrelated sequence

. . .

**Interpretation**: Best linear time-invariant approximation of true system (based on second-order properties)

- $G_0(q)$: Transfer function from input to output
- $H_0(q)$: Noise model shaping filter

. . .

::: {.warning}
**Important caveat**: Depends on data spectra, not just system dynamics

If control excites different frequencies than identification did, this model may no longer be optimal
::::

::: notes
**Define**: We're going to rewrite the Wiener filter formulas to get standard form. These definitions might look abstract, but they're going to clean up the algebra beautifully.

[→ Click to reveal the rewrite]

**Then rewrite as**: $y(t) = G_0(q)u(t) + H_0(q)e_0(t)$

This is the standard form you already know—transfer function G_0 mapping input to output, plus noise model H_0 shaping the white noise e_0(t). The Wiener filter naturally falls into this familiar structure.

[→ Click for interpretation]

**Interpretation**: This represents the best linear time-invariant approximation of the true system, based on second-order properties (spectra and correlations). G_0 is the transfer function from input to output, H_0 is the noise model shaping filter.

[→ Click for important caveat]

**Important caveat**: This model depends on the data spectra, not just on the system dynamics. If your control application excites different frequencies than your identification input did, this model may no longer be optimal. Keep that in mind when using identified models for control design.
:::

------------------------------------------------------------------------

## Frequency-Domain Expression for $\bar{V}(\theta)$

**The Magic of Parseval's Theorem**:

$$\underbrace{\bar{V}(\theta) = \bar{E}\left[\frac{1}{2}\varepsilon^2(t, \theta)\right]}_{\text{Time domain: average error}} = \underbrace{\frac{1}{4\pi}\int_{-\pi}^{\pi} \Phi_{\varepsilon}(\omega, \theta)d\omega}_{\text{Frequency domain: integrated error spectrum}}$$

where $\Phi_{\varepsilon}(\omega, \theta)$ = spectrum of prediction errors

. . .

**What this means**:

- **Before**: Criterion was one number (total error)
- **After**: Criterion is **integral of error spectrum** across frequencies
- = Can see error contribution at each frequency ω!

. . .

**Visualization**:

- Plot $\Phi_{\varepsilon}(\omega, \theta)$ vs ω → see WHERE errors occur
- High values at low ω? → model misses slow dynamics
- High values at high ω? → model misses fast dynamics

::: notes
**The Magic of Parseval's Theorem**: This is Step 2 of our three-step roadmap. We're going to take our criterion and transform it from time domain to frequency domain.

V̄(θ) is our criterion—what we minimize when we identify the system. In the time domain, it's the average squared prediction error. But Parseval's theorem lets us rewrite it as an integral of the error spectrum across all frequencies.

The beauty: the frequency-domain version is exactly equal to the time-domain version. They're two different ways of measuring the same total error. But this frequency-domain version lets us see error as a FUNCTION of frequency.

[→ Click to reveal what this means]

**What this means**:
- **Before**: Criterion was one number (total error = 5.2, for example)
- **After**: Criterion is the integral of error spectrum across frequencies
- We can see the error contribution at each frequency ω!

[→ Click for visualization]

**Visualization**:

Instead of one blurry number, plot Φ_ε(ω, θ) versus ω and you can see:
- If high values appear at low ω, the model misses slow dynamics
- If high values appear at high ω, the model misses fast dynamics
- You get diagnostic information impossible from time domain alone
:::

------------------------------------------------------------------------

## Why This Is Useful

**Frequency-domain view shows**:

- Which frequencies contribute most to total error

- How weighting function affects the fit

- Where model is good vs. poor

. . .

::: {.key-insight}
**Power of frequency analysis**: Go from one error number to an error **function** over frequency. See exactly WHERE your model struggles!
:::

**Example**: Model might fit well at low frequencies but poorly at high frequencies

::: notes
**Frequency-domain view shows**: These three things tell us why this analysis is so powerful.

[→ Click to reveal the key insight]

**Power of frequency analysis**: Go from one error number to an error function over frequency. See exactly WHERE your model struggles!

[→ Click for example]

**Example**: Model might fit well at low frequencies but poorly at high frequencies.

Now, what do you do with this information? If errors are concentrated at high frequencies, you might need more model complexity to capture those dynamics. Or maybe for your application, high-frequency accuracy isn't critical—maybe you're doing slow control and only care about low-frequency behavior. Either way, the frequency-domain view tells you exactly what's going on and what to do about it.

This is the power of frequency-domain analysis: you get localization in frequency. Instead of one blurry error number, you get a complete picture of where your model works well and where it struggles.
:::

------------------------------------------------------------------------

## The True System (Assumption S1)

**Assume** true system is linear time-invariant (LTI):

$$y(t) = G_0(q)u(t) + H_0(q)e_0(t)$$

where:

- $G_0(q)$ = true transfer function (input → output)

- $H_0(q)$ = true noise model

- $e_0(t)$ = white noise with variance $\lambda_0$

. . .

**Next**: Derive explicit expression for $\Phi_{\varepsilon}(\omega, \theta)$ in terms of true system ($G_0$, $H_0$) and model ($G(\cdot, \theta)$, $H(\cdot, \theta)$)

::: notes
**Assume** true system is linear time-invariant (LTI). We make this simplifying assumption now, even though we showed earlier that the Wiener filter approach works for nonlinear systems. Here we assume the truth is linear.

The formula shows: y(t) = G₀(q)u(t) + H₀(q)e₀(t), where G₀ is the true transfer function, H₀ is the true noise model, and e₀ is white noise.

Our parametric model has the SAME form but with estimated parameters: G(q,θ), H(q,θ).

[→ Click for the next step]

**Next**: Derive explicit expression for the prediction error spectrum Φ_ε(ω, θ).

The key question we're about to answer: how does the misfit between (G₀, H₀) and (G, H) show up in the frequency domain? That's what the derivation will reveal.
:::

------------------------------------------------------------------------

## Deriving the Frequency-Domain Expression

**Step 1: Model's predictor**

$$\hat{y}(t|t-1, \theta) = \frac{H(\theta) - G(\theta)}{H(\theta)}y(t) + \frac{G(\theta)}{H(\theta)}u(t)$$

. . .

**Step 2: Prediction error**

$$\varepsilon(t, \theta) = y(t) - \hat{y}(t|t-1, \theta)$$

**Substitute true system** $y(t) = G_0u(t) + H_0e_0(t)$:

$$\varepsilon(t, \theta) = H^{-1}[H_0e_0(t) + (G_0 - G)u(t)]$$

. . .

**Key insight**: Prediction error has TWO sources:

1. $H_0e_0(t)$ = Unavoidable noise (can't predict white noise)
2. $(G_0 - G)u(t)$ = Model misfit (error between true and estimated transfer function)

::: notes
**Step 1: Model's predictor**

Our model uses G and H to make a one-step-ahead prediction. The formula shown is the standard predictor form.

[→ Click for Step 2]

**Step 2: Prediction error**

The prediction error is actual output minus predicted output. Now substitute the true system formula into this.

When we do that algebra, something interesting emerges: the error has TWO distinct parts.

**Key insight**: Prediction error has TWO sources:

1. **H₀e₀(t)** = The true system's noise. You can't predict white noise, so this part is irreducible error. No matter how good your model is, you can't eliminate this.

2. **(G₀ - G)u(t)** = The transfer function misfit. This is reducible: better parameters lead to smaller misfit. This is where we have room to improve.

This separation is key: it shows us what we can control (the G misfit) and what we can't (the noise).
:::

------------------------------------------------------------------------

## The Spectrum of Prediction Errors (Eq. 8.63)

**The fundamental frequency-domain expression**:

$$\Phi_{\varepsilon}(\omega, \theta) = \frac{1}{|H(e^{i\omega}, \theta)|^2}\left[\lambda_0|H_0(e^{i\omega})|^2 + |G_0(e^{i\omega}) - G(e^{i\omega}, \theta)|^2\Phi_u(\omega)\right]$$

. . .

**Interpretation**:

| Component | Meaning | How to improve |
|-----------|---------|----------------|
| $\lambda_0\|H_0\|^2$ | Unavoidable noise (can't eliminate) | Accept as limit |
| $\|G_0 - G\|^2\Phi_u(\omega)$ | **Model misfit at frequency ω** | Better θ or richer model |
| $1/\|H(\theta)\|^2$ | Whitening filter (inverse of noise model) | Better H estimate |

. . .

::: {.key-insight}
**The misfit term depends on input spectrum!** If $\Phi_u(\omega) = 0$, errors at that frequency don't matter. This is why **input design matters** in system identification.
:::

::: notes
**The fundamental frequency-domain expression**: This is Equation 8.63, one of the central results of Section 8.5.

The spectrum has three main components to understand:

1. **λ₀|H₀|²** = The unavoidable noise from the true system. No matter how good your model is, you can never eliminate this. It's the fundamental limit.

2. **|G₀ - G|²Φ_u(ω)** = The model misfit term. Notice it's multiplied by Φ_u(ω), the input spectrum. This means error contribution depends on what frequencies were present in your input! If the input had no energy at some frequency, model errors at that frequency don't contribute.

3. **1/|H|²** = A whitening filter scaling. This comes from how we defined the prediction error.

[→ Click for key insight]

**The misfit term depends on input spectrum!** If Φ_u(ω) = 0 at some frequency, errors there don't matter. This is why input design matters in system identification.

So the prediction error spectrum separates naturally: unavoidable noise plus reducible model misfit, both weighted by the input spectrum.
:::

------------------------------------------------------------------------

## Integrating to Get $\bar{V}(\theta)$ (Eq. 8.64)

**Apply Parseval's theorem**:

$$\bar{V}(\theta) = \frac{1}{4\pi}\int_{-\pi}^{\pi}\Phi_{\varepsilon}(\omega, \theta)d\omega$$

. . .

**Substitute Equation 8.63**:

$$\bar{V}(\theta) = \frac{1}{4\pi}\int_{-\pi}^{\pi}\frac{1}{|H(e^{i\omega}, \theta)|^2}\left[\lambda_0|H_0(e^{i\omega})|^2 + |G_0(e^{i\omega}) - G(e^{i\omega}, \theta)|^2\Phi_u(\omega)\right]d\omega$$

. . .

**This is Equation 8.64** - the frequency-domain expression for the asymptotic criterion

::: notes
**Apply Parseval's theorem**: We bring everything together now. V̄(θ) is the time-domain average squared prediction error. Parseval's theorem lets us write it as an integral of Φ_ε(ω, θ) over all frequencies.

[→ Click to substitute]

**Substitute Equation 8.63**: We plug in the spectrum formula we just derived. This gives us the complete expression for V̄(θ) as a frequency-domain integral.

[→ Click for the complete formula]

**This is Equation 8.64** - the frequency-domain expression for the asymptotic criterion. This is what gets minimized when we find θ*.

What's beautiful about this expression is that it makes everything explicit. You can see exactly how the criterion depends on:
- The true system G₀ and H₀
- Your model G and H
- The input spectrum Φ_u
- The noise level λ₀

Everything is laid out in the frequency domain, frequency by frequency. This is the complete picture of what happens during identification.
:::

------------------------------------------------------------------------

## Open Loop Case (Eq. 8.67-8.68)

**When input is designed (not feedback)**: $u(t) \perp e_0(t)$ (independent)

. . .

**Best case scenario**:

- Optimal $\theta^*$ makes $G(\theta^*) = G_0$ (transfer function is identifiable!)
- Can separate transfer function misfit from noise model misfit

**Simplified criterion** (Equation 8.68):

$$\bar{V}(\theta) = \lambda_0 + \frac{1}{4\pi}\int_{-\pi}^{\pi}\underbrace{|G_0(e^{i\omega}) - G(e^{i\omega}, \theta)|^2}_{\text{Transfer fn misfit}}\ \underbrace{Q^*(\omega)}_{\text{Weighting fn}}d\omega$$

. . .

**The weighting function** $Q^*(\omega)$ shows **which frequencies matter**:

$$Q^*(\omega) = \Phi_u(\omega) \cdot \frac{|H_0(e^{i\omega})|^2}{|H(e^{i\omega}, \theta^*)|^2}$$

- High $\Phi_u(\omega)$ → more excitation at ω → more weight
- Ratio of noise models → adjustment for colored noise

::: notes
**When input is designed (not feedback)**: u(t) ⊥ e₀(t) means input is independent of the noise. This is the open loop case—no controller feedback during identification.

[→ Click for best case scenario]

**Best case scenario**: In open loop, the optimal θ* makes G(θ*) = G₀. The transfer function is identifiable! We can separate transfer function misfit from noise model misfit.

**Simplified criterion** (Equation 8.68): Notice the structure—we have the transfer function misfit |G₀ - G|² weighted by Q*(ω) and integrated over frequency.

[→ Click for weighting function]

**The weighting function** Q*(ω) shows which frequencies matter:
- High Φ_u(ω) → more excitation at that frequency → more weight
- The ratio of noise models adjusts for colored noise

So Q*(ω) tells us: which frequencies matter most when we're fitting G to the true G₀? Frequencies with more input energy get more weight.
:::

------------------------------------------------------------------------

## Closed Loop Case (Eq. 8.69)

**When controller is in the loop**: $u(t) = f[\text{past } y(s), s \leq t]$

Since $y$ contains noise $e_0$, input correlates with noise: $u(t)$ depends on past $e_0(s)$

. . .

**Resulting expression** (Equation 8.69) - Much more complex!

$$\bar{V}(\theta) = \lambda_0 + \frac{1}{4\pi}\int_{-\pi}^{\pi}\left|\underbrace{G_0 - G}_{\text{G misfit}} + \underbrace{\left[\frac{1}{H_0} - \frac{1}{H}\right]c(\omega)}_{\text{H-G coupling term}}\right|^2Q^*(\omega)d\omega$$

. . .

**Key difference from open loop**:

- **Extra coupling term**: $\left[\frac{1}{H_0} - \frac{1}{H}\right]c(\omega)$
- Errors in $H$ affect $G$ estimate (and vice versa!)
- Can't separate transfer function from noise model estimation

. . .

::: {.warning}
**Closed-loop ID is harder**: Must estimate G and H jointly. Wrong H structure hurts G estimate.
:::

::: notes
**When controller is in the loop**: The input u(t) depends on past values of y(s). Since y contains noise e₀, there's a correlation between the input and the noise. This is fundamentally different from open loop.

[→ Click to see the resulting expression]

**Resulting expression** (Equation 8.69) - Much more complex!

The expression looks messy because of the coupling term. Notice the extra term: [1/H₀ - 1/H]c(ω). This captures the feedback coupling between input and noise.

[→ Click for key difference]

**Key difference from open loop**:

In open loop, you had only the G misfit term. Here, G and H errors couple together. Errors in your noise model H affect how you estimate the transfer function G, and vice versa.

**Closed-loop ID is harder**: You must estimate G and H jointly. Wrong H structure hurts your G estimate.

This is why closed-loop identification is more challenging. The controller introduces feedback coupling that makes it harder to get good estimates of both models simultaneously.
:::

------------------------------------------------------------------------

## Example 8.5: The Trade-off

**True system**:

$$y(t) = \frac{0.5q^{-1}}{1 - 1.5q^{-1} + 0.7q^{-2}}u(t) + \frac{1}{1 - 0.5q^{-1}}e(t)$$

**Key structural detail**:

- $G_0$ has poles at **1.5 and 0.7**
- $H_0$ has pole at **0.5**
- **Poles are different!**

. . .

**Model structures**:

| Model | G structure | H structure | Trade-off |
|-------|-------------|-------------|-----------|
| **OE** | Flexible | Fixed at 1 | Right G, wrong H |
| **ARX** | Constrained | Flexible | Coupled G and H |

. . .

**The Question**: Which wins at estimating $G_0$?

- OE can match G exactly but misses H structure
- ARX couples them: can't match both poles perfectly

::: notes
**True system**: Look at the formula and note the specific structure. The transfer function has poles at 1.5 and 0.7, and the noise model has a pole at 0.5. The poles are different—this is key!

[→ Click for model structures]

**Model structures**: We're going to compare two approaches:
- **OE**: Transfer function is flexible but assumes white noise (H = 1)
- **ARX**: More flexible noise model but forces G and H to share the same poles

This creates a trade-off. OE has the right G structure but wrong H. ARX has coupled G and H.

[→ Click for the question]

**The Question**: Which model structure gives a better estimate of G₀?

The answer depends on how the weighting function Q*(ω) interacts with the model errors at different frequencies. Let's see which one wins in practice.
:::

------------------------------------------------------------------------

## Example 8.5: Results - Bode Plot Comparison

![Bode plots showing |G₀(e^(iω))| and estimates from OE and ARX models](book_pictures/figures/figure 8.2.png){width=80%}

. . .

**The Winner: OE Model!** (dashed line ≈ solid line)

- Matches true $G_0$ nearly perfectly across ALL frequencies
- Even though OE has wrong noise model ($H = 1$), it still wins!

**The Loser: ARX Model** (dash-dot line ≠ solid line)

- Systematic errors, especially at resonance peaks
- ARX constraint forces poles of G and H to match
- Can't satisfy both pole structures simultaneously

. . .

::: {.key-insight}
**Lesson**: Having the right G structure > having the right H structure (for this example). But this isn't always true! It depends on the weighting function $Q^*(\omega)$.
:::

::: notes
**Look at the results**: The solid line is true G₀. The dashed line is OE, the dash-dot is ARX.

[→ Click to reveal winner]

**The Winner: OE Model!** It's nearly a perfect match to G₀ across all frequencies. Even though OE assumes white noise (H = 1), it still wins!

**The Loser: ARX Model** shows systematic error, especially at resonance peaks. ARX can't match both G₀ and H₀ simultaneously because of its structural constraint.

[→ Click for the lesson]

**Lesson**: Having the right G structure matters more than having the right H structure—at least for estimating G. But why does OE win despite having the wrong H?

The OE model can fit G₀ exactly with no constraints. ARX is forced to share poles between G and H, but the true system has different poles for each. So ARX makes a compromise that hurts both G and H estimates.

The key takeaway: model structure choice matters profoundly. Even with infinite data, wrong structure leads to biased estimates.
:::

------------------------------------------------------------------------

## Example 8.5: Weighting Function $Q^*(\omega)$

![Weighting function Q*(ω) showing frequency emphasis](book_pictures/figures/figure 8.3.png){width=80%}

. . .

**What this shows**:

- **Vertical axis**: Weighting $Q^*(\omega)$ - how much each frequency "counts"
- **Horizontal axis**: Frequency ω (log scale)
- Explains why OE and ARX get different results!

. . .

**Key insight**:

$$Q^*(\omega) = \Phi_u(\omega) \cdot \frac{|H_0(e^{i\omega})|^2}{|H(e^{i\omega}, \theta^*)|^2}$$

- OE has $H = 1$ → Different weighting than ARX
- The weighting determines which model fits better
- In this example: OE's weighting aligns well with fitting $G_0$

::: notes
**What this shows**: The weighting function Q*(ω) tells us how much each frequency counts in the identification criterion. Higher Q* means that frequency's errors contribute more to total error.

[→ Click for key insight]

**Key insight**: The weighting function is defined as Q*(ω) = Φ_u(ω) · |H₀|² / |H|².

Since OE and ARX have different H structures, they have different weighting functions. This explains why they achieve different results—they're emphasizing different frequencies!

[→ Click to wrap up]

The OE model's weighting aligns well with fitting G₀. The ARX weighting, combined with its structural constraint, leads to the systematic errors we saw.

**Final takeaway**: By analyzing things in the frequency domain, we can understand exactly what's happening. We can see which frequencies are well-fitted, which are poorly-fitted, and why. We can see how model structure affects the weighting function, and thus the quality of the estimate.

This frequency-domain analysis is a powerful tool for model validation and for choosing appropriate model structures for your application. This completes Section 8.5!
:::

------------------------------------------------------------------------