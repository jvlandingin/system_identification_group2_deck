---
title: "Chapter 10: Computing the Estimate"
subtitle: "System Identification: Theory for the User"
author: "Sections 10.1-10.2"
format:
  revealjs:
    theme: serif
    slide-number: true
    chalkboard: true
    preview-links: auto
    transition: slide
    html-math-method: mathjax
    css: styles.css
---

## Chapter Overview

In Chapter 7, we introduced three basic procedures for parameter estimation:

1.  **Prediction-error approach**: Minimize $V_N(\theta, Z^N)$ with respect to $\theta$

2.  **Correlation approach**: Solve equation $f_N(\theta, Z^N) = 0$ for $\theta$

3.  **Subspace approach**: For estimating state space models

. . .

**This chapter**: How to solve these problems numerically

------------------------------------------------------------------------

## The Numerical Problem

At time $N$, when the data set $Z^N$ is known:

-   $V_N$ and $f_N$ are ordinary functions of a finite-dimensional parameter vector $\theta$
-   This amounts to standard **nonlinear programming** and **numerical analysis**

. . .

**However**: The specific structure of parameter estimation problems makes specialized methods worthwhile

# Section 10.1 {background-color="#2d5986"}

## Linear Regressions and Least Squares

------------------------------------------------------------------------

## The Normal Equations

For linear regressions, the prediction is:

$$\hat{y}(t|\theta) = \varphi^T(t)\theta$$

where:

-   $\varphi(t)$ is the **regression vector**
-   $\theta$ is the **parameter vector**

::: notes
**Greek letter pronunciations:**

-   $\theta$ = "theta" (THAY-tah)
-   $\varphi$ = "phi" (fie, rhymes with "pie") or "varphi" for this variant
-   $\hat{y}$ = "y hat" (the hat denotes an estimate/prediction)
:::

------------------------------------------------------------------------

## Least Squares Solution

The prediction-error approach with quadratic norm gives the LS estimate:

$$\hat{\theta}_N^{LS} = R^{-1}(N)f(N)$$

where:

$$R(N) = \frac{1}{N}\sum_{t=1}^{N}\varphi(t)\varphi^T(t)$$

$$f(N) = \frac{1}{N}\sum_{t=1}^{N}\varphi(t)y(t)$$

::: notes
**What is the "quadratic norm"?**

A **norm** is a way to measure the "size" of something (like length of a vector).

A **quadratic norm** means we measure size by **squaring and summing**: - For a prediction error $e(t) = y(t) - \hat{y}(t|\theta)$ - The quadratic norm is: $\|e\|^2 = \sum_{t=1}^{N} e(t)^2 = \sum_{t=1}^{N} |y(t) - \hat{y}(t|\theta)|^2$

**Why "quadratic"?** - Because we square the errors (power of 2) - This is also called the **squared error** or **2-norm squared**

**Why use it?** 1. **Penalizes large errors more**: A large error gets squared, so it's heavily penalized 2. **Mathematically convenient**: Derivatives are easy to compute 3. **Unique minimum**: The squared error criterion has a single, well-defined minimum

**Notation note:** - $|x|^2$ for scalars means $x^2$ - $\|x\|^2$ for vectors means $x^T x = \sum_i x_i^2$

**In our case:** We minimize $V_N(\theta) = \sum_{t=1}^{N} |y(t) - \varphi^T(t)\theta|^2$ with respect to $\theta$
:::

------------------------------------------------------------------------

## Understanding the LS Estimate

$$\hat{y}(t|\theta) = \varphi^T(t)\theta$$

**What is** $\hat{\theta}_N^{LS}$?

The parameter vector that **minimizes the sum of squared errors**

. . .

**Components:**

-   $R(N)$: Sample **covariance matrix** of regressors $\varphi(t)$
-   $f(N)$: Sample **cross-covariance** of regressors $\varphi(t)$ and outputs $y(t)$

. . .

This solution is obtained by solving the system of linear equations

------------------------------------------------------------------------

## The Normal Equations (Alternative View)

The LS estimate $\hat{\theta}_N^{LS}$ solves:

$$R(N)\hat{\theta}_N^{LS} = f(N)$$

. . .

**These are called the *normal equations***

------------------------------------------------------------------------

## Numerical Challenge

**Problem**: The coefficient matrix $R(N)$ may be **ill-conditioned**

-   Particularly when dimension is high
-   Direct solution can be numerically unstable
-   Computing $R(N) = \frac{1}{N}\sum_{t=1}^{N}\varphi(t)\varphi^T(t)$ involves products of original data

. . .

**Solution**: Use matrix factorization techniques

-   Don't form $R(N)$ directly
-   Instead, construct a matrix $R$ such that $RR^T = R(N)$
-   This approach offers superior numerical stability

------------------------------------------------------------------------

## QR Factorization Definition

For an $n \times d$ matrix $A$:

$$A = QR$$

where:

-   $Q$ is $n \times n$ orthogonal: $QQ^T = I$
-   $R$ is $n \times d$ upper triangular

. . .

Various approaches exist: Householder transformations, Gram-Schmidt procedure, Cholesky decomposition

------------------------------------------------------------------------

## Understanding QR Factorization

**What does this decomposition do?**

Any matrix $A$ can be written as the product of:

1.  $Q$: An **orthogonal matrix** (preserves lengths and angles)
    -   Think of it as a rotation/reflection
    -   Property: $QQ^T = I$ (its transpose is its inverse)
2.  $R$: An **upper triangular matrix** (zeros below diagonal)
    -   Easy to solve systems with (back-substitution)

::: notes
**Understanding** $I$ (Identity Matrix): - $I$ is the identity matrix: a square matrix with 1's on the diagonal and 0's everywhere else - Example: $I = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix}$ for 3×3 case - It's like the number 1 for matrices: $I \cdot A = A$ and $A \cdot I = A$

**Understanding** $QQ^T = I$: - This means $Q^T$ is the inverse of $Q$ - so we get the inverse "for free" by just transposing! - Geometrically: $Q$ preserves lengths and angles (it's a pure rotation/reflection) - The columns of $Q$ are orthonormal: length 1 and perpendicular to each other - This is why $Q$ is "nice" - when you multiply by Q, you're essentially rotating/reflecting the space without stretching or distorting

**The R matrix:** - Being upper triangular is key - this means we can solve $Rx = b$ very efficiently using back-substitution (start from bottom row and work up) - **Example to mention**: For a 3×3 upper triangular matrix, the last equation has only one unknown, second-to-last has two unknowns (but we already know one), etc.

**Common question**: "Why is this better than just inverting?" Answer: Forming $R(N)$ involves multiplying matrices which squares the condition number. QR avoids this multiplication step entirely.

**The beauty**: QR gives us the "square root" of what we need, and working with the square root is numerically much more stable
:::

------------------------------------------------------------------------

## Why QR Factorization? The Core Problem

**Recall our goal:** Solve $R(N)\hat{\theta}_N^{LS} = f(N)$

. . .

**The issue with** $R(N)$:

$$R(N) = \frac{1}{N}\sum_{t=1}^{N}\varphi(t)\varphi^T(t) = \frac{1}{N}\mathbf{\Phi}^T\mathbf{\Phi}$$

. . .

Notice: $R(N)$ is formed by **multiplying** $\mathbf{\Phi}^T$ by $\mathbf{\Phi}$

This multiplication **squares the condition number**, amplifying numerical errors!

::: notes
**Key terminology to explain:**

**Condition number (**$\kappa$): - Measures how "sensitive" a matrix is to numerical errors - Think of it as: "How much do small errors in input get magnified in output?" - Low condition number (close to 1): Well-conditioned, stable - High condition number (e.g., 1000+): Ill-conditioned, unstable - Example: If $\kappa = 100$, a 1% error in data can become a 100% error in solution!

**Big** $\mathbf{\Phi}$ vs. small $\varphi(t)$: - $\varphi(t)$ (small phi): The regression vector at a **single time** $t$ (a column vector) - $\mathbf{\Phi}$ (big bold Phi): The **entire data matrix** stacking all regression vectors - $\mathbf{\Phi}$ has $N$ rows (one for each time point) - Each row of $\mathbf{\Phi}$ is $\varphi(t)^T$ (transposed regression vector) - So: $\mathbf{\Phi} = [\varphi(1)^T; \varphi(2)^T; ...; \varphi(N)^T]$ (stacked vertically)

**Why "squares" the condition number:**

-   When you compute $\mathbf{\Phi}^T\mathbf{\Phi}$, you're multiplying two matrices

-   This operation squares the condition number: $\kappa(R(N)) = \kappa(\mathbf{\Phi})^2$

-   Example: If $\mathbf{\Phi}$ has $\kappa = 100$, then $R(N)$ has $\kappa = 10,000$!

**Greek letter pronunciations:** - $\kappa$ = "kappa" (CAP-uh) - $\varphi$ = "phi" (fie, rhymes with "pie") - $\mathbf{\Phi}$ = "capital Phi" (same pronunciation, but refers to the matrix) - $\theta$ = "theta" (THAY-tah) - $\epsilon$ = "epsilon" (EP-sih-lon) - appears in error terms
:::

------------------------------------------------------------------------

## Numerical Example: Why Multiplication is Bad

Consider a simple case where data has condition number $\kappa = 100$

. . .

**Direct approach:**

-   Form $R(N) = \mathbf{\Phi}^T\mathbf{\Phi}$

-   Condition number of $R(N)$: $\kappa^2 = 10,000$

-   Relative error magnified by factor of 10,000!

------------------------------------------------------------------------

## Numerical Example: Why Multiplication is Bad

Consider a simple case where data has condition number $\kappa = 100$

**QR approach:**

-   Work with $R_1$ from QR factorization

-   Condition number of $R_1$: $\kappa = 100$

-   Relative error magnified only by factor of 100

. . .

**Result:** 100× improvement in numerical stability!

------------------------------------------------------------------------

## QR Factorization: The Key Insight

**Instead of computing** $R(N) = \mathbf{\Phi}^T\mathbf{\Phi}$ directly...

. . .

**Factor** $\mathbf{\Phi}$ itself:

$$\mathbf{\Phi} = QR_1$$

. . .

**Then:**

$$R(N) = \mathbf{\Phi}^T\mathbf{\Phi} = (QR_1)^T(QR_1) = R_1^T Q^T Q R_1 = R_1^T R_1$$

(using $Q^TQ = I$)

. . .

**Key point:** - We never form $\mathbf{\Phi}^T\mathbf{\Phi}$

-   we work with $R_1$ directly!

------------------------------------------------------------------------

## Concrete Example: 2×2 Case

Let's work through a small example:

$$\mathbf{\Phi} = \begin{bmatrix} 3 & 0 \\ 4 & 0 \\ 0 & 5 \end{bmatrix}$$

. . .

**Direct approach:** Form $R(N) = \mathbf{\Phi}^T\mathbf{\Phi}$

$$R(N) = \begin{bmatrix} 3 & 4 & 0 \\ 0 & 0 & 5 \end{bmatrix} \begin{bmatrix} 3 & 0 \\ 4 & 0 \\ 0 & 5 \end{bmatrix} = \begin{bmatrix} 25 & 0 \\ 0 & 25 \end{bmatrix}$$

------------------------------------------------------------------------

## Concrete Example: QR Approach - Setup {.small-font}

Same matrix: $\mathbf{\Phi} = \begin{bmatrix} 3 & 0 \\ 4 & 0 \\ 0 & 5 \end{bmatrix}$

. . .

We want to factor this as: $\mathbf{\Phi} = QR_1$

. . .

**What will we get?**

-   $Q$: An orthogonal matrix (preserves geometry)
-   $R_1$: An upper triangular matrix (easy to solve)

------------------------------------------------------------------------

## Concrete Example: QR Approach - Results {.small-font}

**QR factorization of** $\mathbf{\Phi} = \begin{bmatrix} 3 & 0 \\ 4 & 0 \\ 0 & 5 \end{bmatrix}$ **gives:**

$$Q = \begin{bmatrix} 0.6 & 0 & -0.8 \\ 0.8 & 0 & 0.6 \\ 0 & 1 & 0 \end{bmatrix}$$

. . .

$$R_1 = \begin{bmatrix} 5 & 0 \\ 0 & 5 \end{bmatrix}$$

------------------------------------------------------------------------

## QR Approach: Verification

**Check:** $R_1^T R_1$ equals $R(N)$:

$$R_1^T R_1 = \begin{bmatrix} 5 & 0 \\ 0 & 5 \end{bmatrix} \begin{bmatrix} 5 & 0 \\ 0 & 5 \end{bmatrix} = \begin{bmatrix} 25 & 0 \\ 0 & 25 \end{bmatrix} = R(N) \checkmark$$

. . .

**Key difference:**

We work with $R_1$ (entries \~5) not $R(N)$ (entries \~25)!

------------------------------------------------------------------------

## Analogy: Square Root

**Computing** $R(N)$ directly is like:

-   Squaring a number with errors: $(x + \epsilon)^2 = x^2 + 2x\epsilon + \epsilon^2$

-   The error term gets amplified!

. . .

**QR factorization** is like:

-   Finding the square root first: If $R(N) = 100$, work with $R_1 = 10$

-   Solving with smaller, better-conditioned numbers

-   Less amplification of errors

------------------------------------------------------------------------

## Analogy: Square Root

**Mathematical analogy:**

-   Instead of solving: $x^2 = 100$ (error grows), we solve: $x = 10$ (error controlled)

------------------------------------------------------------------------

## Applying QR to LS Estimation

Define matrices for the multivariable case:

$$\mathbf{Y}^T = [y^T(1) \; \cdots \; y^T(N)], \quad \mathbf{Y} \text{ is } Np \times 1$$

$$\mathbf{\Phi}^T = [\varphi(1) \; \cdots \; \varphi(N)], \quad \mathbf{\Phi} \text{ is } Np \times d$$

. . .

The LS criterion:

$$V_N(\theta, Z^N) = |\mathbf{Y} - \mathbf{\Phi}\theta|^2 = \sum_{t=1}^{N}|y(t) - \varphi^T(t)\theta|^2$$

------------------------------------------------------------------------

## Orthonormal Transformation Property

**Key insight**: The norm is invariant under orthonormal transformations

For any vector $v$ and orthonormal matrix $Q$ ($QQ^T = I$):

$$|Qv|^2 = |v|^2$$

. . .

**Why?** Because $|Qv|^2 = (Qv)^T(Qv) = v^TQ^TQv = v^Tv = |v|^2$

. . .

**Application to our problem:**

$$V_N(\theta) = |\mathbf{Y} - \mathbf{\Phi}\theta|^2 = |Q(\mathbf{Y} - \mathbf{\Phi}\theta)|^2$$

We can multiply by $Q$ without changing the criterion!

------------------------------------------------------------------------

## QR Factorization of Augmented Matrix

**Key idea**: Stack data matrix $\mathbf{\Phi}$ and output vector $\mathbf{Y}$ side-by-side

$$[\mathbf{\Phi} \; \mathbf{Y}] = QR$$

::: notes
**Why augment** $\mathbf{\Phi}$ with $\mathbf{Y}$?

-   We need to solve: minimize $|\mathbf{Y} - \mathbf{\Phi}\theta|^2$
-   **Clever trick**: Put both $\mathbf{\Phi}$ and $\mathbf{Y}$ into one big matrix
-   Augmented matrix: $[\mathbf{\Phi} \; \mathbf{Y}]$ has dimensions $(Np) \times (d+1)$
    -   First $d$ columns: the data matrix $\mathbf{\Phi}$

    -   Last column: the output vector $\mathbf{Y}$

**What happens when we do QR?**

-   We get $[\mathbf{\Phi} \; \mathbf{Y}] = QR$ \* $Q$ is $(Np) \times (Np)$ orthogonal

-   $R$ is $(Np) \times (d+1)$ upper triangular

-   Only the top $(d+1) \times (d+1)$ block of $R$ is non-zero - we call this $R_0$
:::

------------------------------------------------------------------------

## Structure of the QR Result

After factorization:

$$[\mathbf{\Phi} \; \mathbf{Y}] = QR, \quad R = \begin{bmatrix} R_0 \\ 0 \\ \vdots \\ 0 \end{bmatrix}$$

Only $R_0$ (the top block) matters - it's $(d+1) \times (d+1)$ and upper triangular

::: notes
**Understanding the structure:**

-   Original augmented matrix: $(Np) \times (d+1)$ - typically $Np \gg d+1$ (many more data points than parameters)
-   After QR: $R$ has the same dimensions $(Np) \times (d+1)$
-   Because $R$ is upper triangular and skinny (tall and narrow), most of it is zeros!
-   All the information is in the top $(d+1) \times (d+1)$ block, which we call $R_0$
-   The rest is just zeros (shown as 0, $\vdots$, 0)

**Why does this help?**

-   We don't need to store the huge $(Np) \times (d+1)$ matrix

-   Just store the small $(d+1) \times (d+1)$ block $R_0$

-   Massive computational savings!
:::

------------------------------------------------------------------------

## Decomposing $R_0$: Separating Data and Outputs

Partition $R_0$ to separate the $\mathbf{\Phi}$ part from the $\mathbf{Y}$ part:

$$R_0 = \begin{bmatrix} R_1 & R_2 \\ 0 & R_3 \end{bmatrix}$$

where:

-   $R_1$ is $d \times d$ (corresponds to $\mathbf{\Phi}$)
-   $R_2$ is $d \times 1$ (interaction between $\mathbf{\Phi}$ and $\mathbf{Y}$)
-   $R_3$ is scalar (corresponds to $\mathbf{Y}$)

::: notes
**Why partition** $R_0$ this way?

Remember: $[\mathbf{\Phi} \; \mathbf{Y}]$ had $d$ columns from $\mathbf{\Phi}$ and 1 column from $\mathbf{Y}$

So $R_0$ inherits this structure:

-   **First** $d$ columns come from $\mathbf{\Phi}$ → forms $R_1$ and the zero below it
-   **Last column** comes from $\mathbf{Y}$ → forms $R_2$ and $R_3$

**Block structure:**

```         
      ← d cols → ← 1 →
    ┌──────────┬─────┐ ↑
    │          │     │ d rows
    │    R₁    │ R₂  │ ↓
    ├──────────┼─────┤ ↑
    │    0     │ R₃  │ 1 row
    └──────────┴─────┘ ↓
```

-   $R_1$: $d \times d$ upper triangular
-   $R_2$: $d \times 1$ vector
-   Bottom-left: zero (because $R_0$ is upper triangular!)
-   $R_3$: $1 \times 1$ scalar
:::

------------------------------------------------------------------------

## How This Transforms the LS Criterion

Original criterion: $V_N(\theta) = |\mathbf{Y} - \mathbf{\Phi}\theta|^2$

After applying $Q^T$ (using $QQ^T = I$):

$V_N(\theta) = |Q^T(\mathbf{Y} - \mathbf{\Phi}\theta)|^2 = |Q^T[\mathbf{\Phi} \; \mathbf{Y}] \begin{bmatrix} -\theta \\ 1 \end{bmatrix}|^2$

. . .

$V_N(\theta) = |Q^T[\mathbf{\Phi} \; \mathbf{Y}] \begin{bmatrix} -\theta \\ 1 \end{bmatrix}|^2$

. . .

$V_N(\theta) = |R \begin{bmatrix} -\theta \\ 1 \end{bmatrix}|^2$

::: notes
**Step-by-step transformation:**

1.  **Start with**: $V_N(\theta) = |\mathbf{Y} - \mathbf{\Phi}\theta|^2$

2.  **Rewrite as**: $V_N(\theta) = |[\mathbf{\Phi} \; \mathbf{Y}] \begin{bmatrix} -\theta \\ 1 \end{bmatrix}|^2$

    -   Check: $[\mathbf{\Phi} \; \mathbf{Y}] \begin{bmatrix} -\theta \\ 1 \end{bmatrix} = \mathbf{\Phi}(-\theta) + \mathbf{Y}(1) = \mathbf{Y} - \mathbf{\Phi}\theta$ ✓

3.  **Apply orthogonal transformation** $Q^T$ (doesn't change the norm!): $V_N(\theta) = |Q^T[\mathbf{\Phi} \; \mathbf{Y}] \begin{bmatrix} -\theta \\ 1 \end{bmatrix}|^2$

4.  **Use QR factorization** $[\mathbf{\Phi} \; \mathbf{Y}] = QR$: $V_N(\theta) = |R \begin{bmatrix} -\theta \\ 1 \end{bmatrix}|^2$

5.  **Recall structure of** $R$: Since $R$ has the form $\begin{bmatrix} R_0 \\ 0 \\ \vdots \\ 0 \end{bmatrix}$, only the top block $R_0$ contributes!

This is why we factored the augmented matrix!
:::

------------------------------------------------------------------------

## Transformed Criterion (Final Form)

Since only $R_0$ is non-zero, and using the block structure:

$$V_N(\theta) = \left|\begin{bmatrix} R_1 & R_2 \\ 0 & R_3 \end{bmatrix} \begin{bmatrix} -\theta \\ 1 \end{bmatrix}\right|^2$$

$$= \left|\begin{bmatrix} -R_1\theta + R_2 \\ R_3 \end{bmatrix}\right|^2 = |R_2 - R_1\theta|^2 + |R_3|^2$$

::: notes
**Breaking down the matrix multiplication:**

$$\begin{bmatrix} R_1 & R_2 \\ 0 & R_3 \end{bmatrix} \begin{bmatrix} -\theta \\ 1 \end{bmatrix} = \begin{bmatrix} R_1(-\theta) + R_2(1) \\ 0(-\theta) + R_3(1) \end{bmatrix} = \begin{bmatrix} -R_1\theta + R_2 \\ R_3 \end{bmatrix}$$

**Taking the squared norm:**

$$\left|\begin{bmatrix} -R_1\theta + R_2 \\ R_3 \end{bmatrix}\right|^2 = |{-R_1\theta + R_2}|^2 + |R_3|^2 = |R_2 - R_1\theta|^2 + |R_3|^2$$

**Key insight:**

-   First term: $|R_2 - R_1\theta|^2$ **depends on** $\theta$ → we can minimize this!

-   Second term: $|R_3|^2$ **doesn't depend on** $\theta$ → just a constant

So minimizing the whole thing means minimizing $|R_2 - R_1\theta|^2$
:::

------------------------------------------------------------------------

## Finding the Minimum

To minimize: $V_N(\theta) = |R_2 - R_1\theta|^2 + |R_3|^2$

The minimum occurs when $|R_2 - R_1\theta|^2 = 0$, i.e.:

$$R_1\hat{\theta}_N = R_2$$

Minimum value: $V_N(\hat{\theta}_N) = |R_3|^2$

::: notes
**Why this is the minimum:**

-   $|R_3|^2$ is a constant (doesn't depend on $\theta$)
-   $|R_2 - R_1\theta|^2$ is always ≥ 0 (it's a squared norm!)
-   Best we can do: make $|R_2 - R_1\theta|^2 = 0$
-   This happens when $R_1\theta = R_2$

**Solving for** $\hat{\theta}_N$:

-   We need: $R_1\hat{\theta}_N = R_2$

-   Since $R_1$ is upper triangular, solve by **back-substitution** (easy!)

-   No need to invert $R_1$ explicitly

**The minimum value:**

-   When $\theta = \hat{\theta}_N$: first term becomes zero

-   So $V_N(\hat{\theta}_N) = 0 + |R_3|^2 = |R_3|^2$

-   This is the smallest possible least squares error for this problem

-   Bonus: $|R_3|$ tells us the quality of the fit!
:::

------------------------------------------------------------------------

## Summary: Why QR Works

**Step-by-step what we solved:**

Starting from the transformed criterion: $V_N(\theta) = |R_2 - R_1\theta|^2 + |R_3|^2$

1.  **Used all blocks** ($R_1$, $R_2$, $R_3$) to find the optimal $\theta$
2.  **Solved**: $R_1\hat{\theta}_N = R_2$ using back-substitution
3.  **Residual**: $|R_3|^2$ tells us the minimum achievable loss

------------------------------------------------------------------------

## Summary: Why QR Works

**Why this is better than normal equations:**

-   **Conditioning**: $\kappa(R_1) = \sqrt{\kappa(R(N))}$ — square root improvement!
-   **Stability**: Never compute $\mathbf{\Phi}^T\mathbf{\Phi}$ which squares the condition number
-   **Speed**: Back-substitution on triangular $R_1$ is fast and numerically stable

------------------------------------------------------------------------

## Initial Conditions Problem

**Key challenge in real applications**: The regression vector $\varphi(t)$ typically contains **shifted data**:

$$\varphi(t) = \begin{bmatrix} z(t-1) \\ z(t-2) \\ \vdots \\ z(t-n) \end{bmatrix}$$

. . .

**The issue**: When $t = 1$, we need $z(0), z(-1), \ldots, z(1-n)$ but we only have data for $t \geq 1$

**What do we do with these missing initial conditions?**

::: notes
**Shifted data** means you're using **past values** of a signal to predict or analyze **current values**.
:::

------------------------------------------------------------------------

## Example: ARX Model

**Concrete example**: What does shifted data look like in practice?

For ARX model with $n_a = n_b = n$:

$$z(t) = \begin{bmatrix} -y(t) \\ u(t) \end{bmatrix}$$

. . .

For AR model ($p$-dimensional process):

$$z(t) = -y(t)$$

::: notes
**ARX = Autoregressive with eXogenous input**

The regression vector for an ARX model contains past outputs and past inputs. This is a common real-world scenario where you're trying to predict the output based on recent history.

For an AR (autoregressive) model, you only look at past outputs—no external inputs.

Both are examples where the regression vector uses shifted (lagged) data.

**What does the notation mean?**

-   $n_a$ = order of the autoregressive part (how many past outputs you use)
-   $n_b$ = order of the exogenous input part (how many past inputs you use)
-   $n_a = n_b = n$ = both orders are equal to the same value $n$

**Concrete example** with $n_a = n_b = 2$:

Your regression vector would look like: $$\varphi(t) = \begin{bmatrix} -y(t-1) \\ -y(t-2) \\ u(t-1) \\ u(t-2) \end{bmatrix}$$

Where: - $y(t-1), y(t-2)$ are the **past 2 outputs** (the autoregressive part) - $u(t-1), u(t-2)$ are the **past 2 inputs** (the exogenous part)

The "$n_a = n_b = n$" notation is just a simplifying assumption—saying "we use the same number of past outputs as past inputs." In practice, you could have $n_a \neq n_b$.
:::

------------------------------------------------------------------------

## Initial Conditions: "Windowed" Data

**Typical structure** of regression vector $\varphi(t)$:

$$\varphi(t) = \begin{bmatrix} z(t-1) \\ \vdots \\ z(t-n) \end{bmatrix}$$

. . .

It consists of **shifted data** (possibly after trivial reordering)

::: notes
**What is "windowed" data?**

"Windowed" refers to taking a **sliding window** of past data points. At each time $t$, you look back at a fixed-size window of historical values:

-   At time $t=5$: your window contains $z(4), z(3), z(2), \ldots, z(5-n)$ (the last $n$ values)
-   At time $t=6$: your window shifts forward to contain $z(5), z(4), z(3), \ldots, z(6-n)$
-   And so on...

This is called "windowing" because you're looking through a **fixed-size window** that slides along the time axis.

**What is "trivial reordering"?**

Sometimes the regression vector doesn't contain the past values in a simple backward order. For example:

Instead of: $\varphi(t) = \begin{bmatrix} z(t-1) \\ z(t-2) \\ z(t-3) \end{bmatrix}$

You might have: $\varphi(t) = \begin{bmatrix} z(t-2) \\ z(t-1) \\ z(t-3) \end{bmatrix}$ (mixed order)

Or with multiple variables: $\varphi(t) = \begin{bmatrix} u(t-1) \\ y(t-1) \\ u(t-2) \\ y(t-2) \end{bmatrix}$ (inputs and outputs interleaved)

"Trivial reordering" means you can rearrange the entries to put them back in standard order—it doesn't change the mathematical meaning, just the order of the rows. The core idea is still: **past values arranged as a regression vector**.

**Bottom line:** Windowed data is just a structured way of organizing past values into a regression vector for LS estimation.
:::

------------------------------------------------------------------------

## The Initial Conditions Problem (Formal)

With structure $\varphi(t) = [z(t-1)^T \; \cdots \; z(t-n)^T]^T$:

$$R_{ij}(N) = \frac{1}{N}\sum_{t=1}^{N} z(t-i)z^T(t-ji)$$

. . .

**Problem**: If we only have data for $1 \leq t \leq N$, what about initial conditions for $t \leq 0$?

We can't compute $z(0), z(-1), \ldots, z(1-n)$ because they don't exist!

------------------------------------------------------------------------

## Two Approaches

**Approach 1: Start summation later** (Covariance method)

-   Start at $t = n+1$ instead of $t=1$
-   All sums involve only known data
-   Loses $n$ data points, but straightforward

. . .

**Approach 2: Prewindowing (zero padding)** (Autocorrelation method)

-   Replace unknown initial values by zeros
-   For symmetry: also replace trailing values by zeros ("postwindowing")
-   **Advantage**: Keeps all $N$ data points

. . .

**Key insight**: Approach 2 gives $R(N)$ a special **block Toeplitz structure**, leading to the **Yule-Walker equations**

::: notes
**Covariance method vs. Autocorrelation method:**

-   **Approach 1 (Covariance)**: Logically more natural—only use data you have. But you lose the first $n$ time steps.

-   **Approach 2 (Autocorrelation)**: Pads with zeros, which seems artificial. But it creates a special **Toeplitz matrix structure** that has computational advantages (faster algorithms like Levinson's algorithm).

**Block Toeplitz matrix:**

When you use prewindowing, $R(N)$ becomes a **block Toeplitz matrix**, meaning: $$R_{ij}(N) = R_{\tau}(N), \quad \tau = i - j$$

In other words, entries depend only on the difference between indices, not the indices themselves. This structure enables fast algorithms.

**When does it matter?**

When $N \gg n$ (lots of data compared to model order), the difference between the two approaches becomes **insignificant**. The padding effect washes out.
:::

------------------------------------------------------------------------

## Levinson Algorithm - Problem & Solution

**Problem:** When fitting AR models of different orders, solving the normal equations separately for each order is expensive

**Solution:** Exploit the **Toeplitz structure** to update solutions recursively

. . .

**Recursive update relationships:**

$$\hat{\theta}_k^{n+1} = \hat{\theta}_k^n + \rho_n \hat{\theta}_{n+1-k}^n, \quad k = 1, \ldots, n$$

$$\hat{\theta}_{n+1}^{n+1} = \rho_n, \quad V_{n+1} = V_n + \rho_n \alpha_n$$

where $\rho_n = -\alpha_n / V_n$ is a **reflection coefficient** capturing new information at order $n+1$

::: notes
**What are "orders" in AR models?**

An AR model of order $n$ means you're using the past $n$ data points to predict the current value:

- **Order 1 AR**: $y(t) = \theta_1 y(t-1) + \text{error}$ (use only 1 past value)

- **Order 2 AR**: $y(t) = \theta_1 y(t-1) + \theta_2 y(t-2) + \text{error}$ (use 2 past values)

- **Order 3 AR**: $y(t) = \theta_1 y(t-1) + \theta_2 y(t-2) + \theta_3 y(t-3) + \text{error}$ (use 3 past values)

Higher order = more parameters = potentially better predictions, but also more complexity.

**What does "Recursive update relationships" mean?**

Instead of solving completely new equations for each order, we use a formula that **updates** the old solution:

- "Recursive" means each new solution builds from the previous one

- "Update relationship" is the formula that connects them

- Think: Old solution + Correction term = New solution

The benefit: We don't throw away all our work when adding one more parameter!

**Simple explanation of the equations:**

The first equation: $\hat{\theta}_k^{n+1} = \hat{\theta}_k^n + \rho_n \hat{\theta}_{n+1-k}^n$

- $\hat{\theta}_k^{n+1}$: The **updated parameter** $k$ when we add order $n+1$

- $\hat{\theta}_k^n$: The **old parameter** $k$ from order $n$ (keep this!)

- $\rho_n \hat{\theta}_{n+1-k}^n$: The **correction term** (new information we learned)

- $\rho_n$: A **scaling factor** (reflection coefficient)

In words: **New parameter = Old parameter + (scaling factor) × (correction)**

The second equation has two parts:
- $\hat{\theta}_{n+1}^{n+1} = \rho_n$ ← The **newest parameter** added at order $n+1$
- $V_{n+1} = V_n + \rho_n \alpha_n$ ← The **error** decreases as we add more orders (usually)

**What is a "reflection coefficient"?**

The reflection coefficient $\rho_n = -\alpha_n / V_n$ measures **how much new information we gain** by adding order $n$:

- **Magnitude** $|\rho_n|$:
  - Small value (close to 0): Adding order $n$ barely helps; the new parameter is weak
  - Large value (close to 1): Adding order $n$ is critical; the new parameter is strong
  - Always bounded: $|\rho_n| < 1$ for stable systems

- **Interpretation**:
  - It's a **correlation measure**: How much does the new parameter correlate with existing ones?
  - Comes from the Toeplitz structure: In a structured problem, this single number captures all the information needed for the update
  - Also called **PACF** (Partial Autocorrelation Function) in time series analysis

- **Why "reflection"?**:
  - In signal processing, these coefficients represent **reflections in a transmission line**
  - The Lattice filter architecture (coming next) is literally shaped like a physical transmission line with reflections!
  - Historically coined this term because the problem came from studying electrical transmission

**Intuition summary:**
The Levinson algorithm says: "If you know the parameters for order $n$, you can compute order $n+1$ by adding a tiny adjustment that's proportional to the reflection coefficient. The coefficient tells you how important this new order is."
:::


------------------------------------------------------------------------

## Levinson Algorithm - Computational Advantage

**Key efficiency gains:**

-   **Going from order** $n$ to $n+1$: Only $O(n)$ operations (not $O(n^3)$!)
-   **Computing all orders 1 to** $n$: Total $O(n^2)$ operations

. . .

**Impact:**

-   Enables **model order selection** without solving from scratch each time
-   **Developed by Levinson (1947)** — classical workhorse in signal processing

------------------------------------------------------------------------

## Why Levinson Algorithm Matters

**Real-world scenario**: You're building an AR model but don't know the right order $n$

**Naive approach**: Try $n=1, n=2, n=3, ..., n=10$ separately
- Each requires solving the normal equations from scratch
- For each order: $O(n^3)$ operations
- Total: $10 \times O(n^3)$ = expensive!

**Levinson approach**: Build solutions incrementally
- Start with $n=1$ solution
- Use it to build $n=2$ solution (just $O(2)$ extra work)
- Use $n=2$ to build $n=3$ solution (just $O(3)$ extra work)
- Total: $O(1+2+3+...+10) = O(n^2)$ — **much faster!**

------------------------------------------------------------------------

## How Levinson Algorithm Works

**Core idea**: Build the solution for order $n+1$ from the solution for order $n$

**Key observation**: The Toeplitz structure means previous solutions contain useful information

**Update mechanism**:
1. Start with order $n$ solution: $\hat{\theta}_n$
2. Add a "correction term" proportional to the old solution
3. Scale by **reflection coefficient** $\rho_n$ which measures the new information at order $n+1$

**The recursion**:
$$\hat{\theta}_{n+1} = \begin{bmatrix} \hat{\theta}_n + \rho_n \text{(correction)} \\ \rho_n \text{(new parameter)} \end{bmatrix}$$

**Why it's fast**:
- Old solutions reused (don't need full QR each time)
- Only $O(n)$ new calculations per order increase
- Cumulative: $O(n^2)$ total instead of $O(n^3)$

------------------------------------------------------------------------

## Levinson Algorithm - A Concrete Intuition

**Think of it as building a "ladder" of models**:

- **Order 1**: Simple model with 1 parameter
- **Order 2**: Add parameter 2 using info from order 1
- **Order 3**: Add parameter 3 using info from orders 1 & 2
- ... and so on

Each step **reuses previous work** rather than starting from scratch. This is why adding one more order only costs $O(n)$ extra work, not $O(n^3)$.

**The reflection coefficient** $\rho_n$ acts like a **scaling factor**:
- If $\rho_n \approx 0$: Adding order $n$ gives little new information
- If $\rho_n \approx 1$: Adding order $n$ is critical for the fit

------------------------------------------------------------------------

## Lattice Filters

**What is a lattice filter?**

An alternative **network architecture** for computing the same Levinson recursion, but structured differently:

**How it works**: Instead of updating parameter vectors directly, it processes two parallel **error streams**:

-   **Forward error** $e_n(t)$: Predicting $y(t)$ from its past (standard prediction)
-   **Backward error** $f_n(t)$: "Predicting" past data from future (novel idea!)

. . .

**Key insight**: These two errors are **orthogonal** (independent) at different orders
$$\frac{1}{N}\sum_{t=1}^{N} e_n(t)e_{n-k}(t-k) = \begin{cases} V_n, & k = 0 \\ 0, & k \neq 0 \end{cases}$$

This orthogonality is **numerically stabilizing**.

------------------------------------------------------------------------

## Lattice Filters - Why They're Superior

**Reflection coefficients** $\rho_n$ (also called PACF):

-   **Bounded**: Always $|\rho_n| < 1$ (naturally prevents numerical overflow!)
-   **Interpretation**: Measure of how much order $n$ adds beyond previous orders
-   **Stability detector**: If $|\rho_n| \approx 1$ → system becoming unstable

. . .

**Advantages over standard Levinson**:

1.  **Better numerical stability**: Bounded coefficients prevent round-off errors from growing
2.  **Adaptive/real-time capable**: Can process data one sample at a time (streaming)
3.  **Applications**: Speech processing, Kalman filtering, adaptive signal processing

------------------------------------------------------------------------

## Comparing Levinson vs. Lattice Filters

| Aspect | Levinson | Lattice Filters |
|--------|----------|-----------------|
| **What it updates** | Parameter vector $\hat{\theta}_n$ | Error streams $e_n(t)$ and $f_n(t)$ |
| **Complexity** | $O(n^2)$ | $O(n^2)$ |
| **Numerical stability** | Good | Excellent (bounded coefficients) |
| **Real-time capable** | Not naturally | Yes (process sample-by-sample) |
| **Industry use** | Academic/theoretical | Speech, adaptive filtering, control |

. . .

**Bottom line**: Both solve the same problem recursively. Lattice filters trade computation for better stability and adaptability.

------------------------------------------------------------------------

## Data Tapering (Optional refinement)

To soften artifacts from zero padding:

-   Apply **tapering weights** to both ends of the data record
-   Reduces edge effects from the appended zeros
-   Used in conjunction with prewindowing for refinement

::: notes
**What is tapering?**

Instead of abrupt zero padding, you gradually taper the data to zero at the boundaries. This is like multiplying your data by a **window function** (like a Hann or Hamming window) that smoothly goes from 1 to 0 at the edges.

**Why use it?**

Zero padding is artificial—it introduces a discontinuity. When you abruptly multiply by zero at the boundary, you introduce spectral leakage and edge artifacts. Tapering softens this by gradually reducing the data rather than cutting it off abruptly.

**When to apply tapering:**

-   Use with prewindowing to reduce edge effects
-   Common in spectral estimation and signal processing
-   Typically applied as a **weighting function** applied to the data before processing

This is refinement-level detail and not critical for understanding the main concepts.
:::

------------------------------------------------------------------------

## Summary of Section 10.1

**Key takeaways:**

-   **Normal equations** provide the foundation for LS estimation

-   **QR factorization** offers superior numerical stability

-   **Why it works**: Better conditioning, triangular structure, avoids ill-conditioned matrix products

-   **Initial conditions** are handled by prewindowing or starting summation later

-   **Practical insight**: Implement via $R_0$ only, avoiding large matrix storage