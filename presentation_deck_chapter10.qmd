---
title: "System Identification: Chapter 10"
subtitle: "Sections 10.1, 10.2, 10.5: Computing the Estimate"
author: "System Identification: Theory for the User"
format:
  revealjs:
    theme: serif
    slide-number: true
    chalkboard: true
    preview-links: auto
    transition: slide
    html-math-method: mathjax
    css: assets/styles/styles.css
revealjs-plugins:
  - pointer
---

## Presentation Overview

**Chapter 10 - Computing the Estimate**

-   Section 10.1: Linear Regressions and Least Squares

-   Section 10.2: Numerical Solution by Iterative Search Methods

-   Section 10.5: Local Solutions and Initial Values

::: notes
Let's proceed now to chapter 10, Computing the Estimate.

The main question this chapter answers is: how do we actually compute parameter estimates?

We'll cover three main sections.

Section 10.1 covers Linear Regressions and Least Squares—for when we have linear problems we can solve directly.

Section 10.2 covers Numerical Solution by Iterative Search Methods—for nonlinear problems where we need to search for the solution.

Section 10.5 covers Local Solutions and Initial Values—practical issues like where to start and how to avoid getting stuck.
:::

------------------------------------------------------------------------

# Chapter 10: Computing the Estimate {background-color="#1a4d6b"}

------------------------------------------------------------------------

## Chapter 10 Overview

In Chapter 7, we introduced three basic procedures for parameter estimation:

1.  **Prediction-error approach**: Minimize $V_N(\theta, Z^N)$ with respect to $\theta$

2.  **Correlation approach**: Solve equation $f_N(\theta, Z^N) = 0$ for $\theta$

3.  **Subspace approach**: For estimating state space models

. . .

**This chapter**: How to solve these problems numerically

::: notes
Let's start with some context from Chapter 7.

In Chapter 7, we learned about three approaches to parameter estimation. The prediction-error approach minimizes criterion V_N(θ) (THAY-tah). The correlation approach solves equation f_N(θ) = 0. And the subspace approach is for state space models.

[→ Click for this chapter's focus]

Now, Chapter 7 told us what to do—minimize this, solve that—but not how to compute it on a computer. That's what Chapter 10 is about.
:::

------------------------------------------------------------------------

## The Numerical Problem

At time $N$, when the data set $Z^N$ is known:

-   $V_N$ and $f_N$ are ordinary functions of a finite-dimensional parameter vector $\theta$

-   This amounts to standard **nonlinear programming** and **numerical analysis**

. . .

**However**: The specific structure of parameter estimation problems makes specialized methods worthwhile

::: notes
At time N, when the data set Z^N is known, **V_N and f_N are ordinary functions of a finite-dimensional parameter vector θ** (THAY-tah). **This amounts to standard nonlinear programming and numerical analysis**—the kind of optimization problems we'd solve with Excel Solver, `optim()` in R, or `scipy.minimize()` in Python.

[→ Click]

**However**, the specific structure of parameter estimation problems makes specialized methods worthwhile. Generic optimization tools (like R's `optim()`, Python's `scipy.optimize`, or MATLAB's `fminunc`) work, but methods designed for this structure are faster and more reliable.
:::

------------------------------------------------------------------------

# Section 10.1: Linear Regressions and Least Squares {background-color="#2c5f77"}

------------------------------------------------------------------------

## The Normal Equations

For linear regressions, the prediction is:

$$\hat{y}(t|\theta) = \varphi^T(t)\theta$$

where:

-   $\varphi(t)$ is the **regression vector**

-   $\theta$ is the **parameter vector**

::: notes
Now we move to Section 10.1 on linear regressions.

For linear regressions, the prediction is ŷ(t|θ) = φᵀ(t)θ (fie-transpose times THAY-tah). This formula shows that the prediction is a weighted sum of the regression vector components.

φ(t) (fie) is the regression vector, and θ (THAY-tah) is the parameter vector.
:::

------------------------------------------------------------------------

## Least Squares Solution

The prediction-error approach with quadratic norm gives the LS estimate:

$$\hat{\theta}_N^{LS} = R^{-1}(N)f(N)$$

::: notes
Here's the least squares solution.

The prediction-error approach with quadratic norm gives the LS estimate: θ̂ = R⁻¹(N)f(N) (THAY-tah hat equals R inverse times f).

But what are R and f? Let's break them down...
:::

------------------------------------------------------------------------

## Least Squares Solution - The Components

$$\hat{\theta}_N^{LS} = R^{-1}(N)f(N)$$

. . .

**R(N)** — Sample covariance matrix of regressors:

$$R(N) = \frac{1}{N}\sum_{t=1}^{N}\varphi(t)\varphi^T(t)$$

. . .

**f(N)** — Sample cross-covariance of regressors and outputs:

$$f(N) = \frac{1}{N}\sum_{t=1}^{N}\varphi(t)y(t)$$

::: notes
Now let's break down R and f.

[→ Click for R(N)]

R(N) is the sample covariance matrix of regressors. This measures how the regression vectors φ (fie) correlate with each other across all time points.

[→ Click for f(N)]

f(N) is the sample cross-covariance of regressors and outputs. This measures how the regressors φ (fie) correlate with the outputs y.

This solution is analytical—but computing it numerically can be tricky, as we'll see next.
:::

------------------------------------------------------------------------

## The Normal Equations (Alternative View)

The LS estimate $\hat{\theta}_N^{LS}$ solves:

$$R(N)\hat{\theta}_N^{LS} = f(N)$$

. . .

**These are called the *normal equations***

::: notes
Here's an alternative way to express the LS estimate.

The normal equations say R(N) times θ̂ (THAY-tah hat) equals f(N).

[→ Click for why "normal"]

The term "normal" comes from geometry—the solution makes the error vector perpendicular (normal) to the space of regressors. This is a standard linear system: Ax = b.
:::

------------------------------------------------------------------------

## Numerical Challenge

**Problem**: The coefficient matrix $R(N)$ may be **ill-conditioned**

-   Particularly when dimension is high

-   Direct solution can be numerically unstable

. . .

::: {.warning}
**The core issue**: Matrix multiplication **squares the condition number** κ

- Example: κ(Φ) = 100 → κ(R(N)) = 10,000 (100× worse!)
:::

. . .

**Solution**: Use **QR factorization** for superior numerical stability

::: notes
Here's a numerical challenge with the direct approach.

The problem is that the coefficient matrix R(N) may be ill-conditioned—particularly when dimension is high. Direct solution can be numerically unstable.

[→ Click for the core issue]

The core issue is that matrix multiplication squares the condition number κ (CAP-uh). If κ of Φ is 100, then κ of R(N) becomes 10,000—that's 100 times worse!

[→ Click for solution]

The solution is to use QR factorization for superior numerical stability.
:::

------------------------------------------------------------------------

## QR Factorization Definition

For an $n \times d$ matrix $A$:

$$A = QR$$

where:

-   $Q$ is $n \times n$ orthogonal: $QQ^T = I$

-   $R$ is $n \times d$ upper triangular

. . .

Various approaches exist: Householder transformations, Gram-Schmidt procedure, Cholesky decomposition

::: notes
Here's the definition of QR factorization.

For an n × d matrix A, we write A = QR where Q is n × n orthogonal (QQᵀ = I) and R is n × d upper triangular.

[→ Click for implementation methods]

Various approaches exist: Householder, Gram-Schmidt, Cholesky. Standard libraries like R's `qr()` or NumPy's `numpy.linalg.qr()` handle this for us.
:::

------------------------------------------------------------------------

## Understanding QR Factorization

**What does this decomposition do?**

Any matrix $A$ can be written as the product of:

1.  $Q$: An **orthogonal matrix** (preserves lengths and angles)
    -   Think of it as a rotation/reflection
    -   Property: $QQ^T = I$ (its transpose is its inverse)
2.  $R$: An **upper triangular matrix** (zeros below diagonal)
    -   Easy to solve systems with (back-substitution)

::: notes
**What does this decomposition do?** The slide explains both components—Q preserves geometry (like rotation/reflection), R is triangular for easy solving.

Now let's see a concrete example of why QR factorization gives better numerical stability.
:::

------------------------------------------------------------------------

## Numerical Example: Why Multiplication is Bad

Consider a simple case where data has condition number $\kappa = 100$

. . .

**Direct approach:**

-   Form $R(N) = \mathbf{\Phi}^T\mathbf{\Phi}$

-   Condition number of $R(N)$: $\kappa^2 = 10,000$

-   Relative error magnified by factor of 10,000!

::: notes
Here's a concrete numerical example.

Consider a simple case where data has condition number κ (CAP-uh) = 100.

[→ Click for direct approach]

With the direct approach, we form R(N) = ΦᵀΦ. The condition number of R(N) becomes κ² = 10,000. Relative error is magnified by a factor of 10,000!

That means a tiny 0.01% data error can become a 100% solution error.
:::

------------------------------------------------------------------------

## Numerical Example: Why Multiplication is Bad

Consider a simple case where data has condition number $\kappa = 100$

**QR approach:**

-   Work with $R_1$ from QR factorization

-   Condition number of $R_1$: $\kappa = 100$

-   Relative error magnified only by factor of 100

. . .

**Result:** 100× improvement in numerical stability!

::: notes
Now compare to the QR approach.

With QR, we work with R₁ from QR factorization. The condition number stays at κ (CAP-uh) = 100—it doesn't get squared!

[→ Click for the result]

The result is a 100× improvement in numerical stability. This is huge—the difference between reliable answers and garbage from rounding errors.

This is why QR factorization is the standard method in practice.
:::


------------------------------------------------------------------------

## QR Factorization: The Key Insight

**Instead of computing** $R(N) = \mathbf{\Phi}^T\mathbf{\Phi}$ directly...

. . .

**Factor** $\mathbf{\Phi}$ itself:

$$\mathbf{\Phi} = QR_1$$

. . .

**Then:**

$$R(N) = \mathbf{\Phi}^T\mathbf{\Phi} = (QR_1)^T(QR_1) = R_1^T Q^T Q R_1 = R_1^T R_1$$

(using $Q^TQ = I$)

. . .

**Key point:**

-   We never form $\mathbf{\Phi}^T\mathbf{\Phi}$

-   We work with $R_1$ directly!

::: notes
Here's the key insight behind QR factorization.

Instead of computing R(N) = ΦᵀΦ directly, which squares κ (CAP-uh)...

[→ Click for the factorization]

We factor Φ (big PHI) itself as Φ = QR₁, where Q is orthogonal and R₁ is triangular.

[→ Click for the derivation]

Then R(N) = R₁ᵀR₁. We use QᵀQ = I to simplify.

[→ Click for the key point]

The key point is we never form ΦᵀΦ—we work with R₁ directly. R₁ has the square root of κ, not the squared version.
:::


------------------------------------------------------------------------

## Concrete Example: 2×2 Case

Let's work through a small example:

$$\mathbf{\Phi} = \begin{bmatrix} 3 & 0 \\ 4 & 0 \\ 0 & 5 \end{bmatrix}$$

. . .

**Direct approach:** Form $R(N) = \mathbf{\Phi}^T\mathbf{\Phi}$

$$R(N) = \begin{bmatrix} 3 & 4 & 0 \\ 0 & 0 & 5 \end{bmatrix} \begin{bmatrix} 3 & 0 \\ 4 & 0 \\ 0 & 5 \end{bmatrix} = \begin{bmatrix} 25 & 0 \\ 0 & 25 \end{bmatrix}$$

::: notes
Let me work through a concrete example.

The data matrix Φ (big PHI) is a 3×2 matrix with entries 3, 4, and 5.

[→ Click for direct approach]

With the direct approach, we form R(N) = ΦᵀΦ. We get 25s on the diagonal.

Notice: Numbers went from 3-5 in Φ to 25 in R(N). That's the squaring effect!
:::


------------------------------------------------------------------------

## Concrete Example: QR Approach - Setup {.small-font}

Same matrix: $\mathbf{\Phi} = \begin{bmatrix} 3 & 0 \\ 4 & 0 \\ 0 & 5 \end{bmatrix}$

. . .

We want to factor this as: $\mathbf{\Phi} = QR_1$

. . .

**What will we get?**

-   $Q$: An orthogonal matrix (preserves geometry)

-   $R_1$: An upper triangular matrix (easy to solve)

::: notes
Now let's use QR on the same matrix.

We want to factor Φ = QR₁.

[→ Click for what we'll get]

What do we get? Q is an orthogonal matrix that preserves lengths and angles, like a rotation. R₁ is upper triangular with zeros below the diagonal, easy to solve.

The key difference is we're not squaring anything—just rearranging Φ into a different form.
:::


------------------------------------------------------------------------

## Concrete Example: QR Approach - Results {.small-font}

**QR factorization of** $\mathbf{\Phi} = \begin{bmatrix} 3 & 0 \\ 4 & 0 \\ 0 & 5 \end{bmatrix}$ **gives:**

$$Q = \begin{bmatrix} 0.6 & 0 & -0.8 \\ 0.8 & 0 & 0.6 \\ 0 & 1 & 0 \end{bmatrix}$$

. . .

$$R_1 = \begin{bmatrix} 5 & 0 \\ 0 & 5 \end{bmatrix}$$

::: notes
Here are the QR factorization results.

Q is a 3×3 orthogonal matrix. We can verify QᵀQ = I.

[→ Click for R₁]

R₁ is upper triangular with 5s on the diagonal.

Compare: The direct approach gave us 25s. QR gives us 5s—the square root. Much better conditioning!
:::


------------------------------------------------------------------------

## QR Approach: Verification

**Check:** $R_1^T R_1$ equals $R(N)$:

$$R_1^T R_1 = \begin{bmatrix} 5 & 0 \\ 0 & 5 \end{bmatrix} \begin{bmatrix} 5 & 0 \\ 0 & 5 \end{bmatrix} = \begin{bmatrix} 25 & 0 \\ 0 & 25 \end{bmatrix} = R(N) \checkmark$$

. . .

**Key difference:**

We work with $R_1$ (entries \~5) not $R(N)$ (entries \~25)!

::: notes
Let's verify our QR approach gives the right answer.

Check: R₁ᵀR₁ should equal R(N). Yes! 5×5 = 25. ✓

[→ Click for the key difference]

Here's the key difference. With the QR approach, we work with R₁ (entries around 5). With the direct approach, we work with R(N) (entries around 25).

Same final answer, but much better numerical stability along the way.
:::

------------------------------------------------------------------------

## QR Factorization in R: Setup and Phi Matrix

```{.r code-line-numbers="|1-5|7-8"}
# Example: Simple linear regression
set.seed(123)
N <- 10
x <- rnorm(N)
y <- 3 * x + 2 + rnorm(N, sd = 0.4)

# Design matrix Phi: [intercept, x]
Phi <- cbind(1, x)
```

. . .

**What Phi looks like** (10×2 matrix, first 5 rows):
```{.r}
head(Phi, 5)
```
```
     [,1]       [,2]
[1,]    1 -0.5604756
[2,]    1  1.7150650
[3,]    1  0.4609162
[4,]    1 -1.2650612
[5,]    1 -0.6868529
```

::: notes
Let's see QR factorization in R.

For setup, we generate data following y = 3x + 2, plus some noise. We build our design matrix Φ (big PHI) with an intercept column (all 1s) and the x values.

[→ Click to see Φ]

What does Φ look like? It's 10×2—column 1 is all 1s for the intercept, column 2 is our x values. This is the standard setup for simple linear regression.
:::

------------------------------------------------------------------------

## QR Factorization in R: Decomposition

```{.r}
# QR decomposition
qr_decomp <- qr(Phi)
Q <- qr.Q(qr_decomp)
R <- qr.R(qr_decomp)
```

. . .

**What Q looks like** (first 4 of 10 rows):
```
           [,1]       [,2]
[1,] -0.3162278 -0.2428765
[2,] -0.3162278  0.5440482
[3,] -0.3162278  0.1995752
[4,] -0.3162278 -0.5073257
```

::: notes
Now we do the QR decomposition.

In R, `qr()` computes the factorization, `qr.Q()` extracts Q, and `qr.R()` extracts R.

[→ Click for Q matrix]

What does Q look like? The first 4 rows are shown—decimal values from the orthogonalization process.
:::

------------------------------------------------------------------------

## QR Factorization in R: Verifying Orthogonality

**Verify Q is orthogonal:** $Q^TQ = I$
```{.r}
round(t(Q) %*% Q, 10)
```
```
     [,1] [,2]
[1,]    1    0
[2,]    0    1
```

The identity matrix—Q is orthogonal! ✓

::: notes
Let's verify Q is orthogonal.

We check that QᵀQ = I. We get 1s on the diagonal, 0s off-diagonal—the identity matrix.

This confirms Q preserves lengths and doesn't amplify errors.
:::

------------------------------------------------------------------------

## QR Factorization in R: The R Matrix

**What R looks like** (2×2 upper triangular):
```{.r}
print(R)
```
```
          [,1]      [,2]
[1,] -3.162278 0.5914424
[2,]  0.000000 2.7595206
```

Note: Bottom left is exactly zero (upper triangular). Entries are ~3.

::: notes
Now let's look at R.

R is 2×2 upper triangular—bottom left is exactly zero. Entries are around 3 and 2.7.
:::

------------------------------------------------------------------------

## QR Factorization in R: Comparison with Direct Approach

**Direct approach:** $\Phi^T\Phi$
```
          [,1]      [,2]
[1,] 10.000000  1.871398
[2,]  1.871398  7.617174
```

. . .

**Verify:** $R^TR = \Phi^T\Phi$
```
        [,1]     [,2]
[1,] 10.000000 1.871398
[2,]  1.871398 7.617174
```

Same result! But we work with R (values ~3), not $\Phi^T\Phi$ (values ~10)

::: notes
Let's compare with the direct approach.

The direct approach ΦᵀΦ gives values around 10 and 7—bigger numbers.

[→ Click to verify]

We verify that RᵀR = ΦᵀΦ. Same result! But we work with R (entries around 3) not ΦᵀΦ (entries around 10). Smaller numbers mean less error amplification.
:::

------------------------------------------------------------------------

## QR Factorization in R: Solution

**Solve for theta:**
```{.r}
theta <- solve(R, t(Q) %*% y)
print(theta)
```
```
         [,1]
[1,] 1.897726  # Intercept (true: 2)
[2,] 3.219099  # Slope (true: 3)
```

. . .

**Compare with direct approach:**
```{.r}
theta_direct <- solve(t(Phi) %*% Phi, t(Phi) %*% y)
print(theta_direct)
```
```
         [,1]
[1,] 1.897726  # Same!
[2,] 3.219099  # Same!
```

Same answer, but QR has better numerical stability!

::: notes
Now let's solve for θ (THAY-tah).

We solve Rθ = Qᵀy. The intercept is 1.9 (true: 2), the slope is 3.2 (true: 3)—close estimates!

[→ Click to compare with direct approach]

Comparing with the direct approach, we get the same answer—same intercept, same slope. They're mathematically equivalent.

Same answer, but QR has better numerical stability! For this small problem both work fine, but for large ill-conditioned problems, the direct approach can fail while QR still works.
:::


------------------------------------------------------------------------

## Applying QR to LS Estimation

Define matrices for the multivariable case:

$$\mathbf{Y}^T = [y^T(1) \; \cdots \; y^T(N)], \quad \mathbf{Y} \text{ is } Np \times 1$$

$$\mathbf{\Phi}^T = [\varphi(1) \; \cdots \; \varphi(N)], \quad \mathbf{\Phi} \text{ is } Np \times d$$

. . .

The LS criterion:

$$V_N(\theta, Z^N) = |\mathbf{Y} - \mathbf{\Phi}\theta|^2 = \sum_{t=1}^{N}|y(t) - \varphi^T(t)\theta|^2$$

::: notes
Now let's apply QR to the full LS problem.

Y (big Y) stacks all outputs from t=1 to N. It's Np × 1.

Φ (big PHI) stacks all regression vectors φ (fie). It's Np × d.

[→ Click for the LS criterion]

The LS criterion is V_N(θ) = |Y − Φθ|²—the sum of squared prediction errors. Our goal is to find θ (THAY-tah) that minimizes this.
:::


------------------------------------------------------------------------

## Orthonormal Transformation Property

**Key insight**: The norm is invariant under orthonormal transformations

For any vector $v$ and orthonormal matrix $Q$ ($QQ^T = I$):

$$|Qv|^2 = |v|^2$$

. . .

**Why?** Because $|Qv|^2 = (Qv)^T(Qv) = v^TQ^TQv = v^Tv = |v|^2$

. . .

**Application to our problem:**

$$V_N(\theta) = |\mathbf{Y} - \mathbf{\Phi}\theta|^2 = |Q(\mathbf{Y} - \mathbf{\Phi}\theta)|^2$$

We can multiply by $Q$ without changing the criterion!

::: notes
Here's a key property we need.

The norm is invariant under orthonormal transformations. Multiplying a vector by Q doesn't change its length.

[→ Click for why]

Why? The proof is on the slide. The key step is that Q-transpose times Q equals the identity, so they cancel out.

[→ Click for application]

How does this apply to our problem? We can multiply our error vector by Q without changing the loss function. This is the key trick that makes QR work!
:::


------------------------------------------------------------------------

## QR Factorization of Augmented Matrix

**Key idea**: Stack $\mathbf{\Phi}$ and $\mathbf{Y}$ side-by-side, then factor

$$[\mathbf{\Phi} \; \mathbf{Y}] = QR$$

. . .

**What does this look like?**

$$\begin{bmatrix} \varphi^T(1) & y(1) \\ \varphi^T(2) & y(2) \\ \vdots & \vdots \\ \varphi^T(N) & y(N) \end{bmatrix} \quad \leftarrow \text{Each row: one time point}$$

. . .

**Why?** Transforming by $Q$ doesn't change the loss function!

::: notes
Here's a clever trick.

The key idea is to stack Φ and Y side-by-side, then factor.

[→ Click for what it looks like]

What does this look like? Each row is one time point. On the left we have the regression vector—our inputs. On the right we have y—our output. We're just gluing them together into one matrix.

[→ Click for why]

Why do this? Because multiplying by Q doesn't change the loss function. So we can transform both Φ and Y together, and the optimization problem stays the same.
:::

------------------------------------------------------------------------

## Structure of the QR Result

After factorization: $[\mathbf{\Phi} \; \mathbf{Y}] = QR$

**R is mostly zeros:**

$$R = \begin{bmatrix} R_0 \\ 0 \\ 0 \\ \vdots \\ 0 \end{bmatrix} \quad \leftarrow \text{Only top block } R_0 \text{ is non-zero}$$

. . .

**Big picture:** We have lots of data (many rows), but few parameters (few columns). Since R is upper triangular, everything below the small top block is zeros.

→ **We only need to work with the small block $R_0$, not the huge matrix $R$!**

::: notes
After factorization, let's look at R.

R is mostly zeros. Since R is upper triangular, everything below the top block is all zeros.

[→ Click for big picture]

Here's the big picture. We typically have thousands of data points but only a handful of parameters. So R is a tall, skinny matrix—but we only care about the small square block at the top. This is a massive computational saving.
:::

------------------------------------------------------------------------

## Decomposing $R_0$: Separating Data and Outputs

Partition $R_0$ to separate the $\mathbf{\Phi}$ part from the $\mathbf{Y}$ part:

$$R_0 = \begin{bmatrix} R_1 & R_2 \\ 0 & R_3 \end{bmatrix}$$

where:

-   $R_1$ is $d \times d$ (corresponds to $\mathbf{\Phi}$)

-   $R_2$ is $d \times 1$ (interaction between $\mathbf{\Phi}$ and $\mathbf{Y}$)

-   $R_3$ is scalar (corresponds to $\mathbf{Y}$)

::: notes
Now let's decompose R₀ into smaller blocks.

Remember, R₀ came from our augmented matrix which had both Φ and Y. So R₀ naturally splits into parts: R₁ captures the data, R₃ captures the output, and R₂ captures how they interact.

Why does this matter? Because we'll see that each block plays a specific role in finding our solution.
:::

------------------------------------------------------------------------

## How This Transforms the LS Criterion

Original criterion: $V_N(\theta) = |\mathbf{Y} - \mathbf{\Phi}\theta|^2$

After applying $Q^T$ (using $QQ^T = I$):

$V_N(\theta) = |Q^T(\mathbf{Y} - \mathbf{\Phi}\theta)|^2 = |Q^T[\mathbf{\Phi} \; \mathbf{Y}] \begin{bmatrix} -\theta \\ 1 \end{bmatrix}|^2$

. . .

$V_N(\theta) = |R \begin{bmatrix} -\theta \\ 1 \end{bmatrix}|^2$

::: notes
Now here's where everything comes together.

We started with our original criterion (loss function)—the sum of squared prediction errors. The key trick is that we can multiply by Q-transpose without changing anything, because orthogonal matrices preserve lengths.

[→ Click for final form]

After some algebra, our loss function now only involves R. And since R is mostly zeros, we really only need R₀. This is the payoff of all that setup!
:::

------------------------------------------------------------------------

## Transformed Criterion (Final Form)

Since only $R_0$ is non-zero, and using the block structure:

$$V_N(\theta) = \left|\begin{bmatrix} R_1 & R_2 \\ 0 & R_3 \end{bmatrix} \begin{bmatrix} -\theta \\ 1 \end{bmatrix}\right|^2$$

$$= \left|\begin{bmatrix} -R_1\theta + R_2 \\ R_3 \end{bmatrix}\right|^2 = |R_2 - R_1\theta|^2 + |R_3|^2$$

::: notes
Here's the transformed criterion in its final form.

Using the block structure of R₀, our loss function splits into two parts. Look at the final expression on the slide—it's a sum of two squared terms.

This split is important because one part depends on θ and one part doesn't. That's what lets us find the minimum.
:::

------------------------------------------------------------------------

## Finding the Minimum

**Goal:** Minimize $V_N(\theta) = |R_2 - R_1\theta|^2 + |R_3|^2$

. . .

**Two terms:**
1. $|R_2 - R_1\theta|^2$ ← depends on θ (we can control this!)
2. $|R_3|^2$ ← constant (can't change it)

. . .

**Strategy:** Make term 1 as small as possible → set it to zero!

$$|R_2 - R_1\theta|^2 = 0 \quad \Rightarrow \quad \boxed{R_1\hat{\theta}_N = R_2}$$

. . .

**Solution method:**

- $R_1$ is upper triangular → use **back-substitution** (very fast!)

- No matrix inversion needed

- Minimum value: $V_N(\hat{\theta}_N) = |R_3|^2$ ← goodness-of-fit

::: notes
Now let's find the minimum.

Our goal is on the slide—minimize the loss function.

[→ Click for the two terms]

Notice it has two terms. The first term we can control by choosing θ. The second term is fixed—it doesn't depend on θ at all.

[→ Click for strategy]

So the strategy is simple: make the first term as small as possible. The smallest it can be is zero. That gives us the equation shown in the box.

[→ Click for solution method]

Here's the beautiful part. Because R₁ is upper triangular, we can solve this equation using back-substitution—no matrix inversion needed. It's fast and numerically stable.

And the leftover term tells us how good our fit is.
:::

------------------------------------------------------------------------

## Summary: Why QR Works

**Step-by-step what we solved:**

Starting from the transformed criterion: $V_N(\theta) = |R_2 - R_1\theta|^2 + |R_3|^2$

1.  **Used all blocks** ($R_1$, $R_2$, $R_3$) to find the optimal $\theta$
2.  **Solved**: $R_1\hat{\theta}_N = R_2$ using back-substitution
3.  **Residual**: $|R_3|^2$ tells us the minimum achievable loss

::: notes
Let me summarize what we just did.

We took our transformed loss function and found that all three blocks of R₀ play a role. R₁ and R₂ give us the equation to solve. R₃ tells us the residual error—how well our model fits.

The key is that solving the equation is easy because R₁ is triangular.
:::


------------------------------------------------------------------------

## Summary: Why QR Works

**Why this is better than normal equations:**

-   **Conditioning**: $\kappa(R_1) = \sqrt{\kappa(R(N))}$ — square root improvement!

-   **Stability**: Never compute $\mathbf{\Phi}^T\mathbf{\Phi}$ which squares the condition number

-   **Speed**: Back-substitution on triangular $R_1$ is fast and numerically stable

::: notes
So why is QR better than the normal equations?

Three reasons. First, conditioning. The condition number of R₁ is the square root of what we'd get with the direct method. That's a huge improvement.

Second, stability. We never form Φ-transpose Φ, which is where the condition number gets squared and things go wrong.

Third, speed. Back-substitution on a triangular matrix is fast and stable.

This is why QR is the standard method in practice.
:::


------------------------------------------------------------------------

## Initial Conditions Problem

**Recall**: To solve least squares, we need the sample covariance matrix $R(N) = \frac{1}{N}\Phi^T\Phi$

. . .

**But**: The regression vector $\varphi(t)$ contains **lagged data**—past values of inputs and outputs

. . .

**The problem**: At the start of our dataset, some of these past values don't exist!

::: notes
Now let's address a practical problem—initial conditions.

Recall that to solve least squares, we need the sample covariance matrix R(N). This requires computing products of regression vectors φ(t).

[→ Click for the structure issue]

But here's the catch—the regression vector φ (fie) contains lagged data. It needs past values: y(t−1), y(t−2), u(t−1), and so on.

[→ Click for the problem]

The problem is that at the beginning of our dataset, some of these past values don't exist. If our data starts at t=1, what do we use for y(0) or u(−1)? This is a fundamental issue every system identification algorithm must handle.
:::

------------------------------------------------------------------------

## Structure of the Regression Vector

**General form**: $\varphi(t)$ is built from a **data vector** $z(t)$ containing outputs and inputs:

$$\varphi(t) = \begin{bmatrix} z(t-1) \\ z(t-2) \\ \vdots \\ z(t-n) \end{bmatrix} \quad \text{(shifted copies of } z \text{)}$$

. . .

**What's in** $z(t)$? Depends on the model:

| Model | $z(t)$ contains |
|-------|-----------------|
| ARX   | $[-y(t), \; u(t)]^T$ — outputs and inputs |
| AR    | $-y(t)$ — just outputs |

::: notes
Let's look at the structure more carefully.

The regression vector φ(t) (fie of t) is built by stacking shifted copies of a data vector z(t). Think of it as a sliding window—at each time t, we look back n steps and collect those values.

[→ Click for what z contains]

What's in z(t)? It depends on our model structure. For an ARX model, z(t) contains both the negative output and the input—so our regressor uses both output history and input history. For a pure AR model with no input, z(t) is just the negative output.

The key point is that φ(t) always needs data from the past—specifically from times t−1 back to t−n.
:::

------------------------------------------------------------------------

## Where Initial Conditions Appear

With this shifted structure, the covariance matrix has elements:

$$R_{ij}(N) = \frac{1}{N}\sum_{t=1}^{N} z(t-i)z^T(t-j)$$

. . .

**The issue**: When $t = 1$ and we need $z(0), z(-1), \ldots, z(1-n)$

These values are **before our data begins**—they don't exist!

. . .

**We need a strategy to handle these missing initial conditions**

::: notes
Now we can see exactly where initial conditions cause trouble.

The covariance matrix R has block elements R_ij (R-sub-i-j). Each element requires computing products of lagged data z(t−i) times z(t−j) transpose, summed over all time points.

[→ Click for the issue]

The issue becomes clear when t=1. The term z(t−1) = z(0), and z(t−n) = z(1−n). These are values from before our data begins! We collected data starting at t=1, so z(0), z(−1), etc. simply don't exist.

[→ Click for the need for a strategy]

We need a strategy to handle this. We can't just ignore the problem—every regression-based method faces this at the start of the dataset. Let's look at two common approaches.
:::

------------------------------------------------------------------------

## Two Approaches

**Approach 1: Start summation later** (Covariance method)

-   Start at $t = n+1$ instead of $t=1$

-   All sums involve only known data

-   Loses $n$ data points, but straightforward

. . .

**Approach 2: Prewindowing (zero padding)** (Autocorrelation method)

-   Replace unknown initial values by zeros

-   For symmetry: also replace trailing values by zeros ("postwindowing")

-   **Advantage**: Keeps all $N$ data points

. . .

**Key insight**: Approach 2 gives $R(N)$ a special **block Toeplitz structure**, leading to the **Yule-Walker equations**

::: notes
Here are two approaches to handle missing initial conditions.

Approach 1 is the Covariance method. Start summation at t = n+1 instead of t = 1. All sums involve only known data, but we lose n data points.

[→ Click for Approach 2]

Approach 2 is Prewindowing, also called the Autocorrelation method. Replace unknown values with zeros. For symmetry, also pad zeros at the end. The advantage is that it keeps all N data points.

[→ Click for key insight]

Here's the key insight. Approach 2 gives R(N) a special block Toeplitz structure, leading to the Yule-Walker equations.
:::

------------------------------------------------------------------------

## Why Toeplitz Structure Matters

**The connection we just established:**

-   Prewindowing (zero padding) → Toeplitz structure → Yule-Walker equations

. . .

**Why is this important?**

The Toeplitz structure means the normal equations have **redundancy**:

-   Entries depend only on distance from diagonal, not absolute position

-   This redundancy can be exploited computationally!

. . .

**What does this enable?**

When fitting AR models of different orders, we can **reuse previous computations** instead of starting from scratch each time → **Levinson algorithm**

::: notes
Now let's see why the Toeplitz structure matters.

The connection we just established is this: prewindowing gives us Toeplitz structure, which gives us the Yule-Walker equations.

[→ Click for why important]

Why is this important? The Toeplitz structure means entries depend only on distance from diagonal. This redundancy can be exploited computationally!

[→ Click for what this enables]

What does this enable? When fitting AR models of different orders, we can reuse previous computations—this leads us to the Levinson algorithm with O(n²) ("oh of n squared") instead of O(n³) ("oh of n cubed")!
:::

------------------------------------------------------------------------

## Levinson Algorithm - The Problem

**Real-world scenario**: We're building an AR model but don't know the right order $n$

. . .

**The challenge**:

-   Too low an order: Model misses important dynamics

-   Too high an order: Model overfits noise

. . .

**Solution**: Try multiple orders ($n=1, 2, 3, ..., 10$) and pick the best using criteria like AIC or BIC

. . .

**But there's a computational cost...**

::: notes
Let me set up the problem Levinson solves.

Here's a real-world scenario. We're building an AR model but don't know the right order n.

[→ Click for the challenge]

The challenge is this: Too low and we miss important dynamics. Too high and we overfit noise.

[→ Click for solution]

The solution is to try multiple orders (n=1, 2, ..., 10) and pick the best using AIC or BIC.

[→ Click]

But there's a computational cost... Each order solved from scratch is expensive!
:::

------------------------------------------------------------------------

## Levinson Algorithm - Naive Approach

**Naive approach**: Try $n=1, n=2, n=3, ..., n=10$ separately

-   Each requires solving the normal equations from scratch

-   For each order: $O(n^3)$ operations (using QR factorization or matrix inversion)

-   Total: $10 \times O(n^3)$ = **very expensive!**

. . .

**Example with** $n=10$:

-   Each solve: \~1000 operations

-   Total: $10 \times 1000 = 10,000$ operations

. . .

**The waste**: We throw away all our work from order $n$ when computing order $n+1$

::: notes
Here's the naive approach to trying different model orders.

The naive approach is to try n=1, n=2, n=3, ..., n=10 separately.

Why is this expensive? Each order requires solving the normal equations from scratch using QR factorization—that's O(n³) ("oh of n cubed") operations per order. If we want to try 10 different orders, we pay that cost 10 times.

[→ Click for example]

Here's an example with n=10. Each solve takes roughly 1000 operations. Ten solves = 10,000 operations total.

To put this in perspective: imagine computing a tax return from scratch 10 times instead of just updating last year's return.

[→ Click for the waste]

Here's the waste. We throw away all our work from order n when computing order n+1. But here's the insight—the solution for order n contains useful information for order n+1! The Levinson algorithm exploits this structure.
:::

------------------------------------------------------------------------

## Levinson Algorithm - The Clever Solution

**Levinson approach**: Build solutions incrementally, reusing previous work!

-   Start with $n=1$ solution

-   Use it to build $n=2$ solution (just $O(2)$ extra work)

-   Use $n=2$ to build $n=3$ solution (just $O(3)$ extra work)

-   ... and so on

. . .

**Total cost**: $O(1+2+3+...+10) = O(n^2)$ — **much faster!**

. . .

**Example with** $n=10$:

-   Total: $1+2+3+...+10 = 55$ operations

-   **Speedup**: $10,000 / 55 \approx 180$ times faster!

::: notes
Now here's the clever solution.

The Levinson approach builds solutions incrementally. Start with n=1 solution, use it to build n=2, use n=2 to build n=3, and so on.

[→ Click for total cost]

The total cost is O(1+2+3+...+10) = O(n²) ("oh of n squared")—much faster!

[→ Click for example]

Here's an example with n=10. Total = 1+2+3+...+10 = 55 operations. That's a 180× speedup!
:::

------------------------------------------------------------------------

## How Levinson Algorithm Works

**Core idea**: Build the solution for order $n+1$ from the solution for order $n$

. . .

**Key observation**: When using **prewindowing** (zero padding), the matrix $R(N)$ has **Toeplitz structure**

-   Toeplitz = entries depend only on distance from diagonal

-   This special structure means previous solutions contain useful information!

. . .

**Update mechanism**:

1.  Start with order $n$ solution: $\hat{\theta}_n$

2.  Add a "correction term" proportional to the old solution

3.  Scale by **reflection coefficient** $\rho_n$ which measures the new information at order $n+1$

::: notes
Now let me explain how the Levinson algorithm works.

The core idea is to build the solution for order n+1 from the solution for order n.

[→ Click for key observation]

Here's the key observation. When using prewindowing, R(N) has Toeplitz structure—entries depend only on distance from diagonal. This means previous solutions contain useful information!

[→ Click for update mechanism]

The update mechanism works like this. Start with order n solution, add a correction term, and scale by the reflection coefficient ρₙ (rho) which measures new information at order n+1.
:::

------------------------------------------------------------------------

## Levinson Algorithm - The Key Idea

**Going from order $n$ to order $n+1$:**

1. **Compute one new number** — the reflection coefficient $\rho_n$ (rho-n)

2. **Add one new parameter** — it equals $\rho_n$

3. **Adjust all old parameters** — each gets a small correction

. . .

**The "mirror" trick**: Old parameters are corrected using themselves in reverse order

$$\text{parameter } k \text{ gets corrected by parameter } (n+1-k)$$

This symmetry exploits the Toeplitz structure!

::: notes
Here's the key idea behind the Levinson algorithm.

When we go from order n to order n+1, we do three things. First, compute one new number—the reflection coefficient ρ (rho). Second, add one new parameter, which simply equals that reflection coefficient. Third, adjust all the old parameters with small corrections.

[→ Click for mirror trick]

What makes Levinson clever is the "mirror" trick. When we correct parameter 1, we use information from parameter n. When we correct parameter 2, we use parameter n−1. And so on—each parameter pairs with its mirror image. This symmetric correction pattern is what exploits the Toeplitz structure. The formula is on the next slide for reference, but the intuition is: we're leveraging symmetry to avoid redundant computation.
:::

------------------------------------------------------------------------

## Levinson Algorithm - Formulas (Reference)

**Notation**: $\hat{\theta}_k^n$ = parameter $k$ in an order-$n$ model

**Update formula** (for each old parameter $k = 1, \ldots, n$):

$$\hat{\theta}_k^{n+1} = \hat{\theta}_k^n + \rho_n \hat{\theta}_{n+1-k}^n$$

**New parameter**: $\hat{\theta}_{n+1}^{n+1} = \rho_n$

**Reflection coefficient**: $\rho_n = -\alpha_n / V_n$

::: notes
This slide shows the actual formulas for reference.

The update formula shows the mirror structure we just discussed. The subscript n+1−k is the mirror index—when k=1, we use parameter n; when k=2, we use parameter n−1, and so on.

The reflection coefficient ρ (rho) measures how much new information we gain by adding another parameter. If it's close to zero, the extra parameter doesn't help much. If it exceeds ±1, something is wrong with the data or model.

Don't worry about memorizing these formulas—the key insight is the mirror structure that makes the algorithm efficient.
:::

------------------------------------------------------------------------

## Levinson Algorithm - Computational Impact

**Key efficiency gains:**

-   **Going from order** $n$ to $n+1$: Only $O(n)$ operations (not $O(n^3)$!)

-   **Computing all orders 1 to** $n$: Total $O(n^2)$ operations

. . .

**Real impact**: For $n=20$ (testing orders 1-20):

-   **Standard approach**: $20 \times 8000 = 160,000$ units of work

-   **Levinson approach**: $400$ units of work

-   **Speedup**: 400× faster!

. . .

**Why it matters**:

-   Makes **model order selection** practical

::: notes
Let's look at the computational impact.

The key efficiency gains are significant. Going from order n to n+1 is only O(n) ("oh of n") operations. Computing all orders 1 to n is O(n²) ("oh of n squared") total.

[→ Click for real impact]

Here's the real impact. For n=20, the standard approach takes 160,000 operations. Levinson takes just 400 operations. That's a 400× speedup!

[→ Click for why it matters]

Why does this matter? It makes model order selection practical.
:::

------------------------------------------------------------------------

## Levinson Algorithm - Intuition

**Think of it as building a "ladder" of models**:

-   **Order 1**: Simple model with 1 parameter

-   **Order 2**: Add parameter 2 using info from order 1

-   **Order 3**: Add parameter 3 using info from orders 1 & 2

-   ... and so on

. . .

Each step **reuses previous work** rather than starting from scratch. This is why adding one more order only costs $O(n)$ extra work, not $O(n^3)$.

. . .

**The reflection coefficient** $\rho_n$ acts like a **diagnostic**:

-   If $|\rho_n| \approx 0$: Adding order $n$ gives little new information

-   If $|\rho_n| \approx 1$: Adding order $n$ is critical for the fit

::: notes
Here's the intuition behind Levinson.

Think of it as building a "ladder" of models. Order 1 is a simple model with 1 parameter. Order 2 adds parameter 2 using info from order 1, and so on.

[→ Click for why it works]

Each step reuses previous work—this is why adding one more order costs only O(n) ("oh of n") extra work, not O(n³) ("oh of n cubed").

[→ Click for the diagnostic]

The reflection coefficient ρₙ (rho) acts like a diagnostic. If |ρₙ| ≈ 0, adding order n gives little new information. If |ρₙ| ≈ 1, adding order n is critical for the fit.
:::

------------------------------------------------------------------------

## Lattice Filters

**What is a lattice filter?**

An alternative **network architecture** for computing the same Levinson recursion, but structured differently:

**How it works**: Instead of updating parameter vectors directly, it processes two parallel **error streams**:

-   **Forward error** $e_n(t)$: Predicting $y(t)$ from its past (standard prediction)

-   **Backward error** $f_n(t)$: "Predicting" past data from future (novel idea!)

. . .

**Key insight**: These two errors are **orthogonal** (independent) at different orders $$\frac{1}{N}\sum_{t=1}^{N} e_n(t)e_{n-k}(t-k) = \begin{cases} V_n, & k = 0 \\ 0, & k \neq 0 \end{cases}$$

This orthogonality is **numerically stabilizing**.

::: notes
Now let me introduce lattice filters.

What is a lattice filter? It's an alternative network architecture for computing the same Levinson recursion.

How does it work? It processes two parallel error streams. The forward error e_n(t) predicts y(t) from its past. The backward error f_n(t) "predicts" past data from future.

[→ Click for key insight]

Here's the key insight. These two errors are orthogonal at different orders, as shown in the formula. This orthogonality is numerically stabilizing.
:::


------------------------------------------------------------------------

## Lattice Filters - Why Use Them?

**Recall**: Both Levinson and lattice filters use the same reflection coefficients $\rho_n$.

**The difference**: How they're implemented and what that enables.

. . .

**Advantages of lattice filter architecture**:

1.  **Numerical robustness**: Works directly with bounded $|\rho_n| < 1$ values, preventing coefficient blow-up

2.  **Adaptive/real-time capable**: Processes data one sample at a time (streaming)

3.  **Modular order selection**: Can easily add or remove stages without recomputing everything

4.  **Industry standard**: Speech processing, Kalman filtering, adaptive signal processing

::: notes
Now let's see why we'd choose lattice filters.

Recall that both Levinson and lattice filters use the same reflection coefficients we discussed earlier.

The difference is in the implementation and what that enables.

[→ Click for advantages]

There are several advantages of the lattice filter architecture. First, numerical robustness—it works directly with bounded reflection coefficients. Second, adaptive capability—it can process streaming data. Third, modular order selection—you can add or remove stages easily. Fourth, it's an industry standard in speech processing and adaptive filtering.
:::


------------------------------------------------------------------------

## Comparing Levinson vs. Lattice Filters

| Aspect | Levinson | Lattice Filters |
|------------------|--------------------|----------------------------------|
| **What it updates** | Parameter vector $\hat{\theta}_n$ | Error streams $e_n(t)$ and $f_n(t)$ |
| **Complexity** | $O(n^2)$ | $O(n^2)$ |
| **Numerical stability** | Good | Excellent (bounded coefficients) |
| **Real-time capable** | Not naturally | Yes (process sample-by-sample) |
| **Industry use** | Academic/theoretical | Speech, adaptive filtering, control |

. . .

**Bottom line**: Both solve the same problem recursively. Lattice filters trade computation for better stability and adaptability.

::: notes
Let me compare Levinson versus Lattice filters side by side.

The table shows the key differences: what they update, complexity, stability, real-time capability, and industry use.

[→ Click for the bottom line]

The bottom line is that both solve the same problem recursively. Lattice filters trade computation for better stability and adaptability.
:::


------------------------------------------------------------------------

## Data Tapering (Optional refinement)

To soften artifacts from zero padding:

-   Apply **tapering weights** to both ends of the data record

-   Reduces edge effects from the appended zeros

-   Used in conjunction with prewindowing for refinement

::: notes
Here's an optional refinement called data tapering.

To soften artifacts from zero padding, we apply tapering weights to both ends of the data record. This reduces edge effects from the appended zeros.

This is used in conjunction with prewindowing for refinement in high-precision applications.
:::

------------------------------------------------------------------------

## Summary of Section 10.1

**Key takeaways:**

-   **Normal equations** provide the foundation for LS estimation

-   **QR factorization** offers superior numerical stability

-   **Why it works**: Better conditioning, triangular structure, avoids ill-conditioned matrix products

-   **Initial conditions** are handled by prewindowing or starting summation later

-   **Practical insight**: Implement via $R_0$ only, avoiding large matrix storage

::: notes
Let me summarize Section 10.1 on Linear Regressions and Least Squares.

Here are the key takeaways.

Normal equations provide the foundation for LS estimation.

QR factorization offers superior numerical stability. Why does it work? Better conditioning, triangular structure, and it avoids ill-conditioned matrix products.

Initial conditions are handled by prewindowing or starting summation later.

A practical insight: implement via R₀ only, avoiding large matrix storage.
:::


------------------------------------------------------------------------

# Section 10.2: Numerical Solution by Iterative Search Methods {background-color="#2c5f77"}

------------------------------------------------------------------------

## When Analytical Solutions Fail

In general, the function

$$V_N(\theta, Z^N) = \frac{1}{N}\sum_{t=1}^{N} \ell(\varepsilon(t, \theta), \theta)$$

**cannot be minimized by analytical methods.**

. . .

Similarly, the equation $f_N(\theta, Z^N) = 0$ **cannot be solved directly in general.**

::: notes
Now we move to Section 10.2 on iterative methods.

In Section 10.1, we solved for θ directly using normal equations—the easy case.

Now we face the general situation. When the loss function is non-quadratic, or prediction errors are nonlinear in θ, we can't minimize analytically.

[→ Click for the correlation equation]

Similarly, the correlation equation f_N(θ) = 0 can't be solved directly in general.
:::

------------------------------------------------------------------------

## When Analytical Solutions Fail (cont.)

::: {.warning}
**No closed-form solution exists** for nonlinear problems:

- Nonlinear model structures (e.g., Hammerstein-Wiener models)

- Non-quadratic loss functions (L1, robust errors)

- Output error models with parameter-dependent errors
:::

. . .

::: {.callout}
**Solution**: Use **iterative numerical techniques** to solve these problems step-by-step
:::

::: notes
Here are the specific cases where analytical solutions fail.

There's no closed-form solution for Hammerstein-Wiener models (cascade of static nonlinearity + linear system + static nonlinearity), non-quadratic loss functions like L1 or Huber loss for robust estimation, and output error models where prediction errors are nonlinear in parameters.

[→ Click for the solution]

The solution is to use iterative numerical techniques—start with a guess and improve it step by step.
:::

------------------------------------------------------------------------

## Numerical Minimization - General Approach

Methods for numerical minimization update the estimate iteratively:

$$\hat{\theta}^{(i+1)} = \hat{\theta}^{(i)} + \alpha f^{(i)}$$

where:

-   $f^{(i)}$ is a **search direction** based on information about $V(\theta)$
-   $\alpha$ is a **positive constant** (step size) to ensure decrease in $V(\theta)$

. . .

**Three categories of methods:**

1.  **Function values only** (e.g., simplex methods)

2.  **Function + gradient** (e.g., steepest descent, Gauss-Newton)

3.  **Function + gradient + Hessian** (e.g., Newton's method)

::: notes
Here's the general approach for numerical minimization.

The update formula is θ̂⁽ⁱ⁺¹⁾ = θ̂⁽ⁱ⁾ + αf⁽ⁱ⁾. Start with an initial guess, compute a search direction f, take a step of size α, then repeat until convergence.

[→ Click for three categories]

There are three categories based on what information they use.

Category 1 uses function values only—slow but robust. Example: Nelder-Mead simplex.

Category 2 uses gradients—much faster and most common. Examples: steepest descent, Gauss-Newton.

Category 3 uses gradient plus Hessian—best convergence but expensive. Example: Newton's method.

The trade-off is that more information means smarter directions but costlier iterations. Category 2 hits the sweet spot.
:::

------------------------------------------------------------------------

## Overview: Specific Methods in Section 10.2

**Problem**: Minimize $V_N(\theta, Z^N)$ or solve $f_N(\theta, Z^N) = 0$

. . .

**For minimization problems** (focus of this section):

| Method | Category | Information Used | Typical Use |
|-----------------|-----------------|---------------------|-----------------|
| **Newton's Method** | Group 3 | Gradient + Hessian | Near minimum, expensive |
| **Steepest Descent** | Group 2 | Gradient only | Simple, robust, slow |
| **Gauss-Newton** | Group 2 | Gradient + approx. Hessian | Least squares, fast |
| **Levenberg-Marquardt** | Group 2 | Gradient + adaptive damping | Industrial workhorse |

. . .

**For solving equations** (briefly): - **Newton-Raphson method**: Analog of Newton's method for $f_N(\theta) = 0$ - **Substitution method**: Simple iteration on the equation

::: notes
Here's an overview of the specific methods we'll cover.

**Problem**: Minimize V_N(θ) or solve f_N(θ) = 0.

[→ Click for the methods table]

**For minimization problems**: The table shows the four main methods—Newton's Method (Group 3), Steepest Descent, Gauss-Newton, and Levenberg-Marquardt (all Group 2).

[→ Click for solving equations]

**For solving equations**: Newton-Raphson is analogous to Newton's method. Substitution method is simple iteration.
:::

------------------------------------------------------------------------

## Newton's Method

The classic **Newton algorithm** belongs to group 3:

$$f^{(i)} = -[V''(\hat{\theta}^{(i)})]^{-1} V'(\hat{\theta}^{(i)})$$

where:

-   $V'(\theta)$ is the **gradient** (first derivative)

-   $V''(\theta)$ is the **Hessian** (second derivative matrix)

. . .

This approximates $V(\theta)$ by a quadratic function and finds its minimum

::: notes
Now let's look at Newton's method.

The classic Newton algorithm belongs to group 3. The formula shows the search direction using the inverse Hessian times the gradient.

V'(θ) is the gradient and V''(θ) is the Hessian (second derivative matrix).

[→ Click for the interpretation]

This approximates V(θ) by a quadratic function and finds its minimum—like fitting a bowl and jumping to its bottom.
:::

------------------------------------------------------------------------

## Newton's Method - Properties

::: {.key-insight}
**Why Newton's method is powerful:**

- **Quadratic convergence**: Error decreases quadratically with each iteration

- **One-step for quadratics**: If V(θ) is exactly quadratic, Newton finds the minimum in one step

- **Natural step size**: Formula implicitly determines both direction and step size
:::

. . .

::: {.warning}
**Practical issue**: Computing the full Hessian $V''(\theta)$ is expensive

- Requires computing all $d^2$ entries of the Hessian matrix

- Then inverting it costs $O(d^3)$ operations

- For large parameter spaces: **prohibitively expensive**
:::

. . .

::: {.callout}
**This motivates quasi-Newton methods** (coming next)
:::

::: notes
Here's why Newton's method is powerful.

It has quadratic convergence (error decreases quadratically), one-step solution for quadratic functions, and natural step size built into the formula.

[→ Click for the practical issue]

The practical issue is that computing the full Hessian is expensive—requires d² entries and O(d³) to invert. That's prohibitively expensive for large parameter spaces.

[→ Click for the motivation]

This motivates quasi-Newton methods, which approximate the Hessian more cheaply.
:::

------------------------------------------------------------------------

## Why We Need to Understand the Gradient

**Key insight from Newton's Method:**

-   Newton's method requires **both** $V'(\theta)$ (gradient) and $V''(\theta)$ (Hessian)

-   The Hessian is expensive to compute

-   But to understand quasi-Newton approximations, we need to know what the gradient looks like

. . .

**The gradient formula is central because:**

1.  **All methods need it**: Steepest Descent, Gauss-Newton, Levenberg-Marquardt all compute $V'(\theta)$

2.  **Hessian structure depends on it**: Understanding $V'(\theta)$ helps us approximate $V''(\theta)$

3.  **Computational bottleneck**: Computing the gradient for all $N$ data points is the main cost

. . .

**Next:** Let's see the explicit formula for $V'(\theta)$ and understand its structure

::: notes
Before we look at specific methods, let's understand the gradient.

The key insight from Newton's Method is that it requires both gradient and Hessian. The Hessian is expensive, but we need to understand the gradient to approximate it.

[→ Click for why the gradient is central]

The gradient formula is central because all methods need it, the Hessian structure depends on it, and computing it for all N data points is the main cost.

[→ Click for what's next]

Next, let's see the explicit formula for V'(θ).
:::

------------------------------------------------------------------------

## The Gradient Formula

For the criterion $V_N(\theta, Z^N) = \frac{1}{N}\sum_{t=1}^{N} \ell(\varepsilon(t, \theta), \theta)$

The gradient is:

$$V_N'(\theta, Z^N) = -\frac{1}{N}\sum_{t=1}^{N} \{\psi(t, \theta)\ell_{\varepsilon}'(\varepsilon(t, \theta), \theta) - \ell_{\theta}'(\varepsilon(t, \theta), \theta)\}$$

where $\psi(t, \theta) = \frac{\partial \hat{y}(t|\theta)}{\partial \theta}$ is the $d \times p$ gradient matrix

. . .

**Components of the gradient:**

-   $\psi(t, \theta)$: How predicted output changes with parameters

-   $\ell_{\varepsilon}'$: Derivative of loss w.r.t. prediction error

-   $\ell_{\theta}'$: Direct derivative of loss w.r.t. parameters (often zero)

. . .

**Major computational burden**: Computing $\psi(t, \theta)$ for all $t = 1, \ldots, N$

::: notes
Here's the gradient formula.

The gradient V'_N is shown in the formula. The key term is ψ(t,θ) (sigh)—how predicted output changes with parameters.

[→ Click for the components]

The components of the gradient are: ψ(t,θ) (sigh) is prediction sensitivity, ℓ'_ε (ell-prime-epsilon) is the loss derivative w.r.t. error, and ℓ'_θ is direct parameter dependence (often zero).

[→ Click for the computational burden]

The major computational burden is computing ψ(t,θ) (sigh) for all t = 1, ..., N. That's where most CPU time goes.
:::

------------------------------------------------------------------------

## Overview: Iterative Search Methods

**The general update formula:**

$$\hat{\theta}_N^{(i+1)} = \hat{\theta}_N^{(i)} - \mu_N^{(i)} [R_N^{(i)}]^{-1} V_N'(\hat{\theta}_N^{(i)}, Z^N)$$

Different choices of $R_N^{(i)}$ (Hessian approximation) and $\mu_N^{(i)}$ (step size) give different algorithms.

. . .

**Methods we'll cover:**

1.  **Steepest Descent**: Simple gradient direction, slow convergence

2.  **Gauss-Newton**: Hessian approximation using prediction sensitivity

3.  **Levenberg-Marquardt**: Adaptive blend between Gauss-Newton and Steepest Descent

. . .

**Trade-off to understand:**

-   More accurate $R_N^{(i)}$ means faster convergence but more computation

-   How do we choose step size $\mu_N^{(i)}$? (Line search or adaptive)

::: notes
Here's the framework that unifies all iterative search methods.

The general update formula shows that different choices of R (Hessian approximation) and μ (mew, step size) give different algorithms.

[→ Click for methods we'll cover]

We'll cover Steepest Descent (simple but slow), Gauss-Newton (fast near minimum), and Levenberg-Marquardt (adaptive blend).

[→ Click for the trade-off]

The trade-off is that more accurate R means faster convergence but more computation per iteration.
:::

------------------------------------------------------------------------

## Steepest Descent Method

The simplest choice: Take $R_N^{(i)} = I$ (identity matrix)

$$\hat{\theta}_N^{(i+1)} = \hat{\theta}_N^{(i)} - \mu_N^{(i)} V_N'(\hat{\theta}_N^{(i)}, Z^N)$$

This is the **gradient descent** or **steepest descent method**.

. . .

**The idea:** Move in direction of negative gradient (steepest downhill)

. . .

::::: columns
::: {.column width="50%"}
**Pros** ✓

-   Simple to implement

-   Doesn't require second derivatives

-   Always descends for small enough $\mu_N^{(i)}$
:::

::: {.column width="50%"}
**Cons** ✗

-   Very slow convergence near minimum

-   Zig-zag behavior in narrow valleys

-   Step size $\mu_N^{(i)}$ critical (too small = slow, too large = diverge)
:::
:::::

::: notes
Let's start with the simplest iterative method—steepest descent.

The simplest choice is to take R = I (identity matrix). This gives the gradient descent formula.

[→ Click for the idea]

The idea is to move in the direction of negative gradient—steepest downhill.

[→ Click for pros and cons]

The pros are that it's simple, needs no second derivatives, and always descends for small μ (mew). The cons are that it's slow near the minimum, exhibits zig-zag behavior, and step size is critical.
:::

------------------------------------------------------------------------

## Gauss-Newton Method - The Hessian Structure

The full Hessian for quadratic criterion has two terms:

$$V_N''(\theta, Z^N) = \underbrace{\frac{1}{N}\sum_{t=1}^{N} \psi(t, \theta)\psi^T(t, \theta)}_{\text{First term (always ≥ 0)}} - \underbrace{\frac{1}{N}\sum_{t=1}^{N} \psi'(t, \theta)\varepsilon(t, \theta)}_{\text{Second term (requires 2nd derivatives)}}$$

. . .

**Key insight**: Near the minimum, prediction errors $\varepsilon(t, \theta)$ become independent innovations with mean zero → **second term averages to ≈ 0**

::: notes
Now we get to the Gauss-Newton method—one of the most important algorithms in system identification.

The full Hessian has two terms.

The first term is ψψᵀ (sigh times sigh-transpose)—always positive semidefinite (≥ 0).

The second term is ψ'ε (sigh-prime times EP-sih-lon)—requires expensive second derivatives.

[→ Click for the key insight]

Here's the key insight. Near the minimum, errors ε(t) (EP-sih-lon) become mean-zero innovations. So the second term averages to approximately zero when summed over many data points.
:::

------------------------------------------------------------------------

## Gauss-Newton Method - The Approximation

**Gauss-Newton approximation** (drop the second term):

$$V_N''(\theta, Z^N) \approx H_N(\theta) = \frac{1}{N}\sum_{t=1}^{N} \psi(t, \theta)\psi^T(t, \theta)$$

. . .

**Benefits**:

- Only needs **first derivatives** (no expensive ψ'(t,θ))

- **Always positive semidefinite** → guarantees descent direction

- We already computed ψ for gradient anyway—minimal extra cost

::: notes
Here's the Gauss-Newton approximation.

**The approximation**: We keep only the first term of the Hessian—the ψψᵀ (sigh times sigh-transpose) term. Drop the second term since it averages to zero near the minimum.

[→ Click for benefits]

**Benefits**: Only needs first derivatives, always positive semidefinite (guarantees descent), and we already computed ψ (sigh) for the gradient.
:::

------------------------------------------------------------------------

## Gauss-Newton Method - The Algorithm

Using $R_N^{(i)} = H_N(\hat{\theta}_N^{(i)})$ in the search formula:

$$\hat{\theta}_N^{(i+1)} = \hat{\theta}_N^{(i)} - \mu_N^{(i)} [H_N(\hat{\theta}_N^{(i)})]^{-1} V_N'(\hat{\theta}_N^{(i)}, Z^N)$$

This is the **Gauss-Newton method**.

. . .

**Why it's powerful:**

1.  **Computational efficiency**: Avoids expensive second derivatives $\psi'(t, \theta)$

2.  **Numerical stability**: $H_N(\theta) \geq 0$ guarantees descent direction

3.  **Fast convergence**: Newton-like speed near the minimum

. . .

**Common choice**: $\mu_N^{(i)} = 1$ (no line search needed)

Also called **"method of scoring"** in statistics literature

::: notes
Now let's see the Gauss-Newton algorithm.

**Using R = H_N(θ)** in the search formula gives us the Gauss-Newton method.

[→ Click for why it's powerful]

**Why it's powerful**: Computational efficiency (avoids ψ' (sigh-prime)), numerical stability (H_N ≥ 0), and fast convergence near the minimum.

[→ Click for common choice]

**Common choice**: μ = 1 (no line search needed). Also called the "method of scoring" in statistics.
:::

------------------------------------------------------------------------

## Levenberg-Marquardt Method

**Problem with Gauss-Newton**: $H_N(\theta)$ may be singular/nearly singular

-   Overparameterized models (redundant parameters)

-   Insufficient data

-   Numerical ill-conditioning

. . .

**Levenberg-Marquardt solution**: Add regularization!

$$R_N^{(i)}(\lambda) = H_N(\hat{\theta}_N^{(i)}) + \lambda I = \frac{1}{N}\sum_{t=1}^{N} \psi\psi^T + \lambda I$$

where $\lambda > 0$ is the **damping parameter**

. . .

**How** $\lambda$ **controls the algorithm:**

| $\lambda$ value | Behavior         | When to use      |
|-----------------|------------------|------------------|
| Small (≈0)      | Gauss-Newton     | Near minimum     |
| Large           | Steepest descent | Far from minimum |

**Adaptive strategy**: Start large $\lambda$ (robust), decrease as converging (fast)

::: notes
Now let's look at the Levenberg-Marquardt method.

**Problem with Gauss-Newton**: H_N(θ) may be singular due to overparameterization, insufficient data, or ill-conditioning.

[→ Click for the solution]

**Levenberg-Marquardt solution**: Add regularization! R = H_N + λI where λ (LAM-duh) is the damping parameter.

[→ Click for how λ controls the algorithm]

**How λ controls the algorithm**: Small λ behaves like Gauss-Newton—fast when near the minimum. Large λ behaves like steepest descent—robust when far from minimum. **Adaptive strategy**: Start with large λ, decrease as we converge.
:::


------------------------------------------------------------------------

## Levenberg-Marquardt - Adaptive λ Strategy

**The algorithm adapts** $\lambda$ **based on progress:**

```         
Initialize: λ = λ₀ (e.g., 0.01)

At each iteration:
  1. Compute update with current λ
  2. Try the new parameter values

  If V(θ_new) < V(θ_old):  [Success!]
     ✓ Accept the update
     ✓ Decrease λ ← λ/10  (trust model more)

  Else:  [Function increased - bad step]
     ✗ Reject the update
     ✗ Increase λ ← λ×10  (be more cautious)
```

. . .

**Result**: Automatic balance between robustness and speed!

::: notes
Here's how the adaptive λ (LAM-duh) strategy works.

**The algorithm adapts λ based on progress**: Initialize λ, compute update, try new values. If V decreases, we accept and decrease λ—we trust the model more. If V increases, we reject and increase λ—we become more cautious.

[→ Click for the result]

**Result**: Automatic balance between robustness and speed! This is why Levenberg-Marquardt is the default in most optimization software.
:::

------------------------------------------------------------------------

## Summary: Iterative Methods for Nonlinear Least Squares

**Problem**: Minimize $V_N(\theta) = \frac{1}{N}\sum_{t=1}^{N} \frac{1}{2}\varepsilon^2(t, \theta)$ where $\varepsilon(t, \theta)$ is nonlinear in $\theta$

. . .

**Key algorithms**:

| Method | Matrix $R_N^{(i)}$ | Pros | Cons |
|------------------|-------------------|------------------|------------------|
| Steepest Descent | $I$ | Simple | Slow near minimum |
| Gauss-Newton | $H_N(\theta) = \frac{1}{N}\sum \psi\psi^T$ | Fast near minimum | May have singularity |
| Levenberg-Marquardt | $H_N(\theta) + \lambda I$ | Robust + fast | Extra tuning param |

. . .

**Practical recommendation**: Use **Levenberg-Marquardt** with adaptive $\lambda$ for robustness

::: notes
Let me summarize the three iterative methods for nonlinear least squares.

**Problem**: Minimize V_N(θ) where ε(t,θ) is nonlinear in θ.

[→ Click for the key algorithms]

**Key algorithms**: The table shows Steepest Descent (R=I, simple but slow), Gauss-Newton (R=H_N, fast but may be singular), and Levenberg-Marquardt (R=H_N+λI, robust + fast).

[→ Click for the practical recommendation]

**Practical recommendation**: Use Levenberg-Marquardt with adaptive λ for robustness.
:::

------------------------------------------------------------------------

## Solving the Correlation Equation

Recall the general equation:

$$0 = f_N(\theta, Z^N) = \frac{1}{N}\sum_{t=1}^{N} \zeta(t, \theta)\alpha(\varepsilon(t, \theta))$$

This is analogous to minimization of $V_N(\theta)$. Standard procedures:

. . .

**Two methods** (analogous to what we've seen):

-   **Substitution method** [like steepest descent]: $\hat{\theta}_N^{(i)} = \hat{\theta}_N^{(i-1)} - \mu_N^{(i)} f_N(\hat{\theta}_N^{(i-1)}, Z^N)$

-   **Newton-Raphson method** [like Newton]: $\hat{\theta}_N^{(i)} = \hat{\theta}_N^{(i-1)} - \mu_N^{(i)} [f_N']^{-1} f_N$

::: notes
Now let me briefly mention solving correlation equations.

**Recall the general equation**: f_N(θ) = 0. This is analogous to minimization of V_N(θ).

[→ Click for the two methods]

**Two methods**: Substitution method (like steepest descent, simple iteration) and Newton-Raphson method (like Newton, uses Jacobian f'_N for quadratic convergence).
:::

------------------------------------------------------------------------

## Summary of Section 10.2

**Key takeaways:**

-   When analytical solutions fail, use **iterative numerical methods**

-   Three levels of information: function values only, gradient, or gradient + Hessian

-   **Gauss-Newton method**: Quasi-Newton approach using only first derivatives

-   **Levenberg-Marquardt**: Regularized Gauss-Newton for robustness

-   Main computational burden: Computing $\varepsilon(t, \theta)$ and $\psi(t, \theta)$ for all $t$

-   Same techniques apply to solving correlation equations

::: notes
Let me summarize Section 10.2.

**Key takeaways**:

-   When analytical solutions fail, use **iterative numerical methods**

-   Three levels of information: function values only, gradient, or gradient + Hessian

-   **Gauss-Newton method**: Quasi-Newton approach using only first derivatives

-   **Levenberg-Marquardt**: Regularized Gauss-Newton for robustness

-   Main computational burden: Computing ε(t,θ) (EP-sih-lon) and ψ(t,θ) (sigh) for all t

-   Same techniques apply to solving correlation equations
:::

------------------------------------------------------------------------

# Section 10.5: Local Solutions and Initial Values {background-color="#2c5f77"}

------------------------------------------------------------------------

## The Local Minimum Problem

The iterative methods from Section 10.2 converge to a **local minimum**, not necessarily the **global minimum**.

. . .

**For minimization**: We want $\hat{\theta}_N$ that minimizes $V_N(\theta, Z^N)$ globally

. . .

**For equation solving**: We want $\hat{\theta}_N^*$ that satisfies $f_N(\hat{\theta}_N^*, Z^N) = 0$

. . .

**The challenge**: Iterative search only guarantees convergence to a **local solution**

::: notes
Now we discuss a fundamental limitation of iterative methods.

**The iterative methods from Section 10.2** converge to a local minimum, not necessarily the global minimum.

[→ Click for minimization goal]

**For minimization**: We want θ̂_N (THAY-tah-hat) that minimizes V_N(θ) globally.

[→ Click for equation solving goal]

**For equation solving**: We want θ̂*_N that satisfies f_N(θ*) = 0.

[→ Click for the challenge]

**The challenge**: Iterative search only guarantees convergence to a local solution.
:::

------------------------------------------------------------------------

## Finding the Global Minimum

**Strategy**: Start the iterative minimization from different feasible initial values and compare results

. . .

**Why this is necessary**:

-   No algorithm can guarantee finding the global minimum in general

-   Starting point determines which local minimum we converge to

-   Must explore the parameter space with multiple starts

. . .

**Practical approach**:

1.  Use preliminary estimation procedures for good initial values

2.  Run optimization from several different starting points

3.  Select the solution with the lowest criterion value

::: notes
Here's how to find the global minimum.

**Strategy**: Start the iterative minimization from different feasible initial values and compare results.

[→ Click for why this is necessary]

**Why this is necessary**: No algorithm can guarantee finding the global minimum. Starting point determines which local minimum we find. Must explore with multiple starts.

[→ Click for the practical approach]

**Practical approach**: Use preliminary estimation for good initial values, run from several starting points, select the solution with the lowest criterion value.
:::

------------------------------------------------------------------------

## Two Aspects of "False" Local Minima

Local minima arise from **two distinct sources**:

. . .

**Aspect 1: Structural Local Minima**

-   The limit criterion $\bar{V}(\theta)$ itself has multiple minima

-   Inherent to the problem formulation

. . .

**Aspect 2: Sample-Induced Local Minima**

-   Finite-sample criterion $V_N(\theta, Z^N)$ has minima due to data randomness

-   May disappear with more data

::: notes
Not all local minima are created equal.

**Local minima arise from two distinct sources**:

[→ Click for Aspect 1]

**Aspect 1: Structural Local Minima**—the limit criterion V̄(θ) itself has multiple minima. Inherent to the problem formulation.

[→ Click for Aspect 2]

**Aspect 2: Sample-Induced Local Minima**—finite-sample V_N(θ) has minima due to data randomness. May disappear with more data.
:::

------------------------------------------------------------------------

## Aspect 1: Structural Local Minima

**The limit criterion** $\bar{V}(\theta) = \lim_{N \to \infty} V_N(\theta, Z^N)$ **itself has multiple minima**

. . .

**Why this happens:**

-   Complex model structures (neural networks, nonlinear models)

-   Model mismatch: true system $S \notin \mathcal{M}$

-   Identifiability issues (different $\theta$ give similar predictions)

. . .

**Consequence**: For large $N$, $V_N(\theta, Z^N)$ will have minima near those same locations (Lemma 8.2)

. . .

**This is inherent to the problem**, not just sampling variation!

::: notes
Let's dive deeper into structural local minima.

**The limit criterion V̄(θ)** itself has multiple minima—not about our specific data, but about the theoretical problem.

[→ Click for why this happens]

**Why this happens**: Complex model structures, model mismatch (S ∉ M), and identifiability issues.

[→ Click for the consequence]

**Consequence**: For large N, V_N(θ) will have minima near those same locations (Lemma 8.2).

[→ Click for the key point]

**This is inherent to the problem**, not just sampling variation! More data doesn't help.
:::

------------------------------------------------------------------------

## Aspect 2: Sample-Induced Local Minima

**Even if** $\bar{V}(\theta)$ **is convex (single minimum)**, the finite-sample criterion $V_N(\theta, Z^N)$ can have multiple local minima

. . .

**Causes:**

-   Random fluctuations in the data
-   Outliers
-   Finite sample effects

. . .

**Much harder to analyze theoretically** - depends on the specific data realization

. . .

**May disappear with more data** as $N \to \infty$


::: notes
Now let's look at sample-induced local minima.

**Even if V-bar of theta is convex**, the finite-sample criterion V_N can have multiple local minima.

[→ Click for causes]

**Causes**: Random fluctuations in the data, outliers, and finite sample effects.

[→ Click for harder to analyze]

**Much harder to analyze theoretically**—depends on the specific data realization.

[→ Click for good news]

**May disappear with more data** as V_N converges to V-bar.
:::

------------------------------------------------------------------------

## The Linear Regression Exception

**For linear regression with least squares:**

$$V_N(\theta) = \frac{1}{N}\sum_{t=1}^{N} (y(t) - \varphi^T(t)\theta)^2$$

. . .

This is a **quadratic function** of $\theta$ → strictly convex

. . .

**Result**: Exactly **one minimum** (the global one), regardless of data

. . .

**No false local minima possible by construction!**

::: notes
Here's an important exception to the local minima problem.

**For linear regression with least squares**, the criterion is a quadratic function of θ (THAY-tah).

[→ Click for convexity]

**This is a quadratic function of θ → strictly convex.** Think of it like a perfect bowl—exactly one bottom point.

[→ Click for result]

**Result: Exactly one minimum**, which is the global minimum, regardless of data.

[→ Click for the key point]

**No false local minima possible by construction!** This is why linear regression is so well-behaved—any descent algorithm finds the global optimum.
:::

------------------------------------------------------------------------

## Results for SISO Black-box Models

**Assumption**: The true system can be described within the model set: $S \in \mathcal{M}$

Consider the criterion:

$$\bar{V}(\theta) = \bar{E}\frac{1}{2}\varepsilon^2(t, \theta)$$

. . .

**Key results for different model structures:**

-   **ARMA models** ($B \equiv 0, D \equiv F \equiv 1$): All stationary points are global minima
-   **ARARX models** ($C = F \equiv 1$): No false minima if signal-to-noise ratio is large enough
-   **Single-input models** ($n_f = 1$): No false local minima
-   **Box-Jenkins** ($A = C = D \equiv 1$): No false minima if input is white noise

::: notes
Now let's look at results for SISO—Single-Input Single-Output—black-box models.

**Assumption**: The true system can be described within the model set—S is in M (script M).

[→ Click for key results]

**Key results for different model structures**:

- **ARMA models**: All stationary points are global minima—best case!

- **ARARX models**: No false minima if signal-to-noise ratio is large enough

- **Single-input models** (n_f = 1): No false local minima

- **Box-Jenkins**: No false minima if input is white noise

For ARMAX and output error structures, false local minima can occur. Output error is especially prone to this—initialization matters a lot.
:::

------------------------------------------------------------------------

## Initial Parameter Values

**Why good initial values matter:**

1.  Avoid converging to undesired local minima
2.  Faster convergence (fewer iterations needed)
3.  Shorter total computing time

. . .

**For physically parametrized models**:

-   Use physical insight for reasonable initial values (e.g. mass must be positive, damping ratios between 0 and 1, etc.k)
-   Allows monitoring and interaction with the search

. . .

**For linear black-box models**: Use a start-up procedure...

::: notes
Let me explain why good initial parameter values matter.

Good initial values matter because they determine which minimum we converge to, how many iterations it takes, and total computing time.

[→ Click for physically parametrized models]

For physically parametrized models, use physical insight. Mass must be positive, damping ratios are typically between 0 and 1. We can monitor the search and stop if parameters become unrealistic.

[→ Click for black-box models]

For linear black-box models, parameters don't have direct physical meaning—we need systematic start-up procedures, which we'll cover next.
:::

------------------------------------------------------------------------

## Start-up Procedure for Black-box Models

**General SISO model structure** (10.62):

$$y(t) = \frac{B(q)}{A(q)F(q)}u(t) + \frac{C(q)}{A(q)D(q)}e(t)$$

. . .

**Three-step initialization procedure:**

1.  **Estimate transfer function** $B/AF$ using IV method
    -   For open-loop systems: First estimate ARX model, use it to generate instruments
    -   Most often one of $A$ or $F$ is unity

. . .

2.  **Estimate equation noise model** from residuals

. . .

3.  **Estimate noise model** $C$ and/or $D$ using high-order AR model

::: notes
Here's the three-step initialization procedure for black-box models.

The general SISO model structure from equation 10.62 has B/AF describing input-to-output, and C/AD describing noise.

[→ Click for Step 1]

Step 1 is to estimate the transfer function B/AF using the IV method. For open-loop systems, first estimate an ARX model, then use it to generate instruments.

[→ Click for Step 2]

Step 2 is to estimate the equation noise from the residuals.

[→ Click for Step 3]

Step 3 is to estimate the noise model C and/or D using a high-order AR model.

Why does this work? If S ∈ M (script M), this procedure brings initial estimates close to true values as N increases—globally convergent!
:::

------------------------------------------------------------------------

## Start-up for Nonlinear Black-box Models

**Nonlinear model structure** (10.61):

$$y(t) = g(u(t), y_k, \beta_k) + v(t), \quad v(t) = h(\gamma_k, e_k)$$

where $g$ and $h$ are nonlinear functions.

. . .

**Simple initialization approach**:

1.  "Seed" many fixed values of the nonlinear parameters $\beta_k$, $\gamma_k$
2.  For each seed, estimate the linear parameters by linear least squares
3.  Select the estimates with most significant values (relative to standard deviations)
4.  Use selected $\beta_k$ and $\gamma_k$ as initial values for Gauss-Newton

::: notes
For nonlinear black-box models, initialization is harder—no systematic IV or ARX method exists.

The nonlinear model structure is y(t) = g(u, y_k, β_k) + v(t), where g is nonlinear. Examples include Hammerstein-Wiener models and neural networks.

[→ Click for the seeding approach]

Here's a simple initialization approach called the seeding strategy:

First, seed many fixed values of nonlinear parameters β_k (BAY-tah) and γ_k (GAM-uh). Second, for each seed, estimate linear parameters by LS—this is fast! Third, select estimates with most significant values, meaning large relative to their standard deviations. Fourth, use those selected values as initial values for Gauss-Newton.

This is coarse grid search followed by local refinement—more expensive but necessary for nonlinear problems.
:::

------------------------------------------------------------------------

## Nonlinear Black-box Models: Why Seeding Works

**Why the seeding approach works:**

-   Linear LS is **fast** → can try many seeds cheaply

-   Statistically significant estimates are more likely near true values

-   Provides **multiple good starting points** for Gauss-Newton

. . .

**How many seeds?**

| Model Complexity | Typical Range |
|------------------|---------------|
| Simple nonlinear | 10–100 seeds |
| Complex (e.g., neural nets) | 100–1000 seeds |

Seeds can be chosen **randomly** or on a **grid** in parameter space.

::: notes
Why does the seeding approach work?

Linear LS is fast, so we can try many seeds cheaply. Statistically significant estimates are more likely to be near true values. And this gives us multiple good starting points for Gauss-Newton.

[→ Click to reveal "How many seeds"]

How many seeds should you use? For simple nonlinear models, 10 to 100 seeds is typical. For complex models like neural nets, 100 to 1000 seeds.

Seeds can be chosen randomly or on a grid in parameter space. Random often works better in high dimensions.
:::

------------------------------------------------------------------------

## Initial Filter Conditions: The Problem

**The predictor** computes $\hat{y}(t)$ from past data using filters:

$$\hat{y}(t|\theta) = \underbrace{H_f(q, \theta)}_{\text{output filter}}y(t) + \underbrace{H_g(q, \theta)}_{\text{input filter}}u(t)$$

where $q$ = shift operator, $\theta$ = parameters

. . .

**Problem**: Filters are **recursive** - need past values $\varphi(0, \theta)$ to start

. . .

**At $t=1$**: Need $\varphi(0, \theta)$ - but we have **no data before** $t=1$!

. . .

**Example (ARX)**: To predict $y(1)$, need $y(0), y(-1), u(0)$ - not measured!

::: notes
Now a different practical issue—initial filter conditions.

**The predictor** computes ŷ(t) from past data using filters H_f (output filter) and H_g (input filter). The q is the shift operator (cue).

[→ Click for the problem]

**Problem: Filters are recursive**—they need past values φ(0,θ) (fie of zero) to start.

[→ Click for t=1]

**At t=1**: We need φ(0,θ) but we have no data before t=1!

[→ Click for example]

**Example (ARX)**: To predict y(1), need y(0), y(−1), u(0)—not measured! We need a strategy for handling this.
:::

------------------------------------------------------------------------

## Four Approaches to Initial Conditions

| Approach | Method | Best for |
|----------|--------|----------|
| **1. Zero** | $\varphi(0, \theta) = 0$ | Most cases (default) ⭐ |
| **2. Match data** | Choose so $\hat{y}(t) = y(t)$ initially | When we don't want initial errors messing up our first predictions |
| **3. As parameter** | Estimate $\varphi(0) = \eta$ with $\theta$ | Short data, slow dynamics |
| **4. Backforecast** | Run filters backwards | High-precision apps |

. . .

**Complexity**: Simple —► Complex

**Performance**: Good enough —► Optimal

::: notes
Here are **four approaches to initial conditions**.

The table shows: **Zero** (default, most cases), **Match data**, **As parameter** (short data, slow dynamics), and **Backforecast** (high-precision).

[→ Click for complexity/performance]

**Complexity**: Simple → Complex. **Performance**: Good enough → Optimal.

In practice, **Approach 1 (zero) works ~90% of the time**. Only use sophisticated methods for slow dynamics or limited data.
:::

------------------------------------------------------------------------

## When Do Initial Conditions Matter?

**Doesn't matter much** ✓ (zero init is fine):

- Long data records (N >> system time constant)

- Fast system dynamics

- ARX models with reasonable data

. . .

**Matters significantly** ⚠️ (use better methods):

- Short data records (N ≈ system time constant)

- Slow/poorly damped dynamics

- **Output Error (OE) models** 

- especially sensitive!

. . .

**Rule of thumb**: Data length > 10× time constant means zero init is okay

::: notes
When do initial conditions actually matter?

**Doesn't matter much** (zero init is fine): Long data records (N >> time constant), fast dynamics, ARX models.

[→ Click for when it matters]

**Matters significantly** (use better methods): Short data (N ≈ time constant), slow/poorly damped dynamics, **Output Error models**—especially sensitive!

[→ Click for rule of thumb]

**Rule of thumb**: Data length > 10× time constant → zero init is okay. OE models are problematic because all error goes into transfer function estimate.
:::

------------------------------------------------------------------------

## Summary of Section 10.5

**Key takeaways:**

-   Iterative methods converge to **local minima**, not necessarily global

-   **Multi-start strategy**: Use multiple initial values and compare results

-   **Two types of local minima**: Structural (in $\bar{V}(\theta)$) and Sample-induced (randomness in $V_N(\theta, Z^N)$)

-   **Good initialization is crucial** for efficiency and finding global minimum

. . .

**Practical recommendations:**

-   For black-box models: Use systematic start-up procedures (IV method, then noise estimation)

-   For nonlinear models: Use seeding + grid search

-   Initial filter conditions: Usually zero initialization is sufficient

::: notes
Let me wrap up Section 10.5 with the key takeaways.

First, iterative methods converge to local minima, not necessarily the global minimum. This is a fundamental limitation.

Second, we use a multi-start strategy. Try multiple initial values, compare results, and pick the one with the lowest criterion value.

Third, there are two types of local minima. Structural ones come from V̄(θ) (V-bar of THAY-tah) having multiple minima. Sample-induced ones come from finite data randomness.

Fourth, good initialization is crucial for both efficiency and finding the global minimum.

[→ Click for practical recommendations]

Now for practical recommendations.

For black-box models, use the IV method first, then estimate the noise model.

For nonlinear models, use the seeding approach with grid search—typically 10 to 1000 seeds depending on complexity.

For initial filter conditions, zero initialization is usually sufficient, unless you have short data, slow dynamics, or output error models.

That concludes Chapter 10—the computational workhorses of system identification!
:::