---
title: "System Identification: Chapter 8"
subtitle: "Section 8.5: Frequency-Domain Description of the Limit Model"
author: "System Identification: Theory for the User"
format:
  revealjs:
    theme: serif
    slide-number: true
    chalkboard: true
    preview-links: auto
    transition: slide
    html-math-method: mathjax
    css: assets/styles/styles.css
revealjs-plugins:
  - pointer
---

## Presentation Overview

**Chapter 8 - Convergence and Consistency**

Section 8.5: Linear Time-Invariant Models: Frequency-Domain Description of the Limit Model

::: notes
**SCRIPT:**

Hello everyone! Welcome to our presentation on Chapter 8 from Ljung's System Identification textbook. Chapter 8 is all about convergence and consistency - basically, what happens to our parameter estimates as we collect more and more data.

Now, Chapter 8 is quite comprehensive, but today we're going to focus specifically on Section 8.5. The full title is "Linear Time-Invariant Models: Frequency-Domain Description of the Limit Model." I know that's a mouthful, so let me break it down for you.

We're looking at linear time-invariant models - these are systems where the dynamics don't change over time and the principle of superposition applies. And we're going to describe the "limit model" - that's the model we converge to as our sample size goes to infinity - in the frequency domain rather than the time domain.

Why the frequency domain? Because as we'll see, it gives us incredibly powerful tools to understand where our model is accurate and where it struggles. Instead of just getting one overall error metric, we'll be able to see error as a function of frequency. That's the journey we're going on today!

________________________________________
ADDITIONAL NOTES:

**Chapter 8 context:**
- Convergence: Does θ̂_N → something as N → ∞?
- Consistency: Does θ̂_N → θ₀ (true parameters)?
- This chapter proves fundamental theorems about limiting behavior

**Why Section 8.5:**
Among many sections in Chapter 8, Section 8.5 is particularly important because:
- Provides frequency-domain tools (familiar to control engineers)
- Shows explicitly where model misfit occurs
- Connects to classical Bode analysis and frequency response

**Chapter 8 full scope:**
- 8.1-8.4: General convergence results
- 8.5: Frequency-domain description (our focus)
- 8.6-8.7: Additional special cases and extensions

**Audience consideration:**
Students may be familiar with frequency response from controls courses, so emphasizing the connection helps build intuition.
:::

------------------------------------------------------------------------

## Section 8.5: Introduction

**Goal**: Characterize the misfit between limiting model and true system in the **frequency domain**

. . .

**Context from Theorem 8.2** (as $N \to \infty$):

$$\theta^* = \arg\min_{\theta \in D_c} \bar{V}(\theta)$$

where $\bar{V}(\theta) = E[\varepsilon^2(t, \theta)]$

. . .

**Notation**:

- $\bar{V}(\theta)$ = criterion function (average prediction error)
- $\varepsilon(t, \theta)$ = prediction error at time $t$
- $\theta^*$ = limiting parameter estimate
- $D_c$ = constrained parameter space

::: notes
**SCRIPT:**

Welcome everyone! Today we're diving into Section 8.5, and I want to start by explaining what we're trying to accomplish here. The big picture goal is to understand how well our model matches the true system - but we're going to do this in the frequency domain rather than the time domain.

Now, you might be wondering - why the frequency domain? We'll get to that in a moment, but first let me give you some important context from what we learned earlier in Chapter 8.

Remember Theorem 8.2? It told us that as we collect more and more data - as N goes to infinity - our parameter estimate converges to something called theta-star. Think of theta-star as the best possible parameter values we could ever hope to get with our particular model structure.

Mathematically, theta-star is the value of theta that minimizes V-bar of theta. Here, V-bar represents the expected value of the squared prediction error - it's basically asking "on average, how wrong are our predictions?"

Let me make sure the notation is clear because we'll be using these symbols a lot. V-bar of theta is our criterion function - it measures average prediction error. Epsilon of t comma theta is the prediction error at any given time t. Theta-star is that limiting parameter estimate I mentioned. And D-c is the constrained parameter space - basically, the allowed region where theta can live.

________________________________________
ADDITIONAL NOTES:

**Greek letter pronunciations:**
- θ (theta) - "THAY-tuh"
- ε (epsilon) - "EP-sih-lon"
- θ* (theta-star) - "THAY-tuh star"

**Technical terms:**
- Criterion function: An objective function we're trying to minimize
- Prediction error: The difference between what we predict and what actually happens
- Limiting estimate: What the estimate converges to as sample size approaches infinity
- Constrained parameter space: The valid region for parameter values (might have physical constraints like positivity)

**Connection to previous material:**
- This builds directly on Theorem 8.2 (convergence of parameter estimates)
- Chapter 8 overall is about what happens as N → ∞
:::

------------------------------------------------------------------------

## Two Cases: Model Match

**Two cases**:

. . .

**Case 1**: If $S \in \mathcal{M}$: $\theta^* = \theta_0$

- True system is in our model set
- We find the true parameters (true description, Theorem 8.3)

. . .

**Case 2**: If $S \notin \mathcal{M}$: Model differs from true system

- Model structure cannot perfectly represent true system
- $\theta^*$ is still best approximation within model class
- There will be inherent misfit

. . .

**This section**: Develop expressions for this misfit for linear time-invariant models

::: notes
**SCRIPT:**

Alright, so now that we know what theta-star is, we need to understand two fundamentally different situations we might encounter. This is really important because it affects how we interpret our results.

Let me walk you through Case 1 first. Imagine we're really lucky - the true system S is actually in our model set M. What does "S in M" mean? It means our model structure is flexible enough that it can perfectly represent the true system. In this ideal scenario, theta-star equals theta-zero - the true parameters. This is the best-case scenario, and Theorem 8.3 covers this situation.

But here's the thing - in practice, we're almost never this lucky! Which brings us to Case 2. Usually, S is NOT in M. Our model structure - maybe it's an ARX model, or an ARMAX model, or whatever we chose - simply cannot perfectly represent the true system. Maybe the true system has different dynamics, or nonlinearities we didn't account for.

Now you might be thinking, "Is our model useless then?" Not at all! Theta-star is still the BEST approximation we can get within our chosen model class. It's the closest we can get to the truth given our model structure. But there will be some inherent misfit - some difference between our model and reality.

And that's exactly what this section is about - developing mathematical tools to characterize and understand this misfit, specifically in the frequency domain. We want to know: where does our model do well? Where does it struggle? What frequencies are we capturing accurately, and which ones are we missing?

________________________________________
ADDITIONAL NOTES:

**Greek letters:**
- θ* (theta-star) - "THAY-tuh star" - limiting parameter estimate
- θ₀ (theta-zero) - "THAY-tuh zero" - true parameter values

**Mathematical notation:**
- S ∈ M - "S in M" - true system is in model set
- S ∉ M - "S not in M" - true system is not in model set
- M (script M) - the model set/class we've chosen

**Real-world analogy:**
Think of it like fitting a straight line to data that actually follows a curve. If the data is truly linear (Case 1), you can find the perfect line. But if it's curved (Case 2), you'll find the best straight line approximation, but there's inherent error you can't eliminate without changing your model structure.

**Why this matters:**
Understanding which case you're in affects:
- How you interpret model validation results
- Whether you need to increase model complexity
- What accuracy you can realistically expect
:::

------------------------------------------------------------------------

## Frequency Domain Benefits

For linear time-invariant (LTI) systems, frequency domain provides:

. . .

**1. Intuitive understanding**

- See how well model fits at different frequencies

- Which frequency ranges: well-modeled vs. poorly-modeled

. . .

**2. Clear interpretation**

- Connection to classical frequency response analysis

- Explicit weighting functions showing what criterion emphasizes

. . .

**Key benefit**: Tools to understand **where** (at which frequencies) our model is good vs. poor

::: notes
**SCRIPT:**

Okay, so we've established that we want to characterize model misfit. But why are we doing this in the frequency domain? Why not just stick with the time domain that we're more familiar with? Great question - let me explain.

The frequency domain gives us two huge advantages when working with linear time-invariant systems.

First advantage: intuitive understanding. Instead of just getting one number that says "your model has this much error," the frequency domain breaks it down. It shows you how well your model fits at low frequencies versus high frequencies. You might discover, for example, that your model captures the slow dynamics perfectly but struggles with fast oscillations. That's actionable information!

Second advantage: clear interpretation. If you've taken a controls course or worked with Bode plots, this will feel familiar. The frequency domain connects directly to classical frequency response analysis that engineers have been using for decades. Plus, it reveals explicit weighting functions - you can actually see what the identification criterion is emphasizing. Is it putting more weight on low frequencies? High frequencies? The frequency domain makes this transparent.

Here's the bottom line: the frequency domain gives us localization. In the time domain, errors at all times get mixed together. But in the frequency domain, we can pinpoint WHERE - at which specific frequencies - our model is accurate versus where it's struggling. This is absolutely crucial for model validation.

Now, some of you might be thinking, "Wait, what exactly is frequency anyway?" Great question! Let me clarify. Frequency describes how fast something oscillates or repeats. Low frequencies are slow oscillations - think of ocean tides or economic cycles over years. High frequencies are fast oscillations - like the 440 Hertz vibration of a musical note.

When we analyze a system in the frequency domain, we're essentially asking: if I give the system an input that oscillates at different speeds, how does it respond? The beautiful thing about linear systems is we can look at each frequency separately, which is exactly what makes this analysis so powerful.

________________________________________
ADDITIONAL NOTES:

**Greek letter:**
- ω (omega) - "oh-MAY-guh" - frequency variable (radians per sample)

**Frequency domain concepts:**
- Low frequency: Slow oscillations, long-term behavior (e.g., 0.01 Hz = one cycle per 100 seconds)
- High frequency: Fast oscillations, quick dynamics (e.g., 100 Hz = one hundred cycles per second)
- Frequency response: How system gain and phase change with frequency

**Why frequency domain for LTI systems:**
- Superposition principle: Each frequency component can be analyzed independently
- Transfer functions have simple frequency-domain representation
- Separates "what the system does" at different timescales

**Practical examples:**
- A car suspension: Low-frequency response (hitting a speed bump) vs. high-frequency response (road vibrations)
- Audio system: Bass response (low freq) vs. treble response (high freq)
- Economic model: Long-term trends (low freq) vs. short-term fluctuations (high freq)

**Connection to controls:**
- Bode plots show magnitude and phase vs. frequency
- This section develops similar tools for system identification
:::

------------------------------------------------------------------------

## Section 8.5 Roadmap

**How do we characterize model misfit in frequency domain?**

. . .

**Step 1**: Define a reference system (even for arbitrary/nonlinear systems)

- Use Wiener filter to get "best linear approximation"

- Computed from signal spectra (frequency-domain)

. . .

**Step 2**: Express prediction error criterion in frequency domain

- Use Parseval's theorem to convert time → frequency

- Shows which frequencies contribute to error

. . .

**Step 3**: Compare model to reference

- Explicit formulas showing misfit at each frequency

- Weighting functions reveal what criterion emphasizes

::: notes
**SCRIPT:**

Alright, before we dive into the technical details, let me give you a clear roadmap so you know where we're headed. The central question driving everything is: how do we characterize model misfit in the frequency domain?

We're going to tackle this in three logical steps. Let me walk you through each one.

Step 1 is to define a reference system. Now, here's something really cool - even if the true system is arbitrary, even if it's nonlinear or time-varying, we can still use something called the Wiener filter to get the best linear approximation. You might be wondering how this connects to frequency domain - and here's the key: the Wiener filter is computed directly from signal spectra, which are frequency-domain quantities. So right from the start, we're working in the frequency domain.

Step 2 is to express our prediction error criterion in the frequency domain. Remember that V-bar of theta we talked about earlier? That was in the time domain - it's an average over time. We're going to use a powerful tool called Parseval's theorem to convert this from time domain to frequency domain. Once we do that, we'll be able to see which frequencies contribute most to the error. Instead of one big error number, we'll have an error contribution at each frequency.

Finally, Step 3 is where it all comes together. We'll compare our parametric model - the one with parameters theta - to this reference system from Step 1. We'll derive explicit formulas showing the misfit at each frequency, and we'll see weighting functions that reveal exactly what the criterion emphasizes. Does it care more about low frequencies? High frequencies? The math will tell us.

Think of this roadmap as our game plan. Each step builds on the previous one, and by the end, we'll have a complete frequency-domain picture of model quality.

________________________________________
ADDITIONAL NOTES:

**The three-step strategy:**
1. Reference system (Wiener filter) - "best possible linear model"
2. Frequency-domain criterion (Parseval's theorem) - "convert time to frequency"
3. Model vs. reference comparison - "see where the misfit is"

**Why this approach works:**
- Wiener filter gives us an optimal benchmark to compare against
- Parseval's theorem lets us work in frequency domain
- Comparison reveals frequency-specific model limitations

**Parseval's theorem preview:**
Relates time-domain energy to frequency-domain energy:
∑ x²(t) ↔ ∫ Φ(ω) dω
Total energy in time = Total energy across all frequencies

**Key insight:**
By working in frequency domain, we go from:
- One number (total error) →
- A function of frequency (error at each ω)
This gives us much richer diagnostic information!
:::

------------------------------------------------------------------------

## Data from Arbitrary Systems

**Surprising result**: Approximation expressions apply to data from:

- Nonlinear systems

- Time-varying systems

- Arbitrary stochastic processes

. . .

**Only assumption**: Signals are jointly quasi-stationary (spectra exist)

::: notes
**SCRIPT:**

Okay, now we're starting Step 1 of our roadmap - defining a reference system. And I want to tell you about something that really surprised me when I first learned it. The approximation expressions we're about to develop - they don't just work for nice, well-behaved linear systems. They actually apply to data from arbitrary systems!

What do I mean by arbitrary? I mean nonlinear systems - systems where doubling the input doesn't double the output. Time-varying systems - where the dynamics change over time. Even arbitrary stochastic processes - random signals that don't follow simple patterns. The theory we're developing is incredibly general.

Now you might be thinking, "Wait, if it works for any system, there must be some catch, right?" Well, there is one assumption we need: the signals must be jointly quasi-stationary. Let me break that down. Quasi-stationary basically means that the spectra - the frequency-domain representations of our signals - are well-defined and don't change drastically over time. So we can't have signals that are completely nonstationary, but most practical signals satisfy this condition.

Here's what's really cool about this. Even though we're going to develop the theory by first assuming a linear system - because that makes the math cleaner and easier to follow - the results apply much more broadly. This generality is what makes this approach so practical for real-world applications where systems are rarely perfectly linear.

________________________________________
ADDITIONAL NOTES:

**Quasi-stationary signals:**
- Statistical properties (like spectra) are approximately constant over the observation period
- Not strictly stationary, but "stationary enough" for spectral analysis
- Most real-world signals in engineering applications satisfy this

**Examples of systems this applies to:**
- Nonlinear mechanical systems (e.g., suspension with nonlinear springs)
- Time-varying economic systems (slowly changing dynamics)
- Biological systems with random components
- Power systems with load variations

**Why this generality matters:**
- Real systems are rarely perfectly linear
- This theory still gives us useful approximations
- The "best linear approximation" is often good enough for control/prediction

**Technical note:**
"Jointly quasi-stationary" means both input u(t) and output y(t) are quasi-stationary, and their cross-correlation properties are also well-defined in the frequency domain (cross-spectra exist).
:::

------------------------------------------------------------------------

## The Wiener Filter: Optimal Predictor

**Optimal linear predictor** of $y(t)$ from past data:

$$\hat{y}(t|t-1) = W_u(q)u(t) + W_y(q)y(t)$$

. . .

**Filters computed from spectra**:

- $\Phi_u(\omega)$: Spectrum of input

- $\Phi_y(\omega)$: Spectrum of output

- $\Phi_{uu}(\omega), \Phi_{yu}(\omega)$: Cross-spectra

. . .

**Key property**: Prediction error $e_0(t) = y(t) - \hat{y}(t|t-1)$ is **uncorrelated** with past inputs/outputs

::: notes
**SCRIPT:**

Now let me introduce you to the Wiener filter - this is the key tool we'll use to define our reference system. The Wiener filter gives us the optimal linear predictor of y at time t based on past data.

Let's break down what this equation is saying. The predicted value - written as y-hat at time t given t minus 1 - consists of two parts. The first part, W-u of q times u of t, is filtering the past inputs. The second part, W-y of q times y of t, is filtering the past outputs. So we're using both past inputs and past outputs to predict the current output. That should make intuitive sense, right?

Now here's the really important part - how do we find these filters W-u and W-y? This is where the frequency domain comes in. These filters are computed directly from the spectra of our signals. We need Phi-u of omega, which is the spectrum of the input - it tells us the frequency content of the input signal. We need Phi-y of omega, the spectrum of the output. And we need the cross-spectra - Phi-u-u and Phi-y-u - which tell us how the input and output are correlated at different frequencies.

All of these - the spectra and cross-spectra - are frequency-domain quantities. This is why I keep emphasizing we're working in the frequency domain!

Now, there's one more crucial property I need to mention. The prediction error from the Wiener filter - we call it e-zero at time t, which equals y at time t minus y-hat at time t given t minus 1 - has a special property: it's uncorrelated with all past inputs and outputs. This means the Wiener filter has extracted all the predictable information from the past. Whatever's left in e-zero is truly unpredictable - it's the innovation, the new information at time t.

________________________________________
ADDITIONAL NOTES:

**Greek letters:**
- Φ (Phi) - "FYE" or "FEE" - spectrum/cross-spectrum
- ω (omega) - "oh-MAY-guh" - frequency variable

**Notation explanation:**
- ŷ(t|t-1) - "y-hat at t given t minus 1" - one-step-ahead prediction
- W_u(q), W_y(q) - filter operators acting on u and y
- q - shift operator (q^(-1) means one time step back)

**Wiener filter properties:**
- Optimal in mean-square error sense
- Linear filter (can be implemented with convolution)
- Computed from second-order statistics (spectra)
- Prediction error is white (uncorrelated over time)

**Spectrum vs. cross-spectrum:**
- Φ_u(ω): Power spectrum of u - "energy" of u at frequency ω
- Φ_y(ω): Power spectrum of y - "energy" of y at frequency ω
- Φ_yu(ω): Cross-spectrum - how u and y are related at frequency ω

**Why "uncorrelated with past" matters:**
- Means we've extracted all predictable information
- Remaining error e₀(t) is "innovation" - truly new info
- This is what makes Wiener filter optimal

**Historical note:**
Developed by Norbert Wiener in 1940s for filtering and prediction problems. Foundational work in signal processing and control theory.
:::

------------------------------------------------------------------------

## Standard Form Representation

**Define**:

- $H_0(q) = (1 - W_y(q))^{-1}$

- $G_0(q) = H_0(q)W_u(q)$

. . .

**Then rewrite as**:

$$y(t) = G_0(q)u(t) + H_0(q)e_0(t)$$

where $e_0(t)$ is uncorrelated sequence

. . .

**Interpretation**: Best linear time-invariant approximation of true system (based on second-order properties)

. . .

**Caveat**: May be useless for control - depends on data spectra, not just system dynamics

::: notes
**SCRIPT:**

Alright, now we're going to take the Wiener filter we just talked about and rewrite it in a more familiar form - what we call the "standard form" for system identification. This is going to look a lot like the model structures you've seen before.

Let me walk you through the definitions. We define H-zero of q as the quantity 1 minus W-y of q, and then we take the inverse of that whole thing. Then we define G-zero of q as H-zero of q times W-u of q. These might seem like arbitrary definitions, but trust me, they lead to something really clean.

With these definitions, we can rewrite the system as: y at time t equals G-zero of q times u at time t, plus H-zero of q times e-zero at time t. Look familiar? This is the same form as our standard model structures! We have a transfer function G-zero mapping input to output, and a noise model H-zero shaping the white noise e-zero.

This is Equation 8.62 from the textbook, and here's how you should interpret it: this represents the best linear time-invariant approximation of the true system. It's "best" in the sense that it minimizes prediction error based on second-order properties - meaning the spectra.

But - and this is important - I need to give you a caveat. This representation might be useless for control applications! Let me explain why. This model depends on the data spectra - specifically, what frequencies were present in the input during the identification experiment. If you're designing a controller and the input spectrum changes - maybe your controller excites different frequencies than your identification signal did - this model may no longer be appropriate. The model is data-dependent, not just system-dependent. So be careful when using this for control design!

________________________________________
ADDITIONAL NOTES:

**Standard form:**
y(t) = G(q)u(t) + H(q)e(t)
- G(q): Transfer function from input to output
- H(q): Noise model (shaping filter for white noise)
- e(t): White noise (innovation)

**Why this form is useful:**
- Matches standard model structures (ARX, ARMAX, OE, BJ)
- G₀ and H₀ are transfer functions we can analyze
- Separates deterministic (G₀u) and stochastic (H₀e₀) parts

**The caveat explained:**
- Wiener filter optimizes for the SPECIFIC input spectrum used in identification
- If control changes the input spectrum, the optimal model may change
- Example: If you identified with low-frequency input but control uses high-frequency, the Wiener solution changes

**Difference from true system:**
- True system G₀, H₀: Physical dynamics (independent of input spectrum)
- Wiener G₀, H₀: Optimal approximation (depends on input spectrum)
- These coincide only if true system is LTI and in model class

**Equation 8.62 reference:**
This is a key equation in Ljung's Chapter 8, establishing the "best linear approximation" concept.
:::

------------------------------------------------------------------------

## Frequency-Domain Expression for $\bar{V}(\theta)$

**Parseval's theorem** (fundamental expression 2.65):

$$\bar{V}(\theta) = \bar{E}\frac{1}{2}\varepsilon^2(t, \theta) = \frac{1}{4\pi}\int_{-\pi}^{\pi} \Phi_{\varepsilon}(\omega, \theta)d\omega$$

where $\Phi_{\varepsilon}(\omega, \theta)$ = spectrum of prediction errors

. . .

**Time domain** → **Frequency domain**

Average squared error → Integral over all frequencies

::: notes
**SCRIPT:**

Excellent! Now we move to Step 2 of our roadmap: expressing the prediction error criterion in the frequency domain. This is where things get really interesting.

Remember V-bar of theta from the beginning? That's our criterion function - the thing we're trying to minimize. In the time domain, it's defined as the expected value of one-half epsilon squared. It's asking: on average, what's the squared prediction error?

But now, thanks to Parseval's theorem - which is fundamental expression 2.65 in the book - we can rewrite this in the frequency domain. V-bar of theta equals one over four pi times the integral from minus pi to plus pi of Phi-epsilon of omega comma theta, d-omega.

Let me explain what this means. Phi-epsilon of omega comma theta is the spectrum of the prediction errors. It tells us how much error we have at each frequency omega. The integral sums up the error contributions across all frequencies from minus pi to plus pi.

Here's the key insight: Parseval's theorem relates time-domain energy to frequency-domain energy. What was "average squared error over time" in the time domain becomes "integral of error spectrum over all frequencies" in the frequency domain. These are two different ways of measuring the same total error!

This transformation is incredibly powerful because now we can analyze which frequencies contribute to the total error. Instead of just knowing "our model has this much total error," we can ask "how much error do we have at low frequencies versus high frequencies?" This frequency-by-frequency breakdown is what makes this analysis so useful.

________________________________________
ADDITIONAL NOTES:

**Parseval's theorem:**
Time domain ↔ Frequency domain
E[x²(t)] = (1/2π) ∫ Φ_x(ω) dω

Key idea: Total energy in time domain = Total energy across all frequencies

**Why the limits -π to π:**
For discrete-time systems, frequency ω is normalized to [-π, π]
- ω = 0: DC (zero frequency)
- ω = π: Nyquist frequency (half the sampling rate)
- Spectrum is periodic, so we only need [-π, π]

**The factor 1/4π:**
Comes from the definition of spectrum and averaging
Different books use different normalizations (1/2π, 1/4π)
Ljung's convention uses 1/4π for this particular expression

**Greek letters:**
- Φ_ε (Phi-epsilon) - "FYE EP-sih-lon" - spectrum of prediction error
- ω (omega) - "oh-MAY-guh" - frequency variable
- θ (theta) - "THAY-tuh" - parameter vector

**Fundamental expression 2.65:**
Reference to earlier chapter where Parseval's theorem was introduced
Connects time-domain and frequency-domain analysis

**Why this matters:**
Before: V̄(θ) = one number (total error)
After: V̄(θ) = ∫ Φ_ε(ω,θ) dω (error contribution at each frequency)
We can see WHERE the error is!
:::

------------------------------------------------------------------------

## Why This Is Useful

**Frequency-domain view shows**:

- Which frequencies contribute most to total error

- How weighting function affects the fit

- Where model is good vs. poor

. . .

**Example**: Model might fit well at low frequencies but poorly at high frequencies

::: notes
**SCRIPT:**

Now you might be asking, "Okay, we can express V-bar of theta in the frequency domain - but why should I care? What does this actually buy me?" Great question! Let me explain why this frequency-domain view is so powerful.

The frequency-domain view shows us three important things. Let me walk through each one.

First, it shows which frequencies contribute most to the total error. Remember, V-bar of theta is an integral over frequency. If Phi-epsilon is large at some frequencies and small at others, you immediately know where your error is coming from. Maybe it's all concentrated at high frequencies, or maybe it's spread out. This diagnostic information is invisible in the time domain!

Second, it reveals how the weighting function affects the fit. When we do weighted least squares or use different identification criteria, they weight different frequencies differently. The frequency domain makes this explicit - you can literally see which frequencies the criterion cares about more.

Third, it tells us where the model is good versus where it's poor. And this is actionable information!

Let me give you a concrete example. Suppose you identify a model and plot Phi-epsilon versus omega. You might find that your model fits well at low frequencies - small error there - but poorly at high frequencies - large error there. What do you do with this information? Well, maybe you need more model complexity to capture high-frequency dynamics. Or maybe, for your application, high-frequency accuracy isn't critical - maybe you're doing slow control and you only care about the low-frequency behavior. Either way, the frequency-domain view tells you exactly what's going on.

This is the power of frequency-domain analysis - you get localization in frequency. Instead of one error number, you get an error function over frequency. You can see exactly which frequency bands are problematic and which are fine.

________________________________________
ADDITIONAL NOTES:

**Diagnostic value:**
Frequency-domain error spectrum Φ_ε(ω,θ) tells you:
- Which frequencies have large prediction error
- Whether model order is adequate
- If weighting is appropriate for application
- Where model complexity should be added

**Example interpretation:**
If Φ_ε(ω,θ) is:
- Large at low ω, small at high ω → Model misses slow dynamics
- Small at low ω, large at high ω → Model misses fast dynamics
- Large everywhere → Fundamental model structure problem
- Large at specific ω → Possible unmodeled resonance at that frequency

**Actionable decisions:**
Based on Φ_ε(ω,θ):
- Increase model order (more poles/zeros)
- Change model structure (e.g., ARX → ARMAX)
- Use frequency-weighted identification
- Accept limitations if error is in unimportant frequency range

**Connection to validation:**
This is essentially frequency-domain residual analysis
Similar to plotting residual autocorrelation, but in frequency domain
Complements time-domain validation tests

**Practical workflow:**
1. Identify model (get θ̂)
2. Compute error spectrum Φ_ε(ω,θ̂)
3. Plot to see frequency-dependent performance
4. Iterate: adjust model structure, reidentify, check again
:::

------------------------------------------------------------------------

## The True System (Assumption S1)

**Assume** true system is linear time-invariant (LTI):

$$y(t) = G_0(q)u(t) + H_0(q)e_0(t)$$

where:

- $G_0(q)$ = true transfer function (input → output)

- $H_0(q)$ = true noise model

- $e_0(t)$ = white noise with variance $\lambda_0$

. . .

**Next**: Derive explicit expression for $\Phi_{\varepsilon}(\omega, \theta)$ in terms of true system ($G_0$, $H_0$) and model ($G(\cdot, \theta)$, $H(\cdot, \theta)$)

::: notes
**SCRIPT:**

Alright, we're getting close to the end now. We've talked about the Wiener filter as a reference, and we've seen how to express the criterion in the frequency domain. Now we need to connect this to a specific assumption about the true system.

We're going to make Assumption S1: the true system is linear time-invariant, or LTI. We write it in our standard form: y at time t equals G-zero of q times u at time t, plus H-zero of q times e-zero at time t.

Let me make sure you understand what each piece means. G-zero of q is the true transfer function - it maps input to output deterministically. If you put in a sine wave at frequency omega, G-zero tells you the gain and phase shift of the output. H-zero of q is the true noise model - it shapes the white noise into colored noise. And e-zero at time t is white noise with variance lambda-zero - this represents the truly unpredictable part of the system, the innovation.

Now, what we haven't shown you yet - and what the textbook develops in detail - is how to derive an explicit expression for Phi-epsilon of omega and theta. This expression will involve both the true system - characterized by G-zero and H-zero - and our parametric model - characterized by G of q comma theta and H of q comma theta. When you work through the algebra, you get a formula that shows exactly how the model misfit depends on the difference between your model and the truth, at each frequency.

This sets up the complete framework for analyzing model misfit in the frequency domain, which is the ultimate goal of Section 8.5. Once you have that expression for Phi-epsilon, you can plug it into the integral we saw earlier, and you get V-bar of theta explicitly in terms of G-zero, H-zero, G of theta, and H of theta.

And that's the power of this whole approach - it gives you explicit, frequency-by-frequency characterization of where your model matches or doesn't match the true system. That concludes our overview of Section 8.5. Thank you!

________________________________________
ADDITIONAL NOTES:

**Assumption S1:**
True system is LTI (linear time-invariant):
y(t) = G₀(q)u(t) + H₀(q)e₀(t)

This is more restrictive than the Wiener filter case, which worked for arbitrary systems. Here we assume true system is actually linear.

**Model vs. True System:**
- True system: G₀(q), H₀(q) - fixed, unknown
- Parametric model: G(q,θ), H(q,θ) - known structure, estimated θ
- Goal: Find θ that makes G(q,θ) ≈ G₀(q) and H(q,θ) ≈ H₀(q)

**Greek letters:**
- G₀ (G-zero) - "G zero" - true transfer function
- H₀ (H-zero) - "H zero" - true noise model
- e₀ (e-zero) - "e zero" - true innovation sequence
- λ₀ (lambda-zero) - "LAM-duh zero" - variance of e₀

**What comes next in textbook:**
Section 8.5 derives expressions like:
Φ_ε(ω,θ) = function of [G₀(e^iω), H₀(e^iω), G(e^iω,θ), H(e^iω,θ)]

This shows exactly how model misfit at frequency ω depends on difference between model and truth at that frequency.

**Connection to earlier slides:**
- Step 1: Wiener filter (best linear approximation)
- Step 2: Parseval's theorem (time → frequency)
- Step 3 (not covered in detail): Derive Φ_ε(ω,θ) under Assumption S1

**Why LTI assumption:**
Makes frequency-domain analysis particularly clean
Transfer functions G(e^iω), H(e^iω) have clear interpretation
Results in explicit formulas for model quality
:::

------------------------------------------------------------------------